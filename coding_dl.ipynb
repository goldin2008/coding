{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b70c2f-bf97-4bfe-bc15-fd992cb5cec0",
   "metadata": {},
   "source": [
    "## Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch\n",
    "\n",
    "> https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d191f3f2-96e0-44fa-8046-d417ef9301f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}\n",
      "tensor([0, 4, 5, 2, 1, 3])\n",
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
      "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
      "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
      "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
      "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
      "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
      "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
      "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
      "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
      "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
      "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
      "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n",
      "torch.Size([6, 16])\n",
      "tensor([ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
      "         0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465])\n",
      "torch.Size([16])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([28])\n",
      "keys.shape: torch.Size([6, 24])\n",
      "values.shape: torch.Size([6, 28])\n",
      "tensor(11.1466, grad_fn=<DotBackward0>)\n",
      "tensor([ 8.5808, -7.6597,  3.2558,  1.0395, 11.1466, -0.4800],\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.2912, 0.0106, 0.0982, 0.0625, 0.4917, 0.0458],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([28])\n",
      "tensor([-1.5993,  0.0156,  1.2670,  0.0032, -0.6460, -1.1407, -0.4908, -1.4632,\n",
      "         0.4747,  1.1926,  0.4506, -0.7110,  0.0602,  0.7125, -0.1628, -2.0184,\n",
      "         0.3838, -2.1188, -0.8136, -1.5694,  0.7934, -0.2911, -1.3640, -0.2366,\n",
      "        -0.9564, -0.5265,  0.0624,  1.7084], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Embedding an Input Sentence\n",
    "sentence = 'Life is short, eat dessert first'\n",
    "\n",
    "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "print(dc)\n",
    "\n",
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(6, 16)\n",
    "embedded_sentence = embed(sentence_int).detach()\n",
    "\n",
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)\n",
    "\n",
    "print(embedded_sentence[1])\n",
    "print(embedded_sentence[1].shape)\n",
    "\n",
    "\n",
    "# Defining the Weight Matrices\n",
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_q, d))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_k, d))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_v, d))\n",
    "\n",
    "\n",
    "# Computing the Unnormalized Attention Weights\n",
    "x_2 = embedded_sentence[1]\n",
    "query_2 = W_query.matmul(x_2)\n",
    "key_2 = W_key.matmul(x_2)\n",
    "value_2 = W_value.matmul(x_2)\n",
    "\n",
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)\n",
    "\n",
    "keys = W_key.matmul(embedded_sentence.T).T\n",
    "values = W_value.matmul(embedded_sentence.T).T\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "omega_24 = query_2.dot(keys[4])\n",
    "print(omega_24)\n",
    "\n",
    "omega_2 = query_2.matmul(keys.T)\n",
    "print(omega_2)\n",
    "\n",
    "\n",
    "# Computing the Attention Scores\n",
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\n",
    "print(attention_weights_2)\n",
    "\n",
    "context_vector_2 = attention_weights_2.matmul(values)\n",
    "\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08481030-0ff9-4da1-9211-9544e0c951be",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a70fbabf-d371-453d-9df0-f1b43c78503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 24])\n",
      "torch.Size([3, 16, 6])\n",
      "multihead_keys.shape: torch.Size([3, 24, 6])\n",
      "multihead_values.shape: torch.Size([3, 28, 6])\n",
      "multihead_keys.shape: torch.Size([3, 6, 24])\n",
      "multihead_values.shape: torch.Size([3, 6, 28])\n"
     ]
    }
   ],
   "source": [
    "h = 3\n",
    "multihead_W_query = torch.nn.Parameter(torch.rand(h, d_q, d))\n",
    "multihead_W_key = torch.nn.Parameter(torch.rand(h, d_k, d))\n",
    "multihead_W_value = torch.nn.Parameter(torch.rand(h, d_v, d))\n",
    "\n",
    "multihead_query_2 = multihead_W_query.matmul(x_2)\n",
    "print(multihead_query_2.shape)\n",
    "\n",
    "multihead_key_2 = multihead_W_key.matmul(x_2)\n",
    "multihead_value_2 = multihead_W_value.matmul(x_2)\n",
    "\n",
    "stacked_inputs = embedded_sentence.T.repeat(3, 1, 1)\n",
    "print(stacked_inputs.shape)\n",
    "\n",
    "multihead_keys = torch.bmm(multihead_W_key, stacked_inputs)\n",
    "multihead_values = torch.bmm(multihead_W_value, stacked_inputs)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)\n",
    "\n",
    "multihead_keys = multihead_keys.permute(0, 2, 1)\n",
    "multihead_values = multihead_values.permute(0, 2, 1)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb75e0b-7ddb-4795-b692-1793ebd21ca2",
   "metadata": {},
   "source": [
    "## Cross-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "601a5370-7cef-4e84-b765-274e3ef5ee32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_sentence.shape: torch.Size([6, 16])\n",
      "query.shape torch.Size([24])\n",
      "keys.shape: torch.Size([6, 24])\n",
      "values.shape: torch.Size([6, 28])\n",
      "keys.shape: torch.Size([8, 24])\n",
      "values.shape: torch.Size([8, 28])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "print(\"embedded_sentence.shape:\", embedded_sentence.shape)\n",
    "\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "\n",
    "W_query = torch.rand(d_q, d)\n",
    "W_key = torch.rand(d_k, d)\n",
    "W_value = torch.rand(d_v, d)\n",
    "\n",
    "x_2 = embedded_sentence[1]\n",
    "query_2 = W_query.matmul(x_2)\n",
    "print(\"query.shape\", query_2.shape)\n",
    "\n",
    "keys = W_key.matmul(embedded_sentence.T).T\n",
    "values = W_value.matmul(embedded_sentence.T).T\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "embedded_sentence_2 = torch.rand(8, 16) # 2nd input sequence\n",
    "\n",
    "keys = W_key.matmul(embedded_sentence_2.T).T\n",
    "values = W_value.matmul(embedded_sentence_2.T).T\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ece10b-bdd4-4191-8d60-3ddf85022684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "595a6137-0e0c-4cab-bdfe-a0ba0376a959",
   "metadata": {},
   "source": [
    "# RAG on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc5600-17bb-4cff-9126-23d626db70c5",
   "metadata": {},
   "source": [
    "> https://aws.amazon.com/blogs/machine-learning/evaluate-rag-responses-with-amazon-bedrock-llamaindex-and-ragas/\n",
    "\n",
    "> https://github.com/aws-samples/sample-rag-evaluation-ragas/blob/main/ragas_notebook.ipynb\n",
    "\n",
    "> https://aws.amazon.com/blogs/machine-learning/from-concept-to-reality-navigating-the-journey-of-rag-from-proof-of-concept-to-production/\n",
    "\n",
    "\n",
    "This post guides you through the process of assessing quality of RAG response with evaluation framework such as RAGAS and LlamaIndex with Amazon Bedrock.\n",
    "\n",
    "In this post, we are also going to leverage Langchain to create a sample RAG application.\n",
    "\n",
    "Amazon `Bedrock` is a fully managed service that offers a choice of high-performing Foundation Models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.\n",
    "\n",
    "The Retrieval Augmented Generation Assessment (`RAGAS`) framework offers multiple metrics to evaluate each part of the RAG system pipeline, identifying areas for improvement. It utilizes foundation models to test individual components, aiding in pinpointing modules for development to enhance overall results.\n",
    "\n",
    "`LlamaIndex` is a framework for building LLM applications. It simplifies data integration from various sources and provides tools for data indexing, engines, agents, and application integrations. Optimized for search and retrieval, it streamlines querying LLMs and retrieving documents. This blog post focuses on using its Observability/Evaluation modules.\n",
    "\n",
    "`LangChain` is an open-source framework that simplifies the creation of applications powered by foundation models. It provides tools for chaining LLM operations, managing context, and integrating external data sources. LangChain is primarily used for building chatbots, question-answering systems, and other AI-driven applications that require complex language processing capabilities.\n",
    "\n",
    "The solution consists of the following components:\n",
    "\n",
    "1. `Evaluation dataset` – The source data for the RAG comes from the Amazon SageMaker FAQ, which represents 170 question-answer pairs. This corresponds to Step 1 in the architecture diagram.\n",
    "2. `Build sample RAG` – Documents are segmented into chunks and stored in an Amazon Bedrock Knowledge Bases (Steps 2–4). We use Langchain Retrieval Q&A to answer user queries. This process retrieves relevant data from an index at runtime and passes it to the Foundation Model (FM).\n",
    "3. `RAG evaluation` – To assess the quality of the Retrieval-Augmented Generation (RAG) solution, we can use both RAGAS and LlamaIndex. An LLM performs the evaluation by comparing its predictions with ground truths (Steps 5–6).\n",
    "\n",
    "### Prerequisites\n",
    "To implement this solution, you need the following:\n",
    "\n",
    "An AWS account with privileges to create AWS Identity and Access Management (`IAM`) roles and policies. For more information, see Overview of access management: Permissions and policies.\n",
    "Access enabled for the `Amazon Titan Embeddings G1 – Text model and Anthropic Claude 3 Sonnet` on `Amazon Bedrock`. For instructions, see Model access.\n",
    "Run the prerequisite code provided in the Python\n",
    "\n",
    "### Evaluation of RAG with RAGAS\n",
    "\n",
    "Evaluating the RAG solution requires to compare LLM predictions with ground truth answers. To do so, we use the batch() function from LangChain to perform inference on all questions inside our evaluation dataset.\n",
    "\n",
    "Then we can use the evaluate() function from RAGAS to perform evaluation on each metric (answer relevancy, faithfulness and answer corectness). It uses an LLM to compute metrics. Feel free to use other Metrics from RAGAS.\n",
    "\n",
    "### Evaluation of RAG with LlamaIndex\n",
    "LlamaIndex, similar to Ragas, provides a comprehensive RAG (Retrieval-Augmented Generation) evaluation module. This module offers a variety of metrics to assess the performance of your RAG system. The evaluation process generates two key outputs:\n",
    "\n",
    "1. `Feedback`: The judge LLM (Language Model) provides detailed evaluation feedback in the form of a string, offering qualitative insights into the system’s performance.\n",
    "2. `Score`: This numerical value indicates how well the answer meets the evaluation criteria. The scoring system varies depending on the specific metric being evaluated. For example, metrics like Answer Relevancy and Faithfulness are typically scored on a scale from 0 to 1.\n",
    "These outputs allow for both qualitative and quantitative assessment of your RAG system’s performance, enabling you to identify areas for improvement and track progress over time.\n",
    "\n",
    "### Chunking\n",
    "1. Standard chunking\n",
    "\n",
    "Amazon Bedrock supports the following standard approaches to chunking:\n",
    "\n",
    "Fixed-size chunking: You can configure the desired chunk size by specifying the number of tokens per chunk, and an overlap percentage, providing flexibility to align with your specific requirements. You can set the maximum number of tokens that must not exceed for a chunk and the overlap percentage between consecutive chunks.\n",
    "\n",
    "Default chunking: Splits content into text chunks of approximately 300 tokens. The chunking process honors sentence boundaries, ensuring that complete sentences are preserved within each chunk.\n",
    "\n",
    "2. Hierarchical chunking\n",
    "\n",
    "Hierarchical chunking involves organizing information into nested structures of child and parent chunks. When creating a data source, you are able to define the parent chunk size, child chunk size and the number of tokens overlapping between each chunk. During retrieval, the system initially retrieves child chunks, but replaces them with broader parent chunks so as to provide the model with more comprehensive context.\n",
    "\n",
    "3. Semantic chunking\n",
    "\n",
    "Semantic chunking is a natural language processing technique that divides text into meaningful chunks to enhance understanding and information retrieval. It aims to improve retrieval accuracy by focusing on the semantic content rather than just syntactic structure. By doing so, it may facilitate more precise extraction and manipulation of relevant information.\n",
    "\n",
    "> https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking.html#kb-hiearchical-chunking\n",
    "\n",
    "### RAG evaluation concepts and metrics\n",
    "> https://aws.amazon.com/blogs/machine-learning/evaluate-the-reliability-of-retrieval-augmented-generation-applications-using-amazon-bedrock/\n",
    "\n",
    "As mentioned previously, RAG-based generative AI application is composed of two main processes: retrieval and generation. Retrieval is the process where the application uses the user query to retrieve the relevant documents from a knowledge base before adding it to as context augmenting the final prompt. Generation is the process of generating the final response from the LLM. It’s important to monitor and evaluate both processes because they impact the performance and reliability of the application.\n",
    "\n",
    "Evaluating RAG systems at scale requires an automated approach to extract metrics that are quantitative indicators of its reliability. Generally, the metrics to look for are grouped by main RAG components or by domains. Aside from the metrics discussed in this section, you can incorporate tailored metrics that align with your business objectives and priorities.\n",
    "\n",
    "1. `Retrieval metrics`\n",
    "You can use the following retrieval metrics:\n",
    "    - `Context relevance` – This measures whether the passages or chunks retrieved by the RAG system are relevant for answering the given query, without including extraneous or irrelevant details. The values range from 0–1, with higher values indicating better context relevancy.\n",
    "    - `Context recall` – This measures the alignment between the context and the expected RAG output, the ground truth. Similar to faithfulness, each statement in the ground truth is checked to see if it is attributed to the context (thereby evaluating the context).\n",
    "    - `Context precision` – This evaluates the relevancy of the context to the answer, or in other words, the retriever’s ability to capture the best context to answer your query. An LLM verifies if the information in the given context is directly relevant to the question with a single “Yes” or “No” response. The context is passed in as a list, so if the list is size one (one chunk), then the metric for context precision is either 0 (representing the context isn’t relevant to the question) or 1 (representing that it is relevant). If the context list is greater than one (or includes multiple chunks), then context precision is between 0–1, representing a specific weighted average precision calculation. This involves the context precision of the first chunk being weighted heavier than the second chunk, which itself is weighted heavier than the third chunk, and onwards, taking into account the ordering of the chunks being outputted as contexts.\n",
    "\n",
    "2. `Generation metrics`\n",
    "You can use the following generation metrics:\n",
    "    - `Faithfulness` – This measures the factual consistency of the generated answer against the given context, so it requires the answer and retrieved context as an input. This is a two-step prompt where the generated answer is first broken down into multiple standalone statements and propositions. Then, the evaluation LLM validates the attribution of the generated statement to the context. If the attribution can’t be validated, it’s assumed that the statement is at risk of hallucination. The answer is scaled to a 0–1 range; the higher the better.\n",
    "    - `Answer relevance` – This focuses on how pertinent the generated RAG output (answer) is to the question. A lower score is assigned to answers that are incomplete or contain redundant information. To calculate this score, the LLM is asked to generate multiple questions from a given answer. Then using an Amazon Titan Embeddings model, embeddings are generated for the generated question and the actual question. The metric therefore is the mean cosine similarity between all the generated questions and the actual question.\n",
    "    - `Answer semantic similarity` – It compares the meaning and content of a generated answer with a reference or ground truth answer. It evaluates how closely the generated answer matches the intended meaning of the ground truth answer. The score ranges from 0–1, with higher scores indicating greater semantic similarity between the two answers. A score of 1 means that the generated answer conveys the same meaning as the ground truth answer, whereas a score of 0 suggests that the two answers have completely different meanings. This assesses the semantic similarity between the RAG output (answer) and expected answer (ground truth), with a range between 0–1. A higher score signifies better performance. First, the embeddings of answer and ground truth are created, and then a score between 0–1 is predicted, representing the semantic similarity of the embeddings using a cross encoder Tiny BERT model.\n",
    "    - `Answer correctness` – This is the accuracy between the generated answer and the ground truth. This is calculated from the semantic similarity metric between the answer and the ground truth in addition to a factual similarity by looking at the context. A threshold value is used if you want to employ a binary 0 or 1 answer correctness score, otherwise a value between 0–1 is generated.\n",
    "\n",
    "3. `Aspects evaluation`\n",
    "Aspects are evaluated as follows:\n",
    "    - `Harmfulness` (Yes, No) – If the generated answer carries the risk of causing harm to people, communities, or more broadly to society\n",
    "    - `Maliciousness` (Yes, No) – If the submission intends to harm, deceive, or exploit users\n",
    "    - `Coherence` (Yes, No) – If the generated answer presents ideas, information, or arguments in a logical and organized manner\n",
    "    - `Correctness` (Yes, No) – If the generated answer is factually accurate and free from errors\n",
    "    - `Conciseness` (Yes, No) – If the submission conveys information or ideas clearly and efficiently, without unnecessary or redundant details\n",
    "\n",
    "`Generator quality` can be assessed through several key metrics. `Context utilization` examines how effectively the generator uses relevant information from the provided source material. `Noise sensitivity` gauges the generator’s propensity to include inaccurate details from the retrieved content. `Hallucination` measures the extent to which the generator produces incorrect claims not present in the source data. `Self-knowledge` reflects the proportion of accurate statements generated that can’t be found in the retrieved chunks. Finally, `faithfulness` evaluates how closely the generator’s output aligns with the information contained in the source material.\n",
    "\n",
    "For measuring the `overall generation quality`, the key metrics include measuring the `precision`, `recall`, and `answer similarity`. `Precision` suggests the proportion of the correct claims in model’s response, whereas `recall` suggests the proportion of the ground truth claims covered by the model’s response. `Answer similarity` compares the meaning and content of a generated answer with a reference or ground truth answer. It evaluates how closely the generated answer matches the intended meaning of the ground truth answer.\n",
    "\n",
    "Establishing a feedback loop with an evaluation framework against these quality metrics allows for continuous improvement, where the system can learn from user interactions and refine its performance over time. By optimizing these quality metrics, the RAG system can be designed to deliver reliable, cost-effective, and high-performing results for users.\n",
    "\n",
    "### Responsible AI\n",
    "Implementing responsible AI practices is crucial for maintaining ethical and safe deployment of RAG systems. This includes using guardrails to filter harmful content, deny certain topics, mask sensitive information, and ground responses in verified sources to reduce hallucinations.\n",
    "\n",
    "You can use Amazon Bedrock Guardrails for implementing responsible AI policies. Along with protecting against toxicity and harmful content, it can also be used for Automated Reasoning checks, which helps you protect against hallucinations.\n",
    "\n",
    "### Cost and latency\n",
    "Cost considers the compute resources and infrastructure required to run the system, and latency evaluates the response times experienced by end-users. To `optimize cost and latency`, implement `caching strategies` to reduce the need for expensive model inferences. Efficient `query batching` can also improve overall `throughput` and reduce resource usage. Balance performance and resource usage to find the ideal configuration that meets your application’s requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f23df2-dd68-42be-8609-85d6ad268f21",
   "metadata": {},
   "source": [
    "### Hosting and scaling\n",
    "When it comes to hosting your web application or service, there are several approaches to consider. The key is to choose a solution that can effectively host your database and compute infrastructure. This could include server-based options like Amazon Elastic Compute Cloud (Amazon EC2), managed services like Amazon Relational Database Service (Amazon RDS) and Amazon DynamoDB, or serverless approaches such as AWS Amplify and Amazon Elastic Container Service (Amazon ECS). For a practical approach to building an automated AI assistant using Amazon ECS, see Develop a fully automated chat-based assistant by using Amazon Bedrock agents and knowledge bases.\n",
    "\n",
    "In addition to the server or compute layer, you will also need to consider an orchestration tool, testing environments, and a continuous integration and delivery (CI/CD) pipeline to streamline your application deployment. Having a feedback loop established based on the quality metrics along with a CI/CD pipeline is an important first step to creating self-healing architectures.\n",
    "\n",
    "As your application grows, you will need to make sure your infrastructure can scale to meet the increasing demand. This can involve containerization with Docker or choosing serverless options, implementing load balancing, setting up auto scaling, and choosing between on-premises, cloud, or hybrid solutions. It also includes unique scaling requirements of your frontend application and backend generative AI workflow, as well as the use of content delivery networks (CDNs) and disaster recovery and backup strategies.\n",
    "\n",
    "The following is a sample architecture for a secure and scalable RAG-based web application. This architecture uses Amazon ECS for hosting the service, Amazon CloudFront as a CDN, AWS WAF as a firewall, and Amazon MemoryDB for providing a semantic cache.\n",
    "\n",
    "![title](pic/POC-TO-Productin-Blog-Architecture.jpeg)\n",
    "\n",
    "### Data privacy, security, and observability\n",
    "Maintaining data privacy and security is of utmost importance. This includes implementing security measures at each layer of your application, from encrypting data in transit to setting up robust authentication and authorization controls. It also involves focusing on compute and storage security, as well as network security. Compliance with relevant regulations and regular security audits are essential. Securing your generative AI system is another crucial aspect. By default, Amazon Bedrock Knowledge Bases encrypts the traffic using AWS managed AWS Key Management Service (AWS KMS) keys. You can also choose customer managed KMS keys for more control over encryption keys. For more information on application security, refer to Safeguard a generative AI travel agent with prompt engineering and Amazon Bedrock Guardrails.\n",
    "\n",
    "Comprehensive logging, monitoring, and maintenance are crucial to maintaining a healthy infrastructure. This includes setting up structured logging, centralized log management, real-time monitoring, and strategies for system updates and migrations.\n",
    "\n",
    "By addressing these critical areas, you can build a secure and resilient infrastructure to support your growing web application or service. Stay tuned for more in-depth coverage of these topics in upcoming blog posts.\n",
    "\n",
    "By using purpose-built tools like Amazon Bedrock Knowledge Bases to streamline the end-to-end RAG workflow, organizations can successfully transition their RAG-powered proofs of concept into high-performing, cost-effective, secure production-ready solutions that deliver business value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82949f-fcfa-4d05-8d5a-4c08d30ebb6e",
   "metadata": {},
   "source": [
    "![title](pic/multilingual_bedrock_figure1.png)\n",
    "\n",
    "The workflow includes the following steps:\n",
    "1. The user sends a prompt for querying the documents to the REST API.\n",
    "2. Amazon API Gateway sends the user prompt as an event to a Lambda function.\n",
    "3. The Lambda function invokes Amazon Bedrock API and sends the prompt to the Anthropic Claude 3 Sonnet model.\n",
    "4. The LLM parses the prompt and does a similarity search with vector embeddings.\n",
    "5. Enhanced context from the knowledge base is used to generate a text response.\n",
    "6. The final text response is returned by Amazon Bedrock to the Lambda function.\n",
    "7. The user receives the final response through RESI API.\n",
    "\n",
    "This is an event-driven architecture composed of individual AWS services that are loosely integrated with each other, with each service handling a specific function. It uses AWS serverless technologies, allowing you to build and run your application without having to manage your own servers. All server management is done by AWS, providing many benefits such as automatic scaling and built-in high availability, letting you take your idea to production quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bfe52b-73ca-4239-97fb-2d2467bdc46f",
   "metadata": {},
   "source": [
    "1. `Data Store Seleciton (Decision Criteria)`\n",
    "\n",
    "A. Data Type & Structure\n",
    "  - Unstructured Data (e.g., images, videos): Use S3.\n",
    "  - Key-Value Pairs (e.g., user profiles): Use DynamoDB or ElastiCache.\n",
    "  - Relational Data (e.g., orders, transactions): Use Aurora or RDS.\n",
    "  - Time-Series (e.g., IoT sensor data): Use Timestream or Keyspaces.\n",
    "  - Vector Embeddings (RAG): Use OpenSearch (k-NN plugin) or Aurora (pgvector).\n",
    "\n",
    "B. Access Patterns\n",
    "High Read/Write Throughput:\n",
    "  - Use DynamoDB (up to 100K+ requests/sec) or ElastiCache (for caching).\n",
    "  - Low Latency (Microseconds):\n",
    "  - Use ElastiCache (Redis).\n",
    "  - Complex Queries (Joins, Aggregations):\n",
    "  - Use Aurora or Redshift (analytics).\n",
    "\n",
    "C. Scalability\n",
    "  - Auto-Scaling: DynamoDB (on-demand mode) or S3 (infinite storage).\n",
    "  - Manual Scaling: RDS/Aurora (requires read replicas) or ElastiCache (resize clusters).\n",
    "\n",
    "D. Cost\n",
    "  - Lowest Storage Cost: S3 (e.g., $0.023/GB for Standard Tier).\n",
    "  - Pay-Per-Request: DynamoDB On-Demand or Keyspaces.\n",
    "  - In-Memory Performance: ElastiCache (higher cost but ultra-fast).\n",
    "\n",
    "When to Use Each Service\n",
    "  - Amazon S3\n",
    "    Use Cases: Static website hosting, data lakes, ML training datasets.\n",
    "    Backup/archival (with S3 Glacier).\n",
    "  - DynamoDB\n",
    "    Use Cases: Serverless apps, real-time dashboards, session stores. Metadata storage for S3 objects (e.g., file attributes).\n",
    "  - ElastiCache (Redis)\n",
    "    Use Cases: Caching LLM responses in RAG to reduce latency. Real-time leaderboards, rate limiting.\n",
    "  - Amazon Aurora\n",
    "    Use Cases: Transactional systems requiring SQL joins. RAG with structured metadata + vector search (via pgvector).\n",
    "\n",
    "Hybrid Architectures\n",
    "\n",
    "Combine services for optimal performance: \n",
    "  - S3 + DynamoDB: Store large files in S3, metadata in DynamoDB.\n",
    "  - ElastiCache + Aurora: Cache frequent SQL query results in Redis.\n",
    "  - OpenSearch + S3: Index documents in OpenSearch, store raw files in S3.\n",
    "\n",
    "`S3 (documents) → OpenSearch (vector index) → ElastiCache (cache) → DynamoDB (user metadata)`\n",
    "\n",
    "Key Takeaways\n",
    "  - Start Simple: Use DynamoDB for serverless apps, S3 for files.\n",
    "  - Prioritize Latency: Use ElastiCache for microsecond responses.\n",
    "\n",
    "RAG-Specific:\n",
    "  - Vector search: OpenSearch or Aurora (pgvector).\n",
    "  - Raw documents: S3.\n",
    "  - Caching: ElastiCache.\n",
    "  - Cost Optimization: Use S3 Intelligent-Tiering for unpredictable access.\n",
    "    Enable DynamoDB Auto-Scaling to avoid overprovisioning.\n",
    "\n",
    "\n",
    "\n",
    "2. `EKS, ECS`\n",
    "- Why Use ECR in RAG?\n",
    "ECR serves as the backbone for securely storing and managing container images required for RAG components (e.g., retrieval services, LLM APIs, vector databases).\n",
    "Key Pros:\n",
    "\n",
    "Tight AWS Integration: Seamlessly works with ECS/EKS for image deployment and IAM-based access control, ensuring secure image pulls 29.\n",
    "\n",
    "Security & Compliance: Built-in vulnerability scanning and encryption (at rest/in transit) for container images, critical for AI/ML workloads handling sensitive data 36.\n",
    "\n",
    "Lifecycle Management: Automatically clean unused images to reduce costs and maintain efficiency 39.\n",
    "\n",
    "High Availability: Replicates images across AWS Availability Zones, ensuring reliability for production-grade RAG systems 9.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Cost for Large Repositories: Storage and data-transfer fees can add up for teams with frequent image updates or large datasets 310.\n",
    "\n",
    "Limited IPv6 Support: May require workarounds if IPv6 is essential for your network setup 10.\n",
    "\n",
    "- Why Use ECS in RAG?\n",
    "ECS simplifies deploying and scaling containerized RAG components (e.g., retrieval APIs, generative models).\n",
    "Key Pros:\n",
    "\n",
    "Serverless with Fargate: Avoid managing servers, reducing operational overhead for bursty RAG workloads 14.\n",
    "\n",
    "AWS Service Integration: Directly integrates with ALB, CloudWatch, and RDS for streamlined monitoring and database connectivity 19.\n",
    "\n",
    "Simplified Scaling: Auto Scaling and load balancing ensure dynamic resource allocation for fluctuating query demands 49.\n",
    "\n",
    "Cost Efficiency: No control-plane fees (unlike EKS), making it cheaper for smaller teams 4.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Limited Flexibility: Less control over infrastructure compared to Kubernetes (e.g., no native pod-level resource sharing) 14.\n",
    "\n",
    "Vendor Lock-in: Tight coupling with AWS services complicates multi-cloud or hybrid deployments 17.\n",
    "\n",
    "- Combined Benefits of ECR + ECS in RAG\n",
    "Streamlined Workflow: Push RAG component images to ECR and deploy via ECS tasks/services without managing external registries 29.\n",
    "\n",
    "Security Synergy: IAM roles in ECS securely pull images from ECR, reducing credential exposure risks 210.\n",
    "\n",
    "Scalability: ECS auto-scaling pairs with ECR’s high-throughput image distribution to handle traffic spikes 49.\n",
    "\n",
    "- Implementation Example\n",
    "ECR Setup:\n",
    "\n",
    "Create a private repository for RAG components (e.g., rag-llm-api).\n",
    "\n",
    "Enable scan-on-push and lifecycle policies to manage image versions 39.\n",
    "\n",
    "ECS Deployment:\n",
    "\n",
    "Define ECS tasks referencing ECR images for retrieval and generation services.\n",
    "\n",
    "Use Fargate for serverless scaling or EC2 for GPU-optimized LLM inference 17.\n",
    "\n",
    "CI/CD Integration:\n",
    "\n",
    "Automate image builds/pushes to ECR via GitHub Actions and deploy updates to ECS 7.\n",
    "\n",
    "3. `platform for different IA provider`\n",
    "4. `Metrics`\n",
    "5. `Retriever and Reranker (bi-encoder, cross-encoder)`\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"display: flex;\">\n",
    "    <img src=\"pic/cross_encoder.png\" style=\"width:50%; margin:5px;\">\n",
    "    <img src=\"pic/rerank.png\" style=\"width:50%; margin:5px;\">\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "<!-- ![title](pic/cross_encoder.png) -->\n",
    "<!-- <img src=\"pic/cross_encoder.png\" alt=\"title\" style=\"width:50%; height:auto;\"> -->\n",
    "\n",
    "<!-- ![title](pic/rerank.png) -->\n",
    "<!-- <img src=\"pic/rerank.png\" alt=\"title\" style=\"width:50%; height:auto;\"> -->\n",
    "\n",
    "> https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings2/\n",
    "\n",
    "Sentence Transformers supports two types of models: Bi-encoders and Cross-encoders. Bi-encoders are faster and more scalable, but cross-encoders are more accurate. Although both tackle similar high-level tasks, when to use one versus the other is quite different. Bi-encoders are better for search, and cross-encoders are better for classification and high-accuracy ranking. Let’s dive into the details!\n",
    "\n",
    "Bi-encoders are models that encode the input text into a fixed-length vector. When you compute the similarity between two sentences, we usually encode the two sentences into two vectors and then compute the similarity between the two vectors (e.g., by using cosine similarity). We train bi-encoders to optimize the increase in the similarity between the query and relevant sentences and decrease the similarity between the query and the other sentences. This is why bi-encoders are better suited for search. As the previous blog post showed, bi-encoders are fast and easily scalable. If multiple sentences are provided, the bi-encoder will encode each sentence independently. This means that the sentence embeddings are independent of each other. This is a good thing for search, as we can encode millions of sentences in parallel. However, this also means that the bi-encoder doesn’t know anything about the relationship between the sentences.\n",
    "\n",
    "When we use cross-encoders, we do something different. Cross-encoders encode the two sentences simultaneously and then output a classification score. The figure below shows the high-level differences.\n",
    "\n",
    "Why would you use one versus the other? Cross-encoders are slower and more memory intensive but also much more accurate. A cross-encoder is an excellent choice to compare a few dozen sentences. If you want to compare hundreds of thousands of sentences, a bi-encoder is a better choice, as otherwise a cross-encoder could take multiple hours. What if you care about accuracy and want to compare thousands of sentences efficiently? This is a typical case when you want to retrieve information. In those cases, an option is first to use a bi-encoder to reduce the number of candidates (i.e., get the top 20 most relevant examples) and then use a cross-encoder to get the final result. This is called re-ranking and is a common technique in information retrieval\n",
    "\n",
    "As mentioned, cross-encoders encode two texts simultaneously and then output a classification label. The cross-encoder first generates a single embedding that captures representations and their relationships. Compared to bi-encoder-generated embeddings (which are independent of each other), cross-encoder embeddings are dependent on each other. This is why cross-encoders are better suited for classification, and their quality is higher: they can capture the relationship between the two sentences! On the flip side, cross-encoders are slow if you need to compare thousands of sentences since they need to encode all the sentence pairs.\n",
    "\n",
    "Let’s say you have four sentences, and you need to compare all the possible pairs:\n",
    "\n",
    "A bi-encoder would need to encode each sentence independently, so it would need to encode four sentences.\n",
    "A cross-encoder would need to encode all the possible pairs, so it would need to encode six sentences (AB, AC, AD, BC, BD, CD).\n",
    "Let’s scale this. Let’s say you have 100,000 sentences, and you need to compare all the possible pairs:\n",
    "\n",
    "A bi-encoder would encode 100,000 sentences.\n",
    "A cross-encoder would encode 4,999,950,000 pairs! (Using the combinations formula: n! / (r!(n-r)!), where n=100,000 and r=2). No wonder they don’t scale well!\n",
    "Hence, it makes sense they are slower!\n",
    "\n",
    "\n",
    "6. `ParentDocumentRetriever, parent_splitter and child_splitter`\n",
    "\n",
    "7. `Airflow VS. MLflow`\n",
    "\n",
    "`Airflow DAG → MLflow Model → FastAPI (LLM) → OpenSearch (Retrieval)`\n",
    "\n",
    "1. Apache Airflow in RAG\n",
    "Purpose:\n",
    "Orchestrate and automate data/ML pipelines (e.g., document ingestion, embedding updates, model retraining).\n",
    "\n",
    "Use Cases:\n",
    "Schedule periodic data ingestion from S3/APIs into your vector DB.\n",
    "\n",
    "Retrain embedding models or update FAISS/OpenSearch indices.\n",
    "\n",
    "Monitor and restart failed pipeline components (e.g., broken API calls to LLMs).\n",
    "\n",
    "\n",
    "Key Features:\n",
    "Directed Acyclic Graphs (DAGs) for workflow dependencies.\n",
    "\n",
    "Integrations: AWS Lambda, ECS, Kubernetes, OpenSearch.\n",
    "\n",
    "Monitoring: Built-in UI for task status, retries, and logs.\n",
    "\n",
    "2. MLflow in RAG\n",
    "Purpose:\n",
    "Manage the machine learning lifecycle (experiment tracking, model registry, deployment).\n",
    "\n",
    "Use Cases:\n",
    "Track experiments with different LLMs/embedding models.\n",
    "\n",
    "Version and deploy fine-tuned models (e.g., custom sentence-transformers).\n",
    "\n",
    "Compare retrieval performance (e.g., recall@k for vector search).\n",
    "\n",
    "Key Features:\n",
    "Experiment Tracking: Log parameters, metrics, and artifacts.\n",
    "\n",
    "Model Registry: Stage models (Staging/Production) with versioning.\n",
    "\n",
    "Deployment: Serve models via REST API or batch inference.\n",
    "\n",
    "4. How They Complement Each Other\n",
    "Combined RAG Pipeline:\n",
    "Airflow schedules daily tasks:\n",
    "\n",
    "Ingests new documents → generates embeddings → updates OpenSearch.\n",
    "\n",
    "Triggers model retraining if data drift is detected.\n",
    "\n",
    "MLflow manages the ML side:\n",
    "\n",
    "Tracks embedding model performance during retraining.\n",
    "\n",
    "Deploys the best model to an API endpoint for real-time inference.\n",
    "\n",
    "5. When to Use Each\n",
    "Use Airflow If:\n",
    "\n",
    "You need to automate multi-step workflows (e.g., ingest → embed → index).\n",
    "\n",
    "Your RAG system requires cron-like scheduling and error handling.\n",
    "\n",
    "Use MLflow If:\n",
    "\n",
    "You’re experimenting with multiple LLMs/embedding models.\n",
    "\n",
    "You need version control for models and reproducibility.\n",
    "\n",
    "6. Final Recommendation\n",
    "For a production RAG system:\n",
    "\n",
    "Use Airflow to automate data/retrieval pipelines.\n",
    "\n",
    "Use MLflow to track/model embedding models and LLMs.\n",
    "\n",
    "Combine both for end-to-end traceability (e.g., Airflow triggers MLflow runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb27365-4f77-401f-afe4-01b1a824544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[User Request] → CloudFront → API Gateway (WAF) → ALB → EKS (Orchestrator)\n",
    "               ↗         ↘                ↖       ↓\n",
    "           Lambda (Caching)          SQS (Priority Queues)\n",
    "               ↓                        ↘\n",
    "[VPC] → SageMaker (BGE Retriever) → OpenSearch (k-NN) → DAX Cache\n",
    "               ↓                          ↖\n",
    "[VPC] → SageMaker (Llama3 Generator)      DynamoDB (Feedback)\n",
    "               ↓                          ↗\n",
    "           CloudWatch/X-Ray         Step Functions (Human Eval)\n",
    "\n",
    "\n",
    "[User Request] → CloudFront (Global Cache)\n",
    "               → API Gateway (Rate Limiting)\n",
    "               → AWS Lambda (Orchestrator)\n",
    "               → SQS (Decouple Retriever & Generator)\n",
    "               ↗                        ↘\n",
    "[Retriever (SageMaker + OpenSearch)]  [Generator (SageMaker LLM)]\n",
    "               ↖                        ↙\n",
    "[S3 (10B PDFs)] → Glue (Catalog) → Textract/EC2 Batch (PDF Processing)\n",
    "\n",
    "\n",
    "User Query:\n",
    "\n",
    "Request → CloudFront (cached responses) → API Gateway → ALB → EKS (orchestrator).\n",
    "\n",
    "Retrieval:\n",
    "\n",
    "EKS calls BGE endpoint → OpenSearch → DAX (cache hit/miss).\n",
    "\n",
    "Generation:\n",
    "\n",
    "EKS sends top-3 chunks + query to Llama3 endpoint.\n",
    "\n",
    "Evaluation:\n",
    "\n",
    "Answer logged in S3 → Bedrock evaluation → Human feedback via Ground Truth.\n",
    "\n",
    "Retraining:\n",
    "\n",
    "Low-quality triggers trigger SageMaker Pipelines to fine-tune BGE/Llama3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9568d8ca-0b32-4c9d-a4ee-2468922f3b6a",
   "metadata": {},
   "source": [
    "`A NAT Gateway (Network Address Translation Gateway)` is a managed AWS service that allows instances in a private subnet to connect to the internet (e.g., for updates, API calls) while blocking unsolicited inbound traffic. It acts as a bridge between private subnets and the public internet. Outbound-Only Internet Access\n",
    "Private instances (e.g., databases, backend servers) can initiate requests to the internet.\n",
    "External entities cannot initiate connections to these instances.\n",
    "\n",
    "1. Secure Infrastructure\n",
    "VPC Design:\n",
    "\n",
    "Deploy SageMaker, OpenSearch, EKS, and RDS in a multi-AZ VPC with private subnets.\n",
    "\n",
    "Use NAT Gateways for outbound traffic; no public IPs for backend services.\n",
    "\n",
    "Encryption:\n",
    "\n",
    "Encrypt all data (S3, OpenSearch, EBS) with AWS KMS (customer-managed keys).\n",
    "\n",
    "Use TLS 1.3 for data in transit via API Gateway and ALB.\n",
    "\n",
    "Network Security:\n",
    "\n",
    "AWS WAF on API Gateway to block SQLi/abusive requests.\n",
    "\n",
    "PrivateLink for SageMaker, OpenSearch, and S3 to avoid public internet exposure.\n",
    "\n",
    "1. Data Ingestion & Preprocessing\n",
    "Storage:\n",
    "\n",
    "Store 10B PDFs in S3 with versioning and lifecycle policies (e.g., tier to Glacier for archived docs).\n",
    "\n",
    "Use S3 Batch Operations to process large volumes.\n",
    "\n",
    "PDF Extraction:\n",
    "\n",
    "Use Amazon Textract with AWS Batch (EC2 Spot Fleet) for parallel text extraction. For complex legal formats, deploy custom OCR containers on ECS/EKS.\n",
    "\n",
    "Chunking & Embeddings:\n",
    "\n",
    "Use AWS Glue to catalog extracted text.\n",
    "\n",
    "Chunk text into 512-token segments using Lambda or AWS Fargate.\n",
    "\n",
    "Generate embeddings with a SageMaker Batch Transform job (e.g., BAAI/bge-large-en model).\n",
    "\n",
    "Vector Database:\n",
    "\n",
    "Use Amazon OpenSearch Serverless (vector engine) for scalable storage of embeddings. For extreme scale, use Cassandra + Vector Plugin on EC2.\n",
    "\n",
    "Encrypt data with AWS KMS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee3a4f-7113-4b3f-bbf1-9175ac42fb58",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05042b6f-92f4-43f3-ae5b-0139a71f54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulumi-rag/\n",
    "├── Pulumi.yaml        # Project config\n",
    "├── __main__.py        # Main stack (Python)\n",
    "└── components/\n",
    "    ├── networking.py  # VPC components\n",
    "    └── llm.py         # SageMaker resources\n",
    "\n",
    "\n",
    "# components/opensearch.py\n",
    "import pulumi\n",
    "from pulumi_aws import opensearch\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, name: str, vpc_id: str):\n",
    "        self.collection = opensearch.ServerlessCollection(\n",
    "            f\"{name}-vectors\",\n",
    "            name=f\"{name}-vectors\",\n",
    "            type=\"VECTORSEARCH\"\n",
    "        )\n",
    "        \n",
    "        # Security policy with KMS encryption\n",
    "        opensearch.ServerlessSecurityPolicy(\n",
    "            f\"{name}-policy\",\n",
    "            policy=json.dumps({\n",
    "                \"Rules\": [{\n",
    "                    \"Resource\": [self.collection.arn],\n",
    "                    \"ResourceType\": \"collection\"\n",
    "                }],\n",
    "                \"AWSOwnedKey\": True\n",
    "            }),\n",
    "            type=\"encryption\"\n",
    "        )\n",
    "\n",
    "# Dynamic scaling based on input\n",
    "for model in [\"llama3-70b\", \"bge-large\"]:\n",
    "    sagemaker.Model(\n",
    "        f\"rag-{model}\",\n",
    "        instance_type=\"ml.g5.2xlarge\" if \"bge\" in model else \"ml.p4d.24xlarge\"\n",
    "    )\n",
    "\n",
    "# Type hints and autocomplete\n",
    "from pulumi_aws.sagemaker import Endpoint\n",
    "\n",
    "endpoint = Endpoint(\n",
    "    \"rag-endpoint\",\n",
    "    config_name=config.name,\n",
    "    tags={\n",
    "        \"Project\": \"Legal-RAG\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Cross-region DB replica\n",
    "from pulumi_aws import rds\n",
    "\n",
    "rds.Instance(\n",
    "    \"replica\",\n",
    "    replicate_source_db=primary_db.identifier,\n",
    "    availability_zone=\"us-west-2a\"\n",
    ")\n",
    "\n",
    "# 8. Monitoring as Code\n",
    "\n",
    "from pulumi_aws import cloudwatch\n",
    "\n",
    "cloudwatch.MetricAlarm(\n",
    "    \"high_latency\",\n",
    "    metric_name=\"ModelLatency\",\n",
    "    threshold=500,\n",
    "    comparison_operator=\"GreaterThanThreshold\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832606ca-e36a-49a1-bdbd-966b00b5698d",
   "metadata": {},
   "source": [
    "## 1. System Architecture\n",
    "The RAG pipeline consists of:\n",
    "\n",
    "Embedding Model (e.g., BAAI/bge-small-en)\n",
    "→ Converts queries/documents into vectors.\n",
    "\n",
    "Vector Database (e.g., Pinecone, FAISS, AWS OpenSearch)\n",
    "→ Stores & retrieves relevant chunks.\n",
    "\n",
    "Generation Model (e.g., Llama 2, GPT-3.5)\n",
    "→ Generates answers using retrieved context.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[User Query] --> B[Embedding Model]\n",
    "    B --> C[Vector DB Retrieval]\n",
    "    C --> D[Generation Model]\n",
    "    D --> E[Final Answer]\n",
    "```\n",
    "\n",
    "## 2. Deploying Models on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f8ee0-a7a2-4906-9701-4f8c4122ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A) Deploy Embedding Model\n",
    "\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Deploy Embedding Model (e.g., BAAI/bge-small-en)\n",
    "embedding_model = HuggingFaceModel(\n",
    "    model_data=\"s3://my-bucket/models/bge-small-en.tar.gz\",  # Custom model .tar.gz\n",
    "    role=role,\n",
    "    transformers_version=\"4.28.1\",\n",
    "    pytorch_version=\"2.0.0\",\n",
    "    entry_script=\"embedding_inference.py\",  # Custom inference script\n",
    ")\n",
    "\n",
    "embedding_predictor = embedding_model.deploy(\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=\"embedding-model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6705c48-6f9f-4045-939e-78cf61a0b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Script (embedding_inference.py):\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModel.from_pretrained(model_dir)\n",
    "    return {\"tokenizer\": tokenizer, \"model\": model}\n",
    "\n",
    "def predict_fn(data, model_dict):\n",
    "    inputs = model_dict[\"tokenizer\"](data[\"text\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_dict[\"model\"](**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).tolist()  # Return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0c871-c0a5-4aa6-9983-905ad2893c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B) Deploy Generation Model\n",
    "\n",
    "# Deploy Llama 2 (HuggingFace Hub)\n",
    "generation_model = HuggingFaceModel(\n",
    "    model_data=\"s3://my-bucket/models/llama-2-7b.tar.gz\",\n",
    "    role=role,\n",
    "    transformers_version=\"4.30.2\",\n",
    "    pytorch_version=\"2.0.0\",\n",
    "    entry_script=\"generation_inference.py\",\n",
    ")\n",
    "\n",
    "generation_predictor = generation_model.deploy(\n",
    "    instance_type=\"ml.g5.4xlarge\",  # Large GPU for LLM\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=\"generation-model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70efea-081d-4192-84de-6d6371d6713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Script (generation_inference.py)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "    return {\"tokenizer\": tokenizer, \"model\": model}\n",
    "\n",
    "def predict_fn(data, model_dict):\n",
    "    inputs = model_dict[\"tokenizer\"](data[\"prompt\"], return_tensors=\"pt\")\n",
    "    outputs = model_dict[\"model\"].generate(**inputs, max_new_tokens=100)\n",
    "    return model_dict[\"tokenizer\"].decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ff021-4681-4ce8-a863-8be7fabb2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Set Up Vector Database\n",
    "\n",
    "# Use Pinecone (Serverless) or FAISS (Self-hosted)\n",
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=\"YOUR_API_KEY\", environment=\"us-west1-gcp\")\n",
    "index = pinecone.Index(\"rag-index\")\n",
    "\n",
    "# Store embeddings (run once)\n",
    "index.upsert([(\"doc1\", [0.1, 0.2, ...]), (\"doc2\", [0.3, 0.4, ...])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a8f2e-fabb-4c64-b58a-7b92feca493b",
   "metadata": {},
   "source": [
    "## 3. Orchestrating the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846b397-2d73-4587-9a2c-3ae44f67b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy a Lambda + API Gateway to connect all components\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sagemaker = boto3.client(\"sagemaker-runtime\")\n",
    "pinecone = boto3.client(\"pinecone\")\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Step 1: Get query embedding\n",
    "    query = event[\"query\"]\n",
    "    embedding_response = sagemaker.invoke_endpoint(\n",
    "        EndpointName=\"embedding-model\",\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps({\"text\": query})\n",
    "    )\n",
    "    query_embedding = json.loads(embedding_response[\"Body\"].read())\n",
    "\n",
    "    # Step 2: Retrieve relevant docs from Vector DB\n",
    "    retrieved_docs = pinecone.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=3,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    # Step 3: Generate answer using LLM\n",
    "    context = \" \".join([doc[\"metadata\"][\"text\"] for doc in retrieved_docs[\"matches\"]])\n",
    "    prompt = f\"Answer based on: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    generation_response = sagemaker.invoke_endpoint(\n",
    "        EndpointName=\"generation-model\",\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps({\"prompt\": prompt})\n",
    "    )\n",
    "    answer = json.loads(generation_response[\"Body\"].read())\n",
    "\n",
    "    return {\"answer\": answer}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
