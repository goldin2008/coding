import re
from datetime import datetime
import pandas as pd
from typing import Dict, Tuple, Optional

import sys
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# Now import config
from config.config import JUDGE_MODELS, MOCK_JUDGE_MODELS


def build_evaluation_prompt(prompt: str, output: str) -> str:
    """Construct evaluation prompt given original prompt and model-generated output.
    
    Args:
        prompt: Original prompt given to the model
        output: Model-generated output to evaluate
        
    Returns:
        Formatted evaluation prompt
    """
    return f"""
You are an evaluation assistant. Given a prompt and a model-generated answer, please assess the quality of the answer based on:
- Clarity (1-5)
- Conciseness (1-5)
- Completeness (1-5)
Provide a score for each, then write a short summary comment.

Prompt:
{prompt}

Model-Generated Answer:
{output}
"""


def build_judge_prompt(human_narrative: str, llm_narrative: str) -> str:
    """Builds a prompt for evaluating a human vs. LLM-generated narrative.

    Parameters:
        human_narrative: Narrative written by a human
        llm_narrative: Narrative generated by the LLM

    Returns:
        Prompt to be sent to an LLM to evaluate the quality of both narratives.
    """
    return f"""
You are an evaluation assistant. You will compare two narratives explaining a risk score for a financial model. 
The first narrative is from a human expert, and the second narrative is generated by a language model. 
Please rate the clarity, completeness, and overall quality of each narrative on a scale from 1 to 5, 
and then provide an overall judgment of which narrative is better.

Human Narrative:
{human_narrative.strip()}

LLM-Generated Narrative:
{llm_narrative.strip()}

Please provide:
1. A table comparing clarity, completeness, and quality for each narrative
2. An overall rating indicating which narrative is better and why
"""


# def evaluate_with_judge(azure_client, deployment_name: str, prompt: str, output: str) -> str:
#     """
#     Send evaluation request to Azure OpenAI judge model via AzureClient and return the evaluation text.

#     Parameters:
#     - azure_client: Your AzureClient instance
#     - deployment_name (str): The name of the deployment (judge model)
#     - prompt (str): The original explanation prompt
#     - output (str): The model's explanation output

#     Returns:
#     - str: Judge model's evaluation response
#     """
#     eval_prompt = build_evaluation_prompt(prompt, output)
#     return azure_client.get_response(eval_prompt, deployment_name=deployment_name)


def extract_scores(eval_text: str) -> Tuple[Optional[int], Optional[int], Optional[int]]:
    """Extract clarity, conciseness, and completeness scores from the evaluation text.
    
    Returns:
        Tuple of (clarity, conciseness, completeness) scores, each as int or None if not found
    """
    clarity = conciseness = completeness = None

    clarity_match = re.search(r'Clarity[:\s]+(\d)', eval_text, re.IGNORECASE)
    conciseness_match = re.search(r'Conciseness[:\s]+(\d)', eval_text, re.IGNORECASE)
    completeness_match = re.search(r'Completeness[:\s]+(\d)', eval_text, re.IGNORECASE)

    if clarity_match:
        clarity = int(clarity_match.group(1))
    if conciseness_match:
        conciseness = int(conciseness_match.group(1))
    if completeness_match:
        completeness = int(completeness_match.group(1))

    return clarity, conciseness, completeness


def evaluate_dataframe(df: pd.DataFrame, prompt_col: str, output_col: str, 
                      judge_models: Dict[str, str] = JUDGE_MODELS) -> pd.DataFrame:
    """Evaluate a DataFrame with prompt and output columns using multiple judge models.
    
    Returns a new DataFrame with evaluation scores and summaries for each row and judge model.

    Output columns added per judge model (e.g., for GPT-4):
      - Clarity_GPT-4
      - Conciseness_GPT-4
      - Completeness_GPT-4
      - EvalSummary_GPT-4
      - EvalTime_GPT-4

    Parameters:
        df: Input DataFrame
        prompt_col: Name of the column containing the prompt text
        output_col: Name of the column containing the generated answer text
        judge_models: Dict of judge model names and deployment names (default JUDGE_MODELS)
    """
    df = df.copy()
    
    for judge_name, deployment_name in judge_models.items():
        clarity_list = []
        conciseness_list = []
        completeness_list = []
        eval_summary_list = []
        eval_time_list = []

        print(f"Starting evaluation with judge model: {judge_name}")

        for idx, row in df.iterrows():
            prompt_text = row[prompt_col]
            output_text = row[output_col]

            start_time = datetime.now()
            # eval_text = "Mock evaluation response"  # Replace with actual judge call
            eval_text = evaluate_with_judge(deployment_name, prompt_text, output_text)
            end_time = datetime.now()

            clarity, conciseness, completeness = extract_scores(eval_text)
            eval_duration = (end_time - start_time).total_seconds()

            clarity_list.append(clarity)
            conciseness_list.append(conciseness)
            completeness_list.append(completeness)
            eval_summary_list.append(eval_text.strip())
            eval_time_list.append(eval_duration)

        # Add columns to DataFrame
        df[f"Clarity_{judge_name}"] = clarity_list
        df[f"Conciseness_{judge_name}"] = conciseness_list
        df[f"Completeness_{judge_name}"] = completeness_list
        df[f"EvalSummary_{judge_name}"] = eval_summary_list
        df[f"EvalTime_{judge_name}"] = eval_time_list

    return df

# Example usage (would be in main.py instead)
if __name__ == "__main__":
    """Test judge functionality standalone."""
    print("=== Testing Judge Module ===")

    # Test the functions
    test_prompt = "Explain this risk score"
    test_output = "The score is high due to frequent transactions"
    
    # Test build_evaluation_prompt
    eval_prompt = build_evaluation_prompt(test_prompt, test_output)
    print("Evaluation Prompt:")
    print(eval_prompt)
    
    # Test extract_scores
    test_eval = """
    Clarity: 4
    Conciseness: 3
    Completeness: 5
    """
    scores = extract_scores(test_eval)
    print(f"\nExtracted Scores - Clarity: {scores[0]}, Conciseness: {scores[1]}, Completeness: {scores[2]}")