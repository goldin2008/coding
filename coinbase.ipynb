{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6759f0-48c4-49d2-a1ed-a1aabe649ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "175b914d-dbe3-4fa0-941e-4e7bb8bddd8c",
   "metadata": {},
   "source": [
    "最长连续相同字符子串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6863b48-45ae-4758-b8be-d9774e19e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 3)\n"
     ]
    }
   ],
   "source": [
    "def longest_consecutive_chars(s):\n",
    "    if not s:\n",
    "        return '', 0\n",
    "    \n",
    "    max_char = s[0]\n",
    "    max_len = 1\n",
    "    current_char = s[0]\n",
    "    current_len = 1\n",
    "    \n",
    "    for i in range(1, len(s)):\n",
    "        if s[i] == current_char:\n",
    "            current_len += 1\n",
    "            if current_len > max_len:\n",
    "                max_len = current_len\n",
    "                max_char = current_char\n",
    "        else:\n",
    "            current_char = s[i]\n",
    "            current_len = 1\n",
    "    \n",
    "    return max_char, max_len\n",
    "\n",
    "# 测试\n",
    "print(longest_consecutive_chars(\"aaabbc\"))  # 输出: ('a', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9add0-c5b6-49d9-9f6f-3f785657daab",
   "metadata": {},
   "source": [
    "寻找Local Max Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3106833-cd8a-46c6-bf95-3858805c6069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n"
     ]
    }
   ],
   "source": [
    "def find_local_max(arr, n):\n",
    "    \"\"\"\n",
    "    找出所有local max value\n",
    "    local max定义：在长度为2n+1的窗口中最大，\n",
    "    且前n个元素严格递增，后n个元素严格递减\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    length = len(arr)\n",
    "    \n",
    "    for i in range(n, length - n):\n",
    "        window = arr[i-n:i+n+1]\n",
    "        \n",
    "        # 检查arr[i]是否为窗口最大值\n",
    "        if arr[i] != max(window):\n",
    "            continue\n",
    "            \n",
    "        # 检查前n个是否严格递增\n",
    "        increasing = True\n",
    "        for j in range(i-n+1, i+1):\n",
    "            if arr[j] <= arr[j-1]:\n",
    "                increasing = False\n",
    "                break\n",
    "                \n",
    "        # 检查后n个是否严格递减\n",
    "        decreasing = True\n",
    "        for j in range(i+1, i+n+1):\n",
    "            if arr[j] >= arr[j-1]:\n",
    "                decreasing = False\n",
    "                break\n",
    "                \n",
    "        if increasing and decreasing:\n",
    "            result.append(arr[i])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 测试\n",
    "arr = [1, 3, 10, 4, 2, 19, 5, 5]\n",
    "n = 2\n",
    "print(find_local_max(arr, n))  # 输出: [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c087eeb-2fed-49c4-9ac1-e396b9b3fd90",
   "metadata": {},
   "source": [
    "补全Bootstrap算法代码\n",
    "\n",
    "bootstrap函数：有放回地随机抽取n个样本索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2127d0d4-b078-4ba6-9501-2db38d112c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from random import randint, seed\n",
    "\n",
    "def bootstrap(n: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Step 1: Bootstrap the train samples for each base classifier.\n",
    "    \"\"\"\n",
    "    indices = [randint(0, n-1) for _ in range(n)]\n",
    "    return indices\n",
    "\n",
    "def fit(classifiers, x, y):\n",
    "    \"\"\"\n",
    "    Step 2: Train each classifier based on its own bootstrapped samples.\n",
    "    \"\"\"\n",
    "    n_samples = len(x)\n",
    "    for clf in classifiers:\n",
    "        indices = bootstrap(n_samples)\n",
    "        x_bootstrapped = [x[i] for i in indices]\n",
    "        y_bootstrapped = [y[i] for i in indices]\n",
    "        clf.fit(x_bootstrapped, y_bootstrapped)\n",
    "\n",
    "def predict(classifiers, x):\n",
    "    \"\"\"\n",
    "    Step 3: Assign class labels by a majority vote of the base classifiers.\n",
    "    \"\"\"\n",
    "    predictions = np.array([clf.predict(x) for clf in classifiers])\n",
    "    # Majority vote\n",
    "    final_predictions = np.apply_along_axis(\n",
    "        lambda x: np.bincount(x).argmax(), \n",
    "        axis=0, \n",
    "        arr=predictions\n",
    "    )\n",
    "    return final_predictions.tolist()\n",
    "\n",
    "def solution(x_train, y_train, x_test, n_estimators):\n",
    "    \"\"\"\n",
    "    Step 4: Pull everything together\n",
    "    \"\"\"\n",
    "    seed(42)\n",
    "    classifiers = [DecisionTreeClassifier(random_state=0) \n",
    "                   for _ in range(n_estimators)]\n",
    "    fit(classifiers, x_train, y_train)\n",
    "    return predict(classifiers, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eca2fa-efb5-41c4-91d8-4b273a8fce36",
   "metadata": {},
   "source": [
    "Naive Bayes实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e62968-1d59-4aa0-8135-bd8b0f1c8308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.feature_likelihoods = defaultdict(list) # 存储每个类别下每个特征取值的概率\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # 计算先验概率 P(y)\n",
    "        total_samples = len(y)\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        for cls, cnt in zip(classes, counts):\n",
    "            self.class_priors[cls] = cnt / total_samples\n",
    "            \n",
    "        # 计算似然概率 P(x|y) - 这里以离散特征为例\n",
    "        # 实际面试中需根据题目要求处理连续特征（如假设高斯分布）\n",
    "        for cls in classes:\n",
    "            X_cls = X[y == cls]\n",
    "            for feature_idx in range(X.shape[1]):\n",
    "                feature_vals, val_counts = np.unique(X_cls[:, feature_idx], return_counts=True)\n",
    "                probabilities = val_counts / len(X_cls)\n",
    "                self.feature_likelihoods[(cls, feature_idx)] = dict(zip(feature_vals, probabilities))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            posteriors = {}\n",
    "            for cls, prior in self.class_priors.items():\n",
    "                likelihood = 1.0\n",
    "                for feature_idx, feature_val in enumerate(sample):\n",
    "                    # 获取该特征值在给定类别下的概率，如果未见则使用平滑（如拉普拉斯平滑）\n",
    "                    prob_dict = self.feature_likelihoods.get((cls, feature_idx), {})\n",
    "                    likelihood *= prob_dict.get(feature_val, 1e-6) # 使用一个极小值做平滑\n",
    "                posteriors[cls] = prior * likelihood\n",
    "            predictions.append(max(posteriors, key=posteriors.get))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e90ff-c24a-423b-af34-566ff8747272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, alpha=1):\n",
    "    # ... 计算先验概率 ...\n",
    "    \n",
    "    for cls in classes:\n",
    "        X_cls = X[y == cls]\n",
    "        n_cls = len(X_cls)\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            # 计算每个特征值的出现次数\n",
    "            feature_vals, val_counts = np.unique(X_cls[:, feature_idx], return_counts=True)\n",
    "            # 获取该特征所有可能取值（来自整个训练集）\n",
    "            all_vals = np.unique(X[:, feature_idx])\n",
    "            V = len(all_vals)  # 可能取值数\n",
    "            \n",
    "            # 拉普拉斯平滑\n",
    "            probabilities = {}\n",
    "            for val, count in zip(feature_vals, val_counts):\n",
    "                probabilities[val] = (count + alpha) / (n_cls + alpha * V)\n",
    "            # 为未出现的值也分配概率\n",
    "            for val in all_vals:\n",
    "                if val not in probabilities:\n",
    "                    probabilities[val] = alpha / (n_cls + alpha * V)\n",
    "                    \n",
    "            self.feature_likelihoods[(cls, feature_idx)] = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b58d2-5c85-484a-a473-116f511c7eda",
   "metadata": {},
   "source": [
    "K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330718d7-0199-486a-8354-0f36b71b84b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initialize_centroids(X, k):\n",
    "    \"\"\"Randomly initialize centroids from the dataset.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    random_indices = np.random.permutation(X.shape[0])\n",
    "    centroids = X[random_indices[:k]]\n",
    "    return centroids\n",
    "\n",
    "def compute_distances(X, centroids):\n",
    "    \"\"\"Compute the distance between each data point and the centroids.\"\"\"\n",
    "    distances = np.zeros((X.shape[0], centroids.shape[0]))\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        distances[:, i] = np.linalg.norm(X - centroid, axis=1)\n",
    "    return distances\n",
    "\n",
    "def assign_clusters(distances):\n",
    "    \"\"\"Assign each data point to the closest centroid.\"\"\"\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "def compute_centroids(X, labels, k):\n",
    "    \"\"\"Compute the new centroids as the mean of all data points assigned to each cluster.\"\"\"\n",
    "    centroids = np.zeros((k, X.shape[1]))\n",
    "    for i in range(k):\n",
    "        centroids[i, :] = X[labels == i].mean(axis=0)\n",
    "    return centroids\n",
    "\n",
    "def kmeans(X, k, max_iters=100):\n",
    "    \"\"\"K-means clustering algorithm.\"\"\"\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        old_centroids = centroids\n",
    "        distances = compute_distances(X, centroids)\n",
    "        labels = assign_clusters(distances)\n",
    "        centroids = compute_centroids(X, labels, k)\n",
    "        \n",
    "        # If centroids do not change, we have converged\n",
    "        if np.all(centroids == old_centroids):\n",
    "            break\n",
    "    \n",
    "    return centroids, labels\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some synthetic data\n",
    "    from sklearn.datasets import make_blobs\n",
    "    X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)\n",
    "    print(X.shape, X[:3])\n",
    "\n",
    "    # Run the K-means algorithm\n",
    "    k = 3\n",
    "    centroids, labels = kmeans(X, k)\n",
    "\n",
    "    # Visualize the results\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red')  # Centroids\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('K-means Clustering')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d0f1d4-0ac0-43f6-9226-84a8e9605e16",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55209a3f-dac4-411a-aee4-cc32771181fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000, \n",
    "                 fit_intercept=True, method='batch'):\n",
    "        \"\"\"\n",
    "        learning_rate: 学习率\n",
    "        n_iter: 迭代次数\n",
    "        fit_intercept: 是否添加偏置项\n",
    "        method: 'batch'（批量梯度下降）或 'sgd'（随机梯度下降）\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.method = method\n",
    "        self.theta = None\n",
    "        self.loss_history = []  # 记录损失变化\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"添加偏置项\"\"\"\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    def _compute_loss(self, X, y):\n",
    "        \"\"\"计算均方误差损失\"\"\"\n",
    "        m = len(y)\n",
    "        y_pred = X @ self.theta\n",
    "        loss = (1/(2*m)) * np.sum((y_pred - y) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, verbose=False):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        # 添加偏置项\n",
    "        if self.fit_intercept:\n",
    "            X = self._add_intercept(X)\n",
    "        \n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros(n)  # 初始化参数\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            if self.method == 'batch':\n",
    "                # 批量梯度下降\n",
    "                gradient = (1/m) * X.T @ (X @ self.theta - y)\n",
    "                self.theta -= self.learning_rate * gradient\n",
    "            \n",
    "            elif self.method == 'sgd':\n",
    "                # 随机梯度下降（随机选一个样本）\n",
    "                idx = np.random.randint(m)\n",
    "                x_i = X[idx:idx+1]  # 保持2D形状\n",
    "                y_i = y[idx:idx+1]\n",
    "                gradient = x_i.T @ (x_i @ self.theta - y_i)\n",
    "                self.theta -= self.learning_rate * gradient\n",
    "            \n",
    "            # 记录损失（每100次迭代）\n",
    "            if i % 100 == 0 or i == self.n_iter - 1:\n",
    "                loss = self._compute_loss(X, y)\n",
    "                self.loss_history.append((i, loss))\n",
    "                if verbose:\n",
    "                    print(f\"Iteration {i}: loss = {loss:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"预测\"\"\"\n",
    "        if self.fit_intercept:\n",
    "            X = self._add_intercept(X)\n",
    "        return X @ self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0967a1f-873d-4bef-b2b0-9dcb8fd95349",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a6bc7-9b28-49ba-9541-b9f520b44ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000, fit_intercept=True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.theta = None\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"添加偏置项\"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.hstack((intercept, X))\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid函数\"\"\"\n",
    "        # 数值稳定性处理\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # 添加偏置项\n",
    "        if self.fit_intercept:\n",
    "            X = self._add_intercept(X)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.zeros(n_features)\n",
    "        \n",
    "        # 梯度下降\n",
    "        for i in range(self.n_iter):\n",
    "            # 计算预测值\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self._sigmoid(z)\n",
    "            \n",
    "            # 计算梯度\n",
    "            gradient = np.dot(X.T, (h - y)) / n_samples\n",
    "            \n",
    "            # 更新参数\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "            \n",
    "            # 可选的：计算损失（用于监控）\n",
    "            # loss = -np.mean(y * np.log(h + 1e-8) + (1-y) * np.log(1-h + 1e-8))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"预测概率\"\"\"\n",
    "        if self.fit_intercept:\n",
    "            X = self._add_intercept(X)\n",
    "        return self._sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"预测类别\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cef27b-a206-467f-bcfd-4619a46076d6",
   "metadata": {},
   "source": [
    "Decision Tree 原理与实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558019c-4c5f-4d27-a212-afb1059973b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.tree) for x in X])\n",
    "    \n",
    "    # ========== 核心递归函数 ==========\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # 停止条件\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            n_classes == 1):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'type': 'leaf', 'value': leaf_value}\n",
    "        \n",
    "        # 寻找最佳分裂\n",
    "        best_feat, best_thresh = self._best_split(X, y)\n",
    "        if best_feat is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'type': 'leaf', 'value': leaf_value}\n",
    "        \n",
    "        # 递归构建子树\n",
    "        left_idx = X[:, best_feat] <= best_thresh\n",
    "        right_idx = X[:, best_feat] > best_thresh\n",
    "        \n",
    "        left_subtree = self._grow_tree(X[left_idx], y[left_idx], depth+1)\n",
    "        right_subtree = self._grow_tree(X[right_idx], y[right_idx], depth+1)\n",
    "        \n",
    "        return {\n",
    "            'type': 'node',\n",
    "            'feature': best_feat,\n",
    "            'threshold': best_thresh,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "    \n",
    "    # ========== 关键辅助函数 ==========\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        split_feat, split_thresh = None, None\n",
    "        \n",
    "        for feat in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feat])\n",
    "            for thresh in thresholds:\n",
    "                gain = self._information_gain(X, y, feat, thresh)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_feat, split_thresh = feat, thresh\n",
    "        \n",
    "        return split_feat, split_thresh\n",
    "    \n",
    "    def _information_gain(self, X, y, feat, thresh):\n",
    "        # 计算基尼不纯度\n",
    "        parent_gini = self._gini(y)\n",
    "        \n",
    "        left_idx = X[:, feat] <= thresh\n",
    "        right_idx = X[:, feat] > thresh\n",
    "        \n",
    "        if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
    "            return 0\n",
    "        \n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y[left_idx]), len(y[right_idx])\n",
    "        \n",
    "        left_gini = self._gini(y[left_idx])\n",
    "        right_gini = self._gini(y[right_idx])\n",
    "        \n",
    "        child_gini = (n_left/n) * left_gini + (n_right/n) * right_gini\n",
    "        \n",
    "        return parent_gini - child_gini\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        # 基尼不纯度: 1 - sum(p_i^2)\n",
    "        n = len(y)\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        proportions = np.bincount(y) / n\n",
    "        return 1 - np.sum(proportions ** 2)\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "    \n",
    "    def _predict_one(self, x, node):\n",
    "        if node['type'] == 'leaf':\n",
    "            return node['value']\n",
    "        \n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self._predict_one(x, node['left'])\n",
    "        else:\n",
    "            return self._predict_one(x, node['right'])\n",
    "\n",
    "# ========== 使用示例 ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建简单数据\n",
    "    X = np.array([[1, 2], [2, 3], [3, 3], [6, 5], [7, 8], [8, 8]])\n",
    "    y = np.array([0, 0, 0, 1, 1, 1])\n",
    "    \n",
    "    # 训练\n",
    "    tree = DecisionTree(max_depth=3)\n",
    "    tree.fit(X, y)\n",
    "    \n",
    "    # 预测\n",
    "    X_test = np.array([[2.5, 3], [7.5, 7]])\n",
    "    predictions = tree.predict(X_test)\n",
    "    print(f\"Predictions: {predictions}\")  # 应输出 [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65b134-92b7-40b8-b118-e2e2580a1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"单个决策树（简化版）\"\"\"\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # 停止条件\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            n_classes == 1):\n",
    "            return np.bincount(y).argmax()\n",
    "        \n",
    "        # 随机选择特征子集（√n_features）\n",
    "        feat_indices = np.random.choice(n_features, int(np.sqrt(n_features)), replace=False)\n",
    "        \n",
    "        # 寻找最佳分裂\n",
    "        best_gain = -1\n",
    "        best_feat, best_thresh = None, None\n",
    "        \n",
    "        for feat in feat_indices:\n",
    "            thresholds = np.unique(X[:, feat])\n",
    "            for thresh in thresholds:\n",
    "                left = y[X[:, feat] <= thresh]\n",
    "                right = y[X[:, feat] > thresh]\n",
    "                \n",
    "                if len(left) == 0 or len(right) == 0:\n",
    "                    continue\n",
    "                \n",
    "                gain = self._gini_gain(y, left, right)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feat, best_thresh = feat, thresh\n",
    "        \n",
    "        if best_gain <= 0:\n",
    "            return np.bincount(y).argmax()\n",
    "        \n",
    "        # 递归构建子树\n",
    "        left_idx = X[:, best_feat] <= best_thresh\n",
    "        right_idx = ~left_idx\n",
    "        \n",
    "        left_tree = self._grow_tree(X[left_idx], y[left_idx], depth+1)\n",
    "        right_tree = self._grow_tree(X[right_idx], y[right_idx], depth+1)\n",
    "        \n",
    "        return (best_feat, best_thresh, left_tree, right_tree)\n",
    "    \n",
    "    def _gini_gain(self, parent, left, right):\n",
    "        def gini(arr):\n",
    "            if len(arr) == 0: return 0\n",
    "            p = np.bincount(arr) / len(arr)\n",
    "            return 1 - np.sum(p ** 2)\n",
    "        \n",
    "        n = len(parent)\n",
    "        n_l, n_r = len(left), len(right)\n",
    "        return gini(parent) - (n_l/n * gini(left) + n_r/n * gini(right))\n",
    "    \n",
    "    def predict_one(self, x, node):\n",
    "        if not isinstance(node, tuple):  # 叶节点\n",
    "            return node\n",
    "        \n",
    "        feat, thresh, left, right = node\n",
    "        if x[feat] <= thresh:\n",
    "            return self.predict_one(x, left)\n",
    "        else:\n",
    "            return self.predict_one(x, right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_one(x, self.tree) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd8cf5-9c3e-4f16-bbed-b241cde53e6d",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790cf84-1986-4dfe-b800-d38eee9e3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \"\"\"随机森林主类\"\"\"\n",
    "    def __init__(self, n_estimators=100, max_depth=10, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # 1. Bootstrap采样（有放回随机抽样）\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_boot = X[indices]\n",
    "            y_boot = y[indices]\n",
    "            \n",
    "            # 2. 训练决策树\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split\n",
    "            )\n",
    "            tree.fit(X_boot, y_boot)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # 可选：打印进度\n",
    "            if (i+1) % 20 == 0:\n",
    "                print(f\"Trained tree {i+1}/{self.n_estimators}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # 收集所有树的预测\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "        # 多数投票（沿树的方向取众数）\n",
    "        final_predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            votes = tree_preds[:, i]\n",
    "            # 使用Counter找众数\n",
    "            most_common = Counter(votes).most_common(1)[0][0]\n",
    "            final_predictions.append(most_common)\n",
    "        \n",
    "        return np.array(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3484e-10a6-4e56-8bca-01d7a9d578cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50806aab-3145-42b3-9698-cd0766083292",
   "metadata": {},
   "source": [
    "TP = 80, FN = 20, FP = 15, TN = 85\n",
    "Recall = TP / (TP + FN) = 80 / (80 + 20) = 0.8 或 80%\n",
    "FPR = FP / (FP + TN) = FP / N\n",
    "\n",
    "3 问题：如果验证集损失（validation loss）显著高于训练集损失（training loss），可能的原因是什么？\n",
    "\n",
    "回答要点：这是过拟合（Overfitting） 的典型标志。模型过度学习了训练数据中的噪声和细节，导致在未见过的验证数据上泛化能力变差。处理方法包括：获取更多数据、进行数据增强、增加正则化（如L1/L2）、使用Dropout（对神经网络）、或简化模型复杂度。\n",
    "\n",
    "4 问题：在处理分类问题时，如何有意识地增加模型的偏差（Bias）并减少方差（Variance）？\n",
    "\n",
    "回答要点：这通常是为了解决过拟合（高方差）。方法包括：简化模型（如减少树的最大深度、减少神经网络层数）、增加正则化强度、减少特征数量、或使用Bagging类方法（如随机森林）来平均多个高方差模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a374945-515a-43bc-b883-f333126b1160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf53a9-ca60-41e5-b2b9-817fbdc65d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "神经网络输出计算题\n",
    "1. MLP输出计算\n",
    "问题：\n",
    "\n",
    "输入层：3个神经元，输入为 [1.0, 0.5, -0.2]\n",
    "\n",
    "隐藏层：2个神经元，激活函数为线性\n",
    "\n",
    "输出层：1个神经元，激活函数为sigmoid\n",
    "\n",
    "权重：\n",
    "\n",
    "W1 = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n",
    "\n",
    "b1 = [0.1, 0.2]\n",
    "\n",
    "W2 = [[0.7], [0.8]]\n",
    "\n",
    "b2 = [0.3]\n",
    "\n",
    "答案：\n",
    "\n",
    "步骤1：计算隐藏层输出\n",
    "h1 = 1.0*0.1 + 0.5*0.3 + (-0.2)*0.5 + 0.1 = 0.1 + 0.15 - 0.1 + 0.1 = 0.25\n",
    "h2 = 1.0*0.2 + 0.5*0.4 + (-0.2)*0.6 + 0.2 = 0.2 + 0.2 - 0.12 + 0.2 = 0.48\n",
    "\n",
    "步骤2：计算输出层输入\n",
    "z = 0.25*0.7 + 0.48*0.8 + 0.3 = 0.175 + 0.384 + 0.3 = 0.859\n",
    "\n",
    "步骤3：应用sigmoid\n",
    "output = 1 / (1 + e^(-0.859)) ≈ 1 / (1 + 0.424) ≈ 1 / 1.424 ≈ 0.702\n",
    "\n",
    "答案：0.702（保留三位小数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bd411c3-94e5-42fe-a5a9-2037e2fceeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25, 0.48000000000000004, 0.859, np.float64(0.7024516833527672))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = 1.0*0.1 + 0.5*0.3 + (-0.2)*0.5 + 0.1\n",
    "h2 = 1.0*0.2 + 0.5*0.4 + (-0.2)*0.6 + 0.2\n",
    "z = 0.25*0.7 + 0.48*0.8 + 0.3\n",
    "output = 1 / (1 + np.exp(-0.859))\n",
    "h1, h2, z, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8244f-54a6-4b6f-bec2-2ad87e2b527d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fb0ae-ef06-461c-8160-cf19154e91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "我现在有interview的一些资料，根据下面资料生成interview时的面试题和对应答案，\n",
    "\n",
    "第二个OA: 70分钟，6道选择题+1道填空题＋3道code。\n",
    "选择题基本上都是ML相关题，比如要你算recall，LDA和PCA区别，overfitting处理啥的。填空题是给你一个NN结构和input，你算最后output。最搞心态的是，前面都是linear function，最后输出层它搞一个sigmoid function，然后算出来x还是个小数。你告诉我这没计算器怎么算？最后结果要求小数点后三位。\n",
    "\n",
    "\n",
    "第一道code：给一个数组和一个区间n，算有哪些local max value。比如[1,3,10,4,2,19,5,5]和区间n=2，10是一个local max value，因为[1,3,10,4,2]里10最大，然后10前面的数字严格单调递增，后面的数字严格单调递减（大于等于，小于等于都不行）。所以19不是。\n",
    "第二道code：手写bootstrap算法，补全code块（不准对已有code任何改动）。给你多个sklearn的classifiers，x和y。输出经过majority voting后的predicted y。答案大概长这样：\n",
    "from random import randint, seed\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "def bootstrap(n: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Step 1: Bootstrap the train samples for each base classifier.\n",
    "    \"\"\"\n",
    "    indices = [randint(0, n-1) for _ in range(n)]\n",
    "    return indices\n",
    "\n",
    "def fit(classifiers: list[DecisionTreeClassifier], x: list[list[float]], y: list[int]):\n",
    "    \"\"\"\n",
    "    Step 2: Train each classifier based on its own bootstrapped samples.\n",
    "    \"\"\"\n",
    "    n_samples = len(x)\n",
    "    for clf in classifiers:\n",
    "        indices = bootstrap(n_samples)\n",
    "        x_bootstrapped = [x[i] for i in indices]\n",
    "        y_bootstrapped = [y[i] for i in indices]\n",
    "        clf.fit(x_bootstrapped, y_bootstrapped)\n",
    "\n",
    "def predict(classifiers: list[DecisionTreeClassifier], x: list[list[float]]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Step 3: Assign class labels by a majority vote of the base classifiers.\n",
    "    \"\"\"\n",
    "    predictions = np.array([clf.predict(x) for clf in classifiers])\n",
    "    # Majority vote\n",
    "    final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
    "    return final_predictions.tolist()\n",
    "\n",
    "def solution(x_train: list[list[float]], y_train: list[int], x_test: list[list[float]], n_estimators: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Step 4: Pull everything together\n",
    "    \"\"\"\n",
    "    seed(42)\n",
    "    classifiers = [DecisionTreeClassifier(random_state=0) for _ in range(n_estimators)]\n",
    "    fit(classifiers, x_train, y_train)\n",
    "    return predict(classifiers, x_test)\n",
    "复制代码\n",
    "第三道code：手动实现Naive Bayes算法。到这我已经没时间了。\n",
    "\n",
    "\n",
    "OA2\n",
    "\n",
    "\n",
    "5 道选择\n",
    "问Random Forest和XGBoost的区别\n",
    "怎么increase bias and reduce variance\n",
    "LDA和PCA分别的适用场景\n",
    "validation loss is significantly higher than training loss，问有可能是什么原因\n",
    "给了4个confusion matrices, 选出所有recall >= 90%同时FPR < 10%的\n",
    "\n",
    "\n",
    "\n",
    "1道填空\n",
    "一个有1个hidden layer的MLP，hidden activation是linear，output activation是sigmoid，给了input和所有weights，算output\n",
    "\n",
    "\n",
    "\n",
    "3道coding\n",
    "find the longest contiguous substring consisting of the same character：找到连续出现最多次的character，返回这个character和连续出现的次数\n",
    "Bootstrap\n",
    "Decision Tree\n",
    "\n",
    "70分钟10题， CodeSignal Online Assessment。时间很紧，楼主也不记得原题了，请大家谅解。几个tips\n",
    "1- coding基本写出来就不错了。时间太紧了，不要worry about optimality，除非test 没过再回来改。ML coding 的description非常的长，可能看起来会很花时间，所以要复习一下tree algorithm自己写一写，再去。可以先做coding，再回来做选择题。\n",
    "2 - multiple choices 考ML fundamental 考的非常非常细。如果不是很confident就不要花太多时间纠结了。\n",
    "\n",
    "\n",
    "7个Multiple Choices，抱歉不是全记得了，就写几个记得 ，TLDR 硬币基地很喜欢考各种tree algorithm，复习好了tree再去做这个tech screen\n",
    "\n",
    "1 - naive bayes 和 knn的优劣（比如说curse of dimensionality, multi-colinearity 方面）\n",
    "2- implement forward propagation for 3 layers of simple feedforward with linear/no activation function in first 2 layers and sigmoid in last layers. 这题有点变态，因为没说能让用calculator，楼主很诚实， 所以楼主纠结了一下sigmoid 估算。\n",
    "3- emsemble algorithm的优劣（比如说是不是training extensive，是不是容易overfit）\n",
    "4- random forest和GBT 的优劣 (比如说inference time，overfitting， difficult to train\n",
    "5 - LDA 和PCA的优劣和区别\n",
    "\n",
    "\n",
    "3个coding。\n",
    "第一题coding非常简单，求string中longest consequtive sequence with identical characters 长度. 比如说aaabbc -> 3 。\n",
    "第二题要求implement random forest里面的bootstrapping，bagging 的training和prediction。\n",
    "第三道题楼主，没时间做了，也是implement random forest里面的其他的一些compoment\n",
    "楼主的Multiple choices是随便答的因为没有复习，coding做到后面也没太多时间做了，总之没啥准备。希望想去的同学还是准备好再做这个。\n",
    "楼主确实做的很烂，主要是真的很久没有用到各种tree algorithm了，但是recruiter说senior 这个算过，staff不算过。\n",
    "\n",
    "70 分钟10道题\n",
    "前几个都是MLE八股文， 还有算MLP output, 最后是两道coding, 一道 logistic regression, test case 全过， 第二道naive bayes, 没时间写了直接提交了。\n",
    "\n",
    "70 分钟 10 道题目\n",
    "\n",
    "1-7 是一些简单的ml basics, 问一些model的不同，怎么处理各种情况，比如overfitting\n",
    "8 一个非常简单的leetcode，大概是找字符串里面最后一个最大的连续重复的substring\n",
    "9 不让用numpy然后手写gradient descent，但是给的蛮简单的，是一个linear function然后迭代的formula也都给了，稍微注意一下matrix dimension就好了\n",
    "10 最后一题也是手写，写kmeans，我只写到了更新cluster center，最后的main function没时间了， 但是最后也过了\n",
    "\n",
    "OA2是mle coding+basics；有选择+填空+coding；影响最深的是有个填空题算loss，精确到小数点后三位… 还没给计算器，实在不知道这题的意义在哪里\n",
    "coding是 decision tree，logistic regression和knn；里面给了些implementation然后让你填上剩下的、我觉得这种对我非常不友好，不如让我写一个…\n",
    "还有就是之前看instruction说不用run，就没看test case是不是pass；太蠢了，太久没做这种OA了；不知道自己咋想的\n",
    "反正fail了，move forward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
