{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6759f0-48c4-49d2-a1ed-a1aabe649ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "175b914d-dbe3-4fa0-941e-4e7bb8bddd8c",
   "metadata": {},
   "source": [
    "最长连续相同字符子串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6863b48-45ae-4758-b8be-d9774e19e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 3)\n"
     ]
    }
   ],
   "source": [
    "def longest_consecutive_chars(s):\n",
    "    if not s:\n",
    "        return '', 0\n",
    "    \n",
    "    max_char = s[0]\n",
    "    max_len = 1\n",
    "    current_char = s[0]\n",
    "    current_len = 1\n",
    "    \n",
    "    for i in range(1, len(s)):\n",
    "        if s[i] == current_char:\n",
    "            current_len += 1\n",
    "            if current_len > max_len:\n",
    "                max_len = current_len\n",
    "                max_char = current_char\n",
    "        else:\n",
    "            current_char = s[i]\n",
    "            current_len = 1\n",
    "    \n",
    "    return max_char, max_len\n",
    "\n",
    "# 测试\n",
    "print(longest_consecutive_chars(\"aaabbc\"))  # 输出: ('a', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9add0-c5b6-49d9-9f6f-3f785657daab",
   "metadata": {},
   "source": [
    "寻找Local Max Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3106833-cd8a-46c6-bf95-3858805c6069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n"
     ]
    }
   ],
   "source": [
    "def find_local_max(arr, n):\n",
    "    \"\"\"\n",
    "    找出所有local max value\n",
    "    local max定义：在长度为2n+1的窗口中最大，\n",
    "    且前n个元素严格递增，后n个元素严格递减\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    length = len(arr)\n",
    "    \n",
    "    for i in range(n, length - n):\n",
    "        window = arr[i-n:i+n+1]\n",
    "        \n",
    "        # 检查arr[i]是否为窗口最大值\n",
    "        if arr[i] != max(window):\n",
    "            continue\n",
    "            \n",
    "        # 检查前n个是否严格递增\n",
    "        increasing = True\n",
    "        for j in range(i-n+1, i+1):\n",
    "            if arr[j] <= arr[j-1]:\n",
    "                increasing = False\n",
    "                break\n",
    "                \n",
    "        # 检查后n个是否严格递减\n",
    "        decreasing = True\n",
    "        for j in range(i+1, i+n+1):\n",
    "            if arr[j] >= arr[j-1]:\n",
    "                decreasing = False\n",
    "                break\n",
    "                \n",
    "        if increasing and decreasing:\n",
    "            result.append(arr[i])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 测试\n",
    "arr = [1, 3, 10, 4, 2, 19, 5, 5]\n",
    "n = 2\n",
    "print(find_local_max(arr, n))  # 输出: [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c087eeb-2fed-49c4-9ac1-e396b9b3fd90",
   "metadata": {},
   "source": [
    "补全Bootstrap算法代码\n",
    "\n",
    "bootstrap函数：有放回地随机抽取n个样本索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2127d0d4-b078-4ba6-9501-2db38d112c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from random import randint, seed\n",
    "\n",
    "def bootstrap(n: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Step 1: Bootstrap the train samples for each base classifier.\n",
    "    \"\"\"\n",
    "    indices = [randint(0, n-1) for _ in range(n)]\n",
    "    return indices\n",
    "\n",
    "def fit(classifiers, x, y):\n",
    "    \"\"\"\n",
    "    Step 2: Train each classifier based on its own bootstrapped samples.\n",
    "    \"\"\"\n",
    "    n_samples = len(x)\n",
    "    for clf in classifiers:\n",
    "        indices = bootstrap(n_samples)\n",
    "        x_bootstrapped = [x[i] for i in indices]\n",
    "        y_bootstrapped = [y[i] for i in indices]\n",
    "        clf.fit(x_bootstrapped, y_bootstrapped)\n",
    "\n",
    "def predict(classifiers, x):\n",
    "    \"\"\"\n",
    "    Step 3: Assign class labels by a majority vote of the base classifiers.\n",
    "    \"\"\"\n",
    "    predictions = np.array([clf.predict(x) for clf in classifiers])\n",
    "    # Majority vote\n",
    "    final_predictions = np.apply_along_axis(\n",
    "        lambda x: np.bincount(x).argmax(), \n",
    "        axis=0, \n",
    "        arr=predictions\n",
    "    )\n",
    "    # predictions = stats.mode(predictions)[0]\n",
    "    return final_predictions.tolist()\n",
    "\n",
    "def solution(x_train, y_train, x_test, n_estimators):\n",
    "    \"\"\"\n",
    "    Step 4: Pull everything together\n",
    "    \"\"\"\n",
    "    seed(42)\n",
    "    classifiers = [DecisionTreeClassifier(random_state=0) \n",
    "                   for _ in range(n_estimators)]\n",
    "    fit(classifiers, x_train, y_train)\n",
    "    return predict(classifiers, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f744c8db-56b8-456b-86c1-a618bc8e5664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 0, 0, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d0f1d4-0ac0-43f6-9226-84a8e9605e16",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55209a3f-dac4-411a-aee4-cc32771181fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000, \n",
    "                 fit_intercept=True, method='batch'):\n",
    "        \"\"\"\n",
    "        learning_rate: 学习率\n",
    "        n_iter: 迭代次数\n",
    "        fit_intercept: 是否添加偏置项\n",
    "        method: 'batch'（批量梯度下降）或 'sgd'（随机梯度下降）\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.method = method\n",
    "        self.theta = None\n",
    "        self.loss_history = []  # 记录损失变化\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"添加偏置项\"\"\"\n",
    "        return np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    def _compute_loss(self, X, y):\n",
    "        \"\"\"计算均方误差损失\"\"\"\n",
    "        m = len(y)\n",
    "        y_pred = X @ self.theta\n",
    "        loss = (1/(2*m)) * np.sum((y_pred - y) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, verbose=False):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        # 添加偏置项\n",
    "        if self.fit_intercept:\n",
    "            X = self._add_intercept(X)\n",
    "        \n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros(n)  # 初始化参数\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            if self.method == 'batch':\n",
    "                # 批量梯度下降\n",
    "                gradient = (1/m) * X.T @ (X @ self.theta - y)\n",
    "                self.theta -= self.learning_rate * gradient\n",
    "            \n",
    "            elif self.method == 'sgd':\n",
    "                # 随机梯度下降（随机选一个样本）\n",
    "                idx = np.random.randint(m)\n",
    "                x_i = X[idx:idx+1]  # 保持2D形状\n",
    "                y_i = y[idx:idx+1]\n",
    "                gradient = x_i.T @ (x_i @ self.theta - y_i)\n",
    "                self.theta -= self.learning_rate * gradient\n",
    "            \n",
    "            # 记录损失（每100次迭代）\n",
    "            if i % 100 == 0 or i == self.n_iter - 1:\n",
    "                loss = self._compute_loss(X, y)\n",
    "                self.loss_history.append((i, loss))\n",
    "                if verbose:\n",
    "                    print(f\"Iteration {i}: loss = {loss:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"预测\"\"\"\n",
    "        if self.fit_intercept:\n",
    "            X = self._add_intercept(X)\n",
    "        return X @ self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0967a1f-873d-4bef-b2b0-9dcb8fd95349",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a6bc7-9b28-49ba-9541-b9f520b44ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000, fit_intercept=True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.theta = None\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"添加偏置项\"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.hstack((intercept, X))\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid函数\"\"\"\n",
    "        # 数值稳定性处理\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # 添加偏置项\n",
    "        if self.fit_intercept:\n",
    "            X = self._add_intercept(X)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        self.theta = np.zeros(n_features)\n",
    "        \n",
    "        # 梯度下降\n",
    "        for i in range(self.n_iter):\n",
    "            # 计算预测值\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self._sigmoid(z)\n",
    "            \n",
    "            # 计算梯度\n",
    "            gradient = np.dot(X.T, (h - y)) / n_samples\n",
    "            \n",
    "            # 更新参数\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "            \n",
    "            # 可选的：计算损失（用于监控）\n",
    "            # loss = -np.mean(y * np.log(h + 1e-8) + (1-y) * np.log(1-h + 1e-8))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"预测概率\"\"\"\n",
    "        if self.fit_intercept:\n",
    "            X = self._add_intercept(X)\n",
    "        return self._sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"预测类别\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3484e-10a6-4e56-8bca-01d7a9d578cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50806aab-3145-42b3-9698-cd0766083292",
   "metadata": {},
   "source": [
    "TP = 80, FN = 20, FP = 15, TN = 85\n",
    "Recall = TP / (TP + FN) = 80 / (80 + 20) = 0.8 或 80%\n",
    "FPR = FP / (FP + TN) = FP / N\n",
    "\n",
    "3 问题：如果验证集损失（validation loss）显著高于训练集损失（training loss），可能的原因是什么？\n",
    "\n",
    "回答要点：这是过拟合（Overfitting） 的典型标志。模型过度学习了训练数据中的噪声和细节，导致在未见过的验证数据上泛化能力变差。处理方法包括：获取更多数据、进行数据增强、增加正则化（如L1/L2）、使用Dropout（对神经网络）、或简化模型复杂度。\n",
    "\n",
    "4 问题：在处理分类问题时，如何有意识地增加模型的偏差（Bias）并减少方差（Variance）？\n",
    "\n",
    "回答要点：这通常是为了解决过拟合（高方差）。方法包括：简化模型（如减少树的最大深度、减少神经网络层数）、增加正则化强度、减少特征数量、或使用Bagging类方法（如随机森林）来平均多个高方差模型的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a374945-515a-43bc-b883-f333126b1160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf53a9-ca60-41e5-b2b9-817fbdc65d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "神经网络输出计算题\n",
    "1. MLP输出计算\n",
    "问题：\n",
    "\n",
    "输入层：3个神经元，输入为 [1.0, 0.5, -0.2]\n",
    "\n",
    "隐藏层：2个神经元，激活函数为线性\n",
    "\n",
    "输出层：1个神经元，激活函数为sigmoid\n",
    "\n",
    "权重：\n",
    "\n",
    "W1 = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n",
    "\n",
    "b1 = [0.1, 0.2]\n",
    "\n",
    "W2 = [[0.7], [0.8]]\n",
    "\n",
    "b2 = [0.3]\n",
    "\n",
    "答案：\n",
    "\n",
    "步骤1：计算隐藏层输出\n",
    "h1 = 1.0*0.1 + 0.5*0.3 + (-0.2)*0.5 + 0.1 = 0.1 + 0.15 - 0.1 + 0.1 = 0.25\n",
    "h2 = 1.0*0.2 + 0.5*0.4 + (-0.2)*0.6 + 0.2 = 0.2 + 0.2 - 0.12 + 0.2 = 0.48\n",
    "\n",
    "步骤2：计算输出层输入\n",
    "z = 0.25*0.7 + 0.48*0.8 + 0.3 = 0.175 + 0.384 + 0.3 = 0.859\n",
    "\n",
    "步骤3：应用sigmoid\n",
    "output = 1 / (1 + e^(-0.859)) ≈ 1 / (1 + 0.424) ≈ 1 / 1.424 ≈ 0.702\n",
    "\n",
    "答案：0.702（保留三位小数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bd411c3-94e5-42fe-a5a9-2037e2fceeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25, 0.48000000000000004, 0.859, np.float64(0.7024516833527672))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = 1.0*0.1 + 0.5*0.3 + (-0.2)*0.5 + 0.1\n",
    "h2 = 1.0*0.2 + 0.5*0.4 + (-0.2)*0.6 + 0.2\n",
    "z = 0.25*0.7 + 0.48*0.8 + 0.3\n",
    "output = 1 / (1 + np.exp(-0.859))\n",
    "h1, h2, z, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8244f-54a6-4b6f-bec2-2ad87e2b527d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de3a33ee-8e19-4041-a83d-075fddc9d965",
   "metadata": {},
   "source": [
    "### Implementing Gradient Descent in Python: The Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62469f88-54a4-4853-9c42-67d7fa841b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/m) * np.sum(np.square(predictions-y)) # Compute mean square error\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    theta_history = np.zeros((iterations,2))\n",
    "    for i in range(iterations): # Iterate until convergence\n",
    "        prediction = np.dot(X,theta)  # Matrix multiplication between X and theta\n",
    "        theta = theta - (1/m)*alpha*(X.T.dot((prediction - y))) # Gradient update rule\n",
    "        theta_history[i,:] = theta.T\n",
    "        cost_history[i] = cost(X,y,theta)\n",
    "    return theta, cost_history, theta_history\n",
    "\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 +3 * X+np.random.randn(100,1)\n",
    "\n",
    "lr = 0.01 # Learning Rate\n",
    "n_iter = 1000 # Max number of iterations\n",
    "theta = np.random.randn(2,1) # Randomly initialized parameters\n",
    "X_b = np.c_[np.ones((len(X),1)),X] # add bias parameter to X\n",
    "theta, cost_history, theta_history = gradient_descent(X_b,y,theta,lr,n_iter) # Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a71cc-c920-4131-8138-7286f5d6a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cost_function(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "def logistic_regression(X, y, num_iterations, learning_rate):\n",
    "    # Add intercept to X\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "    # Weights initialization\n",
    "    theta = np.zeros(X.shape[1])\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        z = np.dot(X, theta)\n",
    "        h = sigmoid(z)\n",
    "        gradient = np.dot(X.T, (h - y)) / y.size\n",
    "        theta -= learning_rate * gradient\n",
    "\n",
    "        z = np.dot(X, theta)\n",
    "        h = sigmoid(z)\n",
    "        loss = cost_function(h, y)\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(f'Loss: {loss}\\t')\n",
    "\n",
    "    return theta\n",
    "\n",
    "def predict_prob(X, theta):\n",
    "    # Add intercept to X\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    X = np.concatenate((intercept, X), axis=1)\n",
    "    return sigmoid(np.dot(X, theta))\n",
    "\n",
    "def predict(X, theta, threshold=0.5):\n",
    "    return predict_prob(X, theta) >= threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e549b5f-ecc1-42c1-9b95-95ac285caa4d",
   "metadata": {},
   "source": [
    "### Classification Algorithms and Metrics\n",
    "#### k-Nearest Neighbors (k-NN) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52372e4b-f202-43f0-848e-8fc66e4b7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# The 'euclidean_distance' function computes the Euclidean distance between two points\n",
    "def euclidean_distance(point1, point2):\n",
    "    # print(point1, point2)\n",
    "    squares = [(p - q) ** 2 for p, q in zip(point1, point2)] # Calculate squared distance for each dimension\n",
    "    return math.sqrt(sum(squares)) # Return the square root of the sum of squares\n",
    "\n",
    "# Test it\n",
    "point1 = (1, 2) # The coordinates of the first point\n",
    "point2 = (4, 6) # The coordinates of the second point\n",
    "print(euclidean_distance(point1, point2)) # 5.0\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def k_nearest_neighbors(data, query, k, distance_fn):\n",
    "    neighbor_distances_and_indices = []\n",
    "    \n",
    "    # # Compute distance from each training data point\n",
    "    # for idx, label in enumerate(data):\n",
    "    #     distance = distance_fn(label[:-1], query)\n",
    "    #     neighbor_distances_and_indices.append((distance, idx))\n",
    "    for idx, (features, label) in enumerate(data):  # Unpack correctly here\n",
    "        distance = distance_fn(features, query)  # Use features directly\n",
    "        neighbor_distances_and_indices.append((distance, idx, label))  # Store label too\n",
    "\n",
    "    # Sort array by distance\n",
    "    sorted_neighbor_distances_and_indices = sorted(neighbor_distances_and_indices)\n",
    "    \n",
    "    # Select k closest data points\n",
    "    k_nearest_distances_and_indices = sorted_neighbor_distances_and_indices[:k]\n",
    "    \n",
    "    # Obtain class labels for those k data points\n",
    "    # k_nearest_labels = [data[i][1] for distance, i in k_nearest_distances_and_indices]\n",
    "    k_nearest_labels = [label for _, _, label in k_nearest_distances_and_indices]\n",
    "    \n",
    "    # Majority vote\n",
    "    most_common = Counter(k_nearest_labels).most_common(1)\n",
    "    return most_common[0][0] # Return the label of the class that receives the majority vote\n",
    "\n",
    "# Define the dataset (training set)\n",
    "# Each element of the dataset is a tuple (features, label)\n",
    "data = [\n",
    "    ((2, 3), 0),\n",
    "    ((5, 4), 0),\n",
    "    ((9, 6), 1),\n",
    "    ((4, 7), 0),\n",
    "    ((8, 1), 1),\n",
    "    ((7, 2), 1)\n",
    "]\n",
    "query = (5, 3)  # test point\n",
    "\n",
    "# Perform the classification\n",
    "predicted_label = k_nearest_neighbors(data, query, k=3, distance_fn=euclidean_distance)\n",
    "print(predicted_label)  # Expected class label is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6369c15-89d6-47b9-9d75-7fe89e36e706",
   "metadata": {},
   "source": [
    "#### Implementing Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326f7dc9-a8c7-47f1-a94e-5b29619da8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Weather:  Snowy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_prior_probabilities(y):\n",
    "    # Calculate prior probabilities for each class\n",
    "    return y.value_counts(normalize=True)\n",
    "\n",
    "def calculate_likelihoods(X, y):\n",
    "    likelihoods = {}\n",
    "    for column in X.columns:\n",
    "        likelihoods[column] = {}\n",
    "        for class_ in y.unique():\n",
    "            # Filter feature column data for each class\n",
    "            class_data = X[y == class_][column]\n",
    "            counts = class_data.value_counts()\n",
    "            total_count = len(class_data)  # Total count of instances for current class\n",
    "            likelihoods[column][class_] = counts / total_count  # Direct likelihoods without smoothing\n",
    "    return likelihoods\n",
    "\n",
    "def naive_bayes_classifier(X_test, priors, likelihoods):\n",
    "    predictions = []\n",
    "    for _, data_point in X_test.iterrows():\n",
    "        class_probabilities = {}\n",
    "        for class_ in priors.index:\n",
    "            class_probabilities[class_] = priors[class_]\n",
    "            for feature in X_test.columns:\n",
    "                # Use .get to safely retrieve probability and get a default of 1/total to handle unseen values\n",
    "                feature_probs = likelihoods[feature][class_]\n",
    "                class_probabilities[class_] *= feature_probs.get(data_point[feature], 1 / (len(feature_probs) + 1))\n",
    "\n",
    "        # Predict class with maximum posterior probability\n",
    "        predictions.append(max(class_probabilities, key=class_probabilities.get))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def calculate_likelihoods_with_smoothing(X, y):\n",
    "    likelihoods = {}\n",
    "    for column in X.columns:\n",
    "        likelihoods[column] = {}\n",
    "        for class_ in y.unique():\n",
    "            # Calculate normalized counts with smoothing\n",
    "            class_data = X[y == class_][column]\n",
    "            counts = class_data.value_counts()\n",
    "            total_count = len(class_data) + len(X[column].unique())  # total count with smoothing\n",
    "            likelihoods[column][class_] = (counts + 1) / total_count  # add-1 smoothing\n",
    "    return likelihoods\n",
    "\n",
    "data = {\n",
    "    'Temperature': ['Hot', 'Hot', 'Cold', 'Hot', 'Cold', 'Cold', 'Cold'],\n",
    "    'Humidity': ['High', 'High', 'Normal', 'Normal', 'High', 'Normal', 'Normal'],\n",
    "    'Weather': ['Sunny', 'Sunny', 'Snowy', 'Rainy', 'Snowy', 'Snowy', 'Sunny']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split features and labels\n",
    "X = df[['Temperature', 'Humidity']]\n",
    "y = df['Weather']\n",
    "\n",
    "# Calculate prior probabilities\n",
    "priors = calculate_prior_probabilities(y)\n",
    "\n",
    "# Calculate likelihoods with smoothing\n",
    "likelihoods = calculate_likelihoods_with_smoothing(X, y)\n",
    "\n",
    "# New observation\n",
    "X_test = pd.DataFrame([{'Temperature': 'Cold', 'Humidity': 'Normal'}])\n",
    "\n",
    "# Make prediction\n",
    "prediction = naive_bayes_classifier(X_test, priors, likelihoods)\n",
    "print(\"Predicted Weather: \", prediction[0])  # Output: Predicted Weather:  Snowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d54eea0-439b-4808-a434-3be61c4254fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y, priors, X, X.columns, likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6086e53c-c66a-4096-9c2b-422ceab0868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive Bayes Classification Example ===\n",
      "\n",
      "1. Without Smoothing:\n",
      "Test sample 1: ['Sunny', 'Cool', 'High', 'Strong'] -> Predicted: No\n",
      "Test sample 2: ['Rain', 'Mild', 'High', 'Weak'] -> Predicted: Yes\n",
      "\n",
      "2. With Laplace Smoothing:\n",
      "Test sample 1: ['Sunny', 'Cool', 'High', 'Strong'] -> Predicted: No\n",
      "Test sample 2: ['Rain', 'Mild', 'High', 'Weak'] -> Predicted: Yes\n",
      "\n",
      "3. Probability Estimates for Test Sample 1:\n",
      "  P(No | features) = 0.7201\n",
      "  P(Yes | features) = 0.2799\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_prior_probabilities(y):\n",
    "    \"\"\"\n",
    "    Calculate prior probabilities P(Class)\n",
    "    \n",
    "    Args:\n",
    "        y: list of class labels\n",
    "    Returns:\n",
    "        dict: {class: probability}\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    total = len(y)\n",
    "    \n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "    \n",
    "    priors = {}\n",
    "    for label, count in counts.items():\n",
    "        priors[label] = count / total\n",
    "    \n",
    "    return priors\n",
    "\n",
    "\n",
    "def calculate_likelihoods(X, y):\n",
    "    \"\"\"\n",
    "    Calculate likelihoods P(Feature|Class) without smoothing\n",
    "    \n",
    "    Args:\n",
    "        X: list of lists, shape [n_samples, n_features]\n",
    "        y: list of class labels, length n_samples\n",
    "    Returns:\n",
    "        dict: {feature_index: {class: {feature_value: probability}}}\n",
    "    \"\"\"\n",
    "    # Get unique classes\n",
    "    unique_classes = list(set(y))\n",
    "    n_features = len(X[0]) if X else 0\n",
    "    \n",
    "    # Initialize data structures\n",
    "    likelihoods = {}\n",
    "    for feat_idx in range(n_features):\n",
    "        likelihoods[feat_idx] = {}\n",
    "        for class_ in unique_classes:\n",
    "            likelihoods[feat_idx][class_] = {}\n",
    "    \n",
    "    # Count class occurrences for each feature value\n",
    "    class_counts = {}\n",
    "    for class_ in unique_classes:\n",
    "        class_counts[class_] = 0\n",
    "    \n",
    "    # First pass: count total instances per class\n",
    "    for i in range(len(y)):\n",
    "        class_label = y[i]\n",
    "        class_counts[class_label] = class_counts.get(class_label, 0) + 1\n",
    "    \n",
    "    # Second pass: count feature values per class\n",
    "    for i in range(len(X)):\n",
    "        class_label = y[i]\n",
    "        features = X[i]\n",
    "        \n",
    "        for feat_idx, feat_value in enumerate(features):\n",
    "            # Initialize if not exists\n",
    "            if feat_value not in likelihoods[feat_idx][class_label]:\n",
    "                likelihoods[feat_idx][class_label][feat_value] = 0\n",
    "            \n",
    "            likelihoods[feat_idx][class_label][feat_value] += 1\n",
    "    \n",
    "    # Convert counts to probabilities\n",
    "    for feat_idx in likelihoods:\n",
    "        for class_label in likelihoods[feat_idx]:\n",
    "            total_in_class = class_counts[class_label]\n",
    "            for feat_value in likelihoods[feat_idx][class_label]:\n",
    "                count = likelihoods[feat_idx][class_label][feat_value]\n",
    "                likelihoods[feat_idx][class_label][feat_value] = count / total_in_class\n",
    "    \n",
    "    return likelihoods, class_counts\n",
    "\n",
    "\n",
    "def calculate_likelihoods_with_smoothing(X, y):\n",
    "    \"\"\"\n",
    "    Calculate likelihoods P(Feature|Class) with Laplace (add-1) smoothing\n",
    "    \n",
    "    Args:\n",
    "        X: list of lists, shape [n_samples, n_features]\n",
    "        y: list of class labels, length n_samples\n",
    "    Returns:\n",
    "        dict: {feature_index: {class: {feature_value: probability}}}\n",
    "    \"\"\"\n",
    "    # Get unique classes\n",
    "    unique_classes = list(set(y))\n",
    "    n_features = len(X[0]) if X else 0\n",
    "    \n",
    "    # Find all unique values for each feature\n",
    "    unique_feature_values = []\n",
    "    for feat_idx in range(n_features):\n",
    "        values = set()\n",
    "        for sample in X:\n",
    "            values.add(sample[feat_idx])\n",
    "        unique_feature_values.append(list(values))\n",
    "    \n",
    "    # Initialize data structures\n",
    "    likelihoods = {}\n",
    "    for feat_idx in range(n_features):\n",
    "        likelihoods[feat_idx] = {}\n",
    "        for class_ in unique_classes:\n",
    "            likelihoods[feat_idx][class_] = {}\n",
    "            # Initialize all feature values for this class\n",
    "            for value in unique_feature_values[feat_idx]:\n",
    "                likelihoods[feat_idx][class_][value] = 0\n",
    "    \n",
    "    # Count class occurrences\n",
    "    class_counts = {}\n",
    "    for class_ in unique_classes:\n",
    "        class_counts[class_] = 0\n",
    "    \n",
    "    # Count occurrences\n",
    "    for i in range(len(X)):\n",
    "        class_label = y[i]\n",
    "        features = X[i]\n",
    "        class_counts[class_label] += 1\n",
    "        \n",
    "        for feat_idx, feat_value in enumerate(features):\n",
    "            likelihoods[feat_idx][class_label][feat_value] += 1\n",
    "    \n",
    "    # Apply Laplace smoothing and convert to probabilities\n",
    "    for feat_idx in likelihoods:\n",
    "        num_unique_values = len(unique_feature_values[feat_idx])\n",
    "        \n",
    "        for class_label in likelihoods[feat_idx]:\n",
    "            total_in_class = class_counts[class_label]\n",
    "            \n",
    "            for feat_value in likelihoods[feat_idx][class_label]:\n",
    "                count = likelihoods[feat_idx][class_label][feat_value]\n",
    "                # Laplace smoothing: (count + 1) / (total_in_class + num_unique_values)\n",
    "                likelihoods[feat_idx][class_label][feat_value] = (count + 1) / (total_in_class + num_unique_values)\n",
    "    \n",
    "    return likelihoods, class_counts, unique_feature_values\n",
    "\n",
    "\n",
    "def naive_bayes_classifier(X_test, priors, likelihoods, use_log=True):\n",
    "    \"\"\"\n",
    "    Classify test samples using Naive Bayes\n",
    "    \n",
    "    Args:\n",
    "        X_test: list of lists, test samples\n",
    "        priors: dict from calculate_prior_probabilities\n",
    "        likelihoods: dict from calculate_likelihoods\n",
    "        use_log: if True, use log probabilities to avoid underflow\n",
    "    Returns:\n",
    "        list: predicted class labels\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    unique_classes = list(priors.keys())\n",
    "    n_features = len(X_test[0]) if X_test else 0\n",
    "    \n",
    "    for sample in X_test:\n",
    "        best_class = None\n",
    "        best_score = -float('inf') if use_log else 0\n",
    "        \n",
    "        for class_ in unique_classes:\n",
    "            # Start with prior probability\n",
    "            if use_log:\n",
    "                score = math.log(priors[class_])  # log(P(class))\n",
    "            else:\n",
    "                score = priors[class_]  # P(class)\n",
    "            \n",
    "            # Multiply by likelihoods for each feature\n",
    "            for feat_idx, feat_value in enumerate(sample):\n",
    "                if feat_idx in likelihoods and class_ in likelihoods[feat_idx]:\n",
    "                    # Get probability, use small value if feature value not seen\n",
    "                    prob = likelihoods[feat_idx][class_].get(feat_value, 1e-10)\n",
    "                    \n",
    "                    if use_log:\n",
    "                        score += math.log(prob)  # log(P(feature|class))\n",
    "                    else:\n",
    "                        score *= prob  # P(feature|class)\n",
    "            \n",
    "            # Update best class\n",
    "            if (use_log and score > best_score) or (not use_log and score > best_score):\n",
    "                best_score = score\n",
    "                best_class = class_\n",
    "        \n",
    "        predictions.append(best_class)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def predict_proba(X_test, priors, likelihoods, use_log=True):\n",
    "    \"\"\"\n",
    "    Get probability estimates for each class\n",
    "    \n",
    "    Args:\n",
    "        X_test: list of lists, test samples\n",
    "        priors: dict from calculate_prior_probabilities\n",
    "        likelihoods: dict from calculate_likelihoods\n",
    "        use_log: if True, use log probabilities\n",
    "    Returns:\n",
    "        list of dicts: [{class: probability}, ...] for each test sample\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    unique_classes = list(priors.keys())\n",
    "    n_features = len(X_test[0]) if X_test else 0\n",
    "    \n",
    "    for sample in X_test:\n",
    "        class_scores = {}\n",
    "        \n",
    "        # Calculate raw scores (log or linear)\n",
    "        for class_ in unique_classes:\n",
    "            if use_log:\n",
    "                score = math.log(priors[class_])\n",
    "            else:\n",
    "                score = priors[class_]\n",
    "            \n",
    "            for feat_idx, feat_value in enumerate(sample):\n",
    "                if feat_idx in likelihoods and class_ in likelihoods[feat_idx]:\n",
    "                    prob = likelihoods[feat_idx][class_].get(feat_value, 1e-10)\n",
    "                    if use_log:\n",
    "                        score += math.log(prob)\n",
    "                    else:\n",
    "                        score *= prob\n",
    "            \n",
    "            class_scores[class_] = score\n",
    "        \n",
    "        # Convert to probabilities (normalize)\n",
    "        if use_log:\n",
    "            # Convert from log space: exp(score) / sum(exp(scores))\n",
    "            # Use log-sum-exp trick for numerical stability\n",
    "            max_score = max(class_scores.values())\n",
    "            exp_scores = {}\n",
    "            total = 0\n",
    "            \n",
    "            for class_, score in class_scores.items():\n",
    "                exp_score = math.exp(score - max_score)\n",
    "                exp_scores[class_] = exp_score\n",
    "                total += exp_score\n",
    "            \n",
    "            # Normalize\n",
    "            probabilities = {}\n",
    "            for class_, exp_score in exp_scores.items():\n",
    "                probabilities[class_] = exp_score / total\n",
    "        else:\n",
    "            # Normalize linear probabilities\n",
    "            total = sum(class_scores.values())\n",
    "            probabilities = {}\n",
    "            for class_, score in class_scores.items():\n",
    "                probabilities[class_] = score / total\n",
    "        \n",
    "        predictions.append(probabilities)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Example usage with simple data\n",
    "def example():\n",
    "    # Training data: [Outlook, Temperature, Humidity, Wind]\n",
    "    X_train = [\n",
    "        ['Sunny', 'Hot', 'High', 'Weak'],\n",
    "        ['Sunny', 'Hot', 'High', 'Strong'],\n",
    "        ['Overcast', 'Hot', 'High', 'Weak'],\n",
    "        ['Rain', 'Mild', 'High', 'Weak'],\n",
    "        ['Rain', 'Cool', 'Normal', 'Weak'],\n",
    "        ['Rain', 'Cool', 'Normal', 'Strong'],\n",
    "        ['Overcast', 'Cool', 'Normal', 'Strong'],\n",
    "        ['Sunny', 'Mild', 'High', 'Weak'],\n",
    "        ['Sunny', 'Cool', 'Normal', 'Weak'],\n",
    "        ['Rain', 'Mild', 'Normal', 'Weak'],\n",
    "        ['Sunny', 'Mild', 'Normal', 'Strong'],\n",
    "        ['Overcast', 'Mild', 'High', 'Strong'],\n",
    "        ['Overcast', 'Hot', 'Normal', 'Weak'],\n",
    "        ['Rain', 'Mild', 'High', 'Strong']\n",
    "    ]\n",
    "    \n",
    "    # Target: PlayTennis? (Yes/No)\n",
    "    y_train = ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
    "    \n",
    "    # Test data\n",
    "    X_test = [\n",
    "        ['Sunny', 'Cool', 'High', 'Strong'],\n",
    "        ['Rain', 'Mild', 'High', 'Weak']\n",
    "    ]\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    print(\"=== Naive Bayes Classification Example ===\")\n",
    "    \n",
    "    # Without smoothing\n",
    "    print(\"\\n1. Without Smoothing:\")\n",
    "    priors = calculate_prior_probabilities(y_train)\n",
    "    likelihoods, _ = calculate_likelihoods(X_train, y_train)\n",
    "    \n",
    "    predictions = naive_bayes_classifier(X_test, priors, likelihoods, use_log=True)\n",
    "    print(f\"Test sample 1: {X_test[0]} -> Predicted: {predictions[0]}\")\n",
    "    print(f\"Test sample 2: {X_test[1]} -> Predicted: {predictions[1]}\")\n",
    "    \n",
    "    # With smoothing\n",
    "    print(\"\\n2. With Laplace Smoothing:\")\n",
    "    likelihoods_smooth, _, _ = calculate_likelihoods_with_smoothing(X_train, y_train)\n",
    "    \n",
    "    predictions_smooth = naive_bayes_classifier(X_test, priors, likelihoods_smooth, use_log=True)\n",
    "    print(f\"Test sample 1: {X_test[0]} -> Predicted: {predictions_smooth[0]}\")\n",
    "    print(f\"Test sample 2: {X_test[1]} -> Predicted: {predictions_smooth[1]}\")\n",
    "    \n",
    "    # Get probability estimates\n",
    "    print(\"\\n3. Probability Estimates for Test Sample 1:\")\n",
    "    proba = predict_proba([X_test[0]], priors, likelihoods_smooth, use_log=True)\n",
    "    for class_, prob in proba[0].items():\n",
    "        print(f\"  P({class_} | features) = {prob:.4f}\")\n",
    "    \n",
    "    return predictions_smooth\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46a419-1277-4f23-b66d-77d60ac5e13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7de9be9d-e7a9-41ac-aa02-7b6dcb689ff5",
   "metadata": {},
   "source": [
    "Naive Bayes实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e62968-1d59-4aa0-8135-bd8b0f1c8308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.feature_likelihoods = defaultdict(list) # 存储每个类别下每个特征取值的概率\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # 计算先验概率 P(y)\n",
    "        total_samples = len(y)\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        for cls, cnt in zip(classes, counts):\n",
    "            self.class_priors[cls] = cnt / total_samples\n",
    "            \n",
    "        # 计算似然概率 P(x|y) - 这里以离散特征为例\n",
    "        # 实际面试中需根据题目要求处理连续特征（如假设高斯分布）\n",
    "        for cls in classes:\n",
    "            X_cls = X[y == cls]\n",
    "            for feature_idx in range(X.shape[1]):\n",
    "                feature_vals, val_counts = np.unique(X_cls[:, feature_idx], return_counts=True)\n",
    "                probabilities = val_counts / len(X_cls)\n",
    "                self.feature_likelihoods[(cls, feature_idx)] = dict(zip(feature_vals, probabilities))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            posteriors = {}\n",
    "            for cls, prior in self.class_priors.items():\n",
    "                likelihood = 1.0\n",
    "                for feature_idx, feature_val in enumerate(sample):\n",
    "                    # 获取该特征值在给定类别下的概率，如果未见则使用平滑（如拉普拉斯平滑）\n",
    "                    prob_dict = self.feature_likelihoods.get((cls, feature_idx), {})\n",
    "                    likelihood *= prob_dict.get(feature_val, 1e-6) # 使用一个极小值做平滑\n",
    "                posteriors[cls] = prior * likelihood\n",
    "            predictions.append(max(posteriors, key=posteriors.get))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e90ff-c24a-423b-af34-566ff8747272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, alpha=1):\n",
    "    # ... 计算先验概率 ...\n",
    "    \n",
    "    for cls in classes:\n",
    "        X_cls = X[y == cls]\n",
    "        n_cls = len(X_cls)\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            # 计算每个特征值的出现次数\n",
    "            feature_vals, val_counts = np.unique(X_cls[:, feature_idx], return_counts=True)\n",
    "            # 获取该特征所有可能取值（来自整个训练集）\n",
    "            all_vals = np.unique(X[:, feature_idx])\n",
    "            V = len(all_vals)  # 可能取值数\n",
    "            \n",
    "            # 拉普拉斯平滑\n",
    "            probabilities = {}\n",
    "            for val, count in zip(feature_vals, val_counts):\n",
    "                probabilities[val] = (count + alpha) / (n_cls + alpha * V)\n",
    "            # 为未出现的值也分配概率\n",
    "            for val in all_vals:\n",
    "                if val not in probabilities:\n",
    "                    probabilities[val] = alpha / (n_cls + alpha * V)\n",
    "                    \n",
    "            self.feature_likelihoods[(cls, feature_idx)] = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81de124-9614-48e5-b01d-9c5cae0b7b7b",
   "metadata": {},
   "source": [
    "#### Bernoulli Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11bb864-d25b-4777-98f3-5b78a32fb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes():\n",
    "    def __init__(self, smoothing=1.0):\n",
    "        self.smoothing = smoothing\n",
    "        self.classes = None\n",
    "        self.priors = None\n",
    "        self.likelihoods = None\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        self.classes, class_counts = np.unique(y, return_counts=True)\n",
    "        self.priors = {cls: np.log(class_counts[i] / len(y)) for i, cls in enumerate(self.classes)}\n",
    "        self.likelihoods = {}\n",
    "        for cls in self.classes:\n",
    "            X_cls = X[y == cls]\n",
    "            prob = (np.sum(X_cls, axis=0) + self.smoothing) / (X_cls.shape[0] + 2 * self.smoothing)\n",
    "            self.likelihoods[cls] = (np.log(prob), np.log(1 - prob))\n",
    "\n",
    "    def _compute_posterior(self, sample):\n",
    "        posteriors = {}\n",
    "        for cls in self.classes:\n",
    "            posterior = self.priors[cls]\n",
    "            prob_1, prob_0 = self.likelihoods[cls]\n",
    "            likelihood = np.sum(sample * prob_1 + (1 - sample) * prob_0)\n",
    "            posterior += likelihood\n",
    "            posteriors[cls] = posterior\n",
    "        return max(posteriors, key=posteriors.get)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._compute_posterior(sample) for sample in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465788dd-33dc-469e-9224-c3aeac013dc0",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37aa807-ccd5-4a93-baef-6c3dfc54290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray:\n",
    "\t\"\"\"\n",
    "\tImplements Gaussian Naive Bayes classifier.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tX_train: Training features (shape: N_train x D)\n",
    "\t\ty_train: Training labels (shape: N_train)\n",
    "\t\tX_test: Test features (shape: N_test x D)\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tPredicted class labels for X_test (shape: N_test)\n",
    "\t\"\"\"\n",
    "\tclasses = np.unique(y_train)\n",
    "\tn_classes = len(classes)\n",
    "\tn_features = X_train.shape[1]\n",
    "\t\n",
    "\t# Compute class priors, means, and variances\n",
    "\tpriors = np.zeros(n_classes)\n",
    "\tmeans = np.zeros((n_classes, n_features))\n",
    "\tvariances = np.zeros((n_classes, n_features))\n",
    "\t\n",
    "\tfor idx, c in enumerate(classes):\n",
    "\t\tX_c = X_train[y_train == c]\n",
    "\t\tpriors[idx] = X_c.shape[0] / X_train.shape[0]\n",
    "\t\tmeans[idx] = np.mean(X_c, axis=0)\n",
    "\t\tvariances[idx] = np.var(X_c, axis=0)\n",
    "\t\n",
    "\t# Add small epsilon for numerical stability\n",
    "\tvariances = variances + 1e-9\n",
    "\t\n",
    "\t# Predict for test samples\n",
    "\tpredictions = []\n",
    "\tfor x in X_test:\n",
    "\t\tposteriors = []\n",
    "\t\tfor idx, c in enumerate(classes):\n",
    "\t\t\t# Log prior\n",
    "\t\t\tlog_prior = np.log(priors[idx])\n",
    "\t\t\t# Log likelihood (Gaussian)\n",
    "\t\t\tlog_likelihood = -0.5 * np.sum(np.log(2 * np.pi * variances[idx]) + \n",
    "\t\t\t\t\t\t\t\t\t\t   (x - means[idx])**2 / variances[idx])\n",
    "\t\t\tposteriors.append(log_prior + log_likelihood)\n",
    "\t\tpredictions.append(classes[np.argmax(posteriors)])\n",
    "\t\n",
    "\treturn np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41812981-4311-482f-a3af-5d55ae999993",
   "metadata": {},
   "source": [
    "### Implementing Decision Tree Building in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f04c44-14da-498a-a38d-819cee1d6981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Split:\n",
      "Column Index: 0, Value: 20\n",
      "([[18, 1, 0]], [[20, 0, 1], [23, 2, 1], [25, 1, 1], [30, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "def gini_index(groups, classes):\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = len(group)\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            score += p * p\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini\n",
    "\n",
    "\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "def get_split(dataset):\n",
    "    class_values = list(set(row[-1] for row in dataset))  # Find unique classes in the dataset\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(len(dataset[0])-1):    # Exclude the last column which is the class\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
    "\n",
    "\n",
    "# Age, Movie Genre, Decision (watch or not)\n",
    "\n",
    "dataset = [\n",
    "    [18, 1, 0],\n",
    "    [20, 0, 1],\n",
    "    [23, 2, 1],\n",
    "    [25, 1, 1],\n",
    "    [30, 1, 0],\n",
    "]\n",
    "\n",
    "split = get_split(dataset)\n",
    "print('\\nBest Split:')\n",
    "print('Column Index: %s, Value: %s' % ((split['index']), (split['value'])))\n",
    "print(split['groups'])\n",
    "# Output: Column Index: 0, Value: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eae16334-a1ae-4b1a-ba5e-5ce6d4371ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree...\n",
      "\n",
      "Tree structure:\n",
      "[Feature 0 < 8.0]\n",
      "  [Class 0]\n",
      "  [Feature 1 < 6.0]\n",
      "    [Class 1]\n",
      "    [Class 0]\n",
      "\n",
      "Making predictions:\n",
      "Sample [7, 3] → Predicted class: 0\n",
      "Sample [11, 5] → Predicted class: 1\n",
      "Sample [13, 9] → Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def gini_index(groups, classes):\n",
    "    \"\"\"Calculate Gini impurity for split groups\"\"\"\n",
    "    n_instances = float(sum(len(group) for group in groups))\n",
    "    if n_instances == 0:\n",
    "        return 0\n",
    "    \n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = len(group)\n",
    "        if size == 0:\n",
    "            continue\n",
    "        \n",
    "        score = 0.0\n",
    "        # Count proportion of each class\n",
    "        for class_val in classes:\n",
    "            proportion = sum(1 for row in group if row[-1] == class_val) / size\n",
    "            score += proportion * proportion\n",
    "        \n",
    "        # Weight by group size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    \n",
    "    return gini\n",
    "\n",
    "\n",
    "def test_split(index, value, dataset):\n",
    "    \"\"\"Split dataset based on feature threshold\"\"\"\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "def get_split(dataset):\n",
    "    \"\"\"Find the best split point for dataset\"\"\"\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "\n",
    "    if len(class_values) == 1:\n",
    "        return None  # All same class!\n",
    "            \n",
    "    b_index, b_value, b_score, b_groups = 999, 999, float('inf'), None\n",
    "    \n",
    "    for index in range(len(dataset[0]) - 1):  # Exclude class column\n",
    "        # Get unique values for this feature, sorted\n",
    "        feature_values = sorted({row[index] for row in dataset})\n",
    "        \n",
    "        # Test thresholds between consecutive values\n",
    "        for i in range(len(feature_values) - 1):\n",
    "            threshold = (feature_values[i] + feature_values[i + 1]) / 2.0\n",
    "            left, right = test_split(index, threshold, dataset)\n",
    "            groups = (left, right)\n",
    "            \n",
    "            gini = gini_index(groups, class_values)\n",
    "            \n",
    "            # Update best split if better\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, threshold, gini, groups\n",
    "    \n",
    "    # # If no valid split found (e.g., all same class)\n",
    "    # if b_groups is None:\n",
    "    #     return None\n",
    "    \n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
    "\n",
    "\n",
    "def create_terminal(group):\n",
    "    \"\"\"Create terminal/leaf node with majority class\"\"\"\n",
    "    if not group:\n",
    "        return None\n",
    "    \n",
    "    outcomes = [row[-1] for row in group]\n",
    "    # Return most common class\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    \"\"\"Build decision tree\"\"\"\n",
    "    root = get_split(train)\n",
    "    if root is None:  # All same class or empty\n",
    "        return create_terminal(train)\n",
    "    \n",
    "    recurse_split(root, max_depth, min_size, 1)\n",
    "    return root\n",
    "\n",
    "\n",
    "def recurse_split(node, max_depth, min_size, depth):\n",
    "    \"\"\"Recursively split nodes\"\"\"\n",
    "    if node is None:\n",
    "        return\n",
    "    \n",
    "    left, right = node['groups']\n",
    "    del node['groups']  # Clean up\n",
    "    \n",
    "    # Check for empty groups\n",
    "    if not left or not right:\n",
    "        terminal_class = create_terminal(left + right)\n",
    "        node['left'] = node['right'] = terminal_class\n",
    "        return\n",
    "    \n",
    "    # Check max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'] = create_terminal(left)\n",
    "        node['right'] = create_terminal(right)\n",
    "        return\n",
    "    \n",
    "    # Process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = create_terminal(left)\n",
    "    else:\n",
    "        left_split = get_split(left)\n",
    "        if left_split:\n",
    "            node['left'] = left_split\n",
    "            recurse_split(node['left'], max_depth, min_size, depth + 1)\n",
    "        else:\n",
    "            node['left'] = create_terminal(left)\n",
    "    \n",
    "    # Process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = create_terminal(right)\n",
    "    else:\n",
    "        right_split = get_split(right)\n",
    "        if right_split:\n",
    "            node['right'] = right_split\n",
    "            recurse_split(node['right'], max_depth, min_size, depth + 1)\n",
    "        else:\n",
    "            node['right'] = create_terminal(right)\n",
    "\n",
    "\n",
    "def predict(node, row):\n",
    "    \"\"\"Make prediction for a single row\"\"\"\n",
    "    if not isinstance(node, dict):\n",
    "        return node  # Leaf node, return class\n",
    "    \n",
    "    if row[node['index']] < node['value']:\n",
    "        return predict(node['left'], row)\n",
    "    else:\n",
    "        return predict(node['right'], row)\n",
    "# def predict(node, sample):\n",
    "#     \"\"\"\n",
    "#     Predict class for a single sample by traversing the tree\n",
    "    \n",
    "#     Args:\n",
    "#         node: Current node in the tree (starts with root)\n",
    "#         sample: Feature values [f1, f2, ...] (without class label)\n",
    "    \n",
    "#     Returns: Predicted class\n",
    "#     \"\"\"\n",
    "#     # If we've reached a leaf node (not a dictionary), return the class\n",
    "#     if not isinstance(node, dict):\n",
    "#         return node\n",
    "    \n",
    "#     # Get the feature index and threshold value for this split\n",
    "#     feature_index = node['index']      # Which feature to check\n",
    "#     threshold = node['value']          # Threshold value for the split\n",
    "    \n",
    "#     # Get the feature value from the sample\n",
    "#     feature_value = sample[feature_index]\n",
    "    \n",
    "#     # Decide which child to go to based on the comparison\n",
    "#     if feature_value < threshold:\n",
    "#         return predict(node['left'], sample)   # Go left\n",
    "#     else:\n",
    "#         return predict(node['right'], sample)  # Go right\n",
    "\n",
    "# Test the corrected code\n",
    "dataset = [\n",
    "    [5, 3, 0], [6, 3, 0], [6, 4, 0], [10, 3, 1],\n",
    "    [11, 4, 1], [12, 8, 0], [5, 5, 0], [12, 4, 1]\n",
    "]\n",
    "\n",
    "print(\"Building tree...\")\n",
    "tree = build_tree(dataset, max_depth=2, min_size=1)\n",
    "\n",
    "print(\"\\nTree structure:\")\n",
    "def print_tree(node, depth=0):\n",
    "    indent = \"  \" * depth\n",
    "    if isinstance(node, dict):\n",
    "        print(f\"{indent}[Feature {node['index']} < {node['value']:.1f}]\")\n",
    "        print_tree(node['left'], depth + 1)\n",
    "        print_tree(node['right'], depth + 1)\n",
    "    else:\n",
    "        print(f\"{indent}[Class {node}]\")\n",
    "\n",
    "print_tree(tree)\n",
    "\n",
    "print(\"\\nMaking predictions:\")\n",
    "test_samples = [\n",
    "    [7, 3],   # Should predict 0 (Age < 10)\n",
    "    [11, 5],  # Should predict 1 (Age ≥ 10, Feature2 < 8)\n",
    "    [13, 9]   # Should predict 0 (Age ≥ 10, Feature2 ≥ 8)\n",
    "]\n",
    "\n",
    "for sample in test_samples:\n",
    "    # Add dummy class for prediction (will be ignored)\n",
    "    prediction = predict(tree, sample + [-1])\n",
    "    print(f\"Sample {sample} → Predicted class: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f30e0-937a-4744-bcd0-7799e3e80764",
   "metadata": {},
   "source": [
    "Decision Tree 原理与实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558019c-4c5f-4d27-a212-afb1059973b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.tree) for x in X])\n",
    "    \n",
    "    # ========== 核心递归函数 ==========\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # 停止条件\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            n_classes == 1):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'type': 'leaf', 'value': leaf_value}\n",
    "        \n",
    "        # 寻找最佳分裂\n",
    "        best_feat, best_thresh = self._best_split(X, y)\n",
    "        if best_feat is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return {'type': 'leaf', 'value': leaf_value}\n",
    "        \n",
    "        # 递归构建子树\n",
    "        left_idx = X[:, best_feat] <= best_thresh\n",
    "        right_idx = X[:, best_feat] > best_thresh\n",
    "        \n",
    "        left_subtree = self._grow_tree(X[left_idx], y[left_idx], depth+1)\n",
    "        right_subtree = self._grow_tree(X[right_idx], y[right_idx], depth+1)\n",
    "        \n",
    "        return {\n",
    "            'type': 'node',\n",
    "            'feature': best_feat,\n",
    "            'threshold': best_thresh,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "    \n",
    "    # ========== 关键辅助函数 ==========\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        split_feat, split_thresh = None, None\n",
    "        \n",
    "        for feat in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feat])\n",
    "            for thresh in thresholds:\n",
    "                gain = self._information_gain(X, y, feat, thresh)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_feat, split_thresh = feat, thresh\n",
    "        \n",
    "        return split_feat, split_thresh\n",
    "    \n",
    "    def _information_gain(self, X, y, feat, thresh):\n",
    "        # 计算基尼不纯度\n",
    "        parent_gini = self._gini(y)\n",
    "        \n",
    "        left_idx = X[:, feat] <= thresh\n",
    "        right_idx = X[:, feat] > thresh\n",
    "        \n",
    "        if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
    "            return 0\n",
    "        \n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y[left_idx]), len(y[right_idx])\n",
    "        \n",
    "        left_gini = self._gini(y[left_idx])\n",
    "        right_gini = self._gini(y[right_idx])\n",
    "        \n",
    "        child_gini = (n_left/n) * left_gini + (n_right/n) * right_gini\n",
    "        \n",
    "        return parent_gini - child_gini\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        # 基尼不纯度: 1 - sum(p_i^2)\n",
    "        n = len(y)\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        proportions = np.bincount(y) / n\n",
    "        return 1 - np.sum(proportions ** 2)\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "    \n",
    "    def _predict_one(self, x, node):\n",
    "        if node['type'] == 'leaf':\n",
    "            return node['value']\n",
    "        \n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self._predict_one(x, node['left'])\n",
    "        else:\n",
    "            return self._predict_one(x, node['right'])\n",
    "\n",
    "# ========== 使用示例 ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建简单数据\n",
    "    X = np.array([[1, 2], [2, 3], [3, 3], [6, 5], [7, 8], [8, 8]])\n",
    "    y = np.array([0, 0, 0, 1, 1, 1])\n",
    "    \n",
    "    # 训练\n",
    "    tree = DecisionTree(max_depth=3)\n",
    "    tree.fit(X, y)\n",
    "    \n",
    "    # 预测\n",
    "    X_test = np.array([[2.5, 3], [7.5, 7]])\n",
    "    predictions = tree.predict(X_test)\n",
    "    print(f\"Predictions: {predictions}\")  # 应输出 [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65b134-92b7-40b8-b118-e2e2580a1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"单个决策树（简化版）\"\"\"\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # 停止条件\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            n_classes == 1):\n",
    "            return np.bincount(y).argmax()\n",
    "        \n",
    "        # 随机选择特征子集（√n_features）\n",
    "        feat_indices = np.random.choice(n_features, int(np.sqrt(n_features)), replace=False)\n",
    "        \n",
    "        # 寻找最佳分裂\n",
    "        best_gain = -1\n",
    "        best_feat, best_thresh = None, None\n",
    "        \n",
    "        for feat in feat_indices:\n",
    "            thresholds = np.unique(X[:, feat])\n",
    "            for thresh in thresholds:\n",
    "                left = y[X[:, feat] <= thresh]\n",
    "                right = y[X[:, feat] > thresh]\n",
    "                \n",
    "                if len(left) == 0 or len(right) == 0:\n",
    "                    continue\n",
    "                \n",
    "                gain = self._gini_gain(y, left, right)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feat, best_thresh = feat, thresh\n",
    "        \n",
    "        if best_gain <= 0:\n",
    "            return np.bincount(y).argmax()\n",
    "        \n",
    "        # 递归构建子树\n",
    "        left_idx = X[:, best_feat] <= best_thresh\n",
    "        right_idx = ~left_idx\n",
    "        \n",
    "        left_tree = self._grow_tree(X[left_idx], y[left_idx], depth+1)\n",
    "        right_tree = self._grow_tree(X[right_idx], y[right_idx], depth+1)\n",
    "        \n",
    "        return (best_feat, best_thresh, left_tree, right_tree)\n",
    "    \n",
    "    def _gini_gain(self, parent, left, right):\n",
    "        def gini(arr):\n",
    "            if len(arr) == 0: return 0\n",
    "            p = np.bincount(arr) / len(arr)\n",
    "            return 1 - np.sum(p ** 2)\n",
    "        \n",
    "        n = len(parent)\n",
    "        n_l, n_r = len(left), len(right)\n",
    "        return gini(parent) - (n_l/n * gini(left) + n_r/n * gini(right))\n",
    "    \n",
    "    def predict_one(self, x, node):\n",
    "        if not isinstance(node, tuple):  # 叶节点\n",
    "            return node\n",
    "        \n",
    "        feat, thresh, left, right = node\n",
    "        if x[feat] <= thresh:\n",
    "            return self.predict_one(x, left)\n",
    "        else:\n",
    "            return self.predict_one(x, right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_one(x, self.tree) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcf3d5-3bf7-459a-b04b-51ca36ce6c96",
   "metadata": {},
   "source": [
    "### Gradient Descent: Building Optimization Algorithms from Scratch\n",
    "#### Implementing Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3a4f0-fc44-4eb0-ad1f-2fb1b56306ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary Library\n",
    "import numpy as np\n",
    "\n",
    "# Linear regression problem\n",
    "X = np.array([0, 1, 2, 3, 4, 5]) \n",
    "Y = np.array([0, 1.1, 1.9, 3, 4.2, 5.2])  \n",
    "\n",
    "# Model initialization\n",
    "m = np.random.randn()  # Initialize the slope (random number)\n",
    "b = np.random.randn()  # Initialize the intercept (random number)\n",
    "\n",
    "learning_rate = 0.01  # Define the learning rate\n",
    "epochs = 10000  # Define the number of iterations\n",
    "\n",
    "# SGD implementation\n",
    "for _ in range(epochs):\n",
    "    random_index = np.random.randint(len(X))  # select a random sample\n",
    "    x = X[random_index]\n",
    "    y = Y[random_index]\n",
    "    pred = m * x + b  # Calculate the predicted y\n",
    "    # Calculate gradients for m (slope) and b (intercept)\n",
    "    grad_m = (pred - y) * x \n",
    "    grad_b = (pred - y)\n",
    "    m -= learning_rate * grad_m  # Update m using the calculated gradient\n",
    "    b -= learning_rate * grad_b  # Update b using the calculated gradient\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X, Y, color = \"m\", marker = \"o\", s = 30)\n",
    "\n",
    "# Predicted line for the model\n",
    "y_pred = m * X + b\n",
    "\n",
    "# Plotting the predicted line\n",
    "plt.plot(X, y_pred, color = \"g\")\n",
    "\n",
    "# Adding labels to the plot\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc67312-d873-4722-9471-2845383c4efa",
   "metadata": {},
   "source": [
    "#### Implementing Mini-Batch Gradient Descent in Python\n",
    "\n",
    "A distinguishing feature of MBGD is its capacity to tune the size of the mini-batches. MBGD behaves as Batch Gradient Descent if the batch size equates to the dataset size. If the batch size is 1, it acts like SGD. However, a mini-batch size between 10 and 1000 is typically selected in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84177368-9ccb-4f83-aec4-a8118993d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.01, batch_size=16, epochs=100):\n",
    "    m, n = X.shape\n",
    "    theta = np.random.randn(n, 1)  # random initialization\n",
    "    # print(\"theta: \", theta)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "\n",
    "        for i in range(0, m, batch_size):\n",
    "            xi = X_shuffled[i:i + batch_size]\n",
    "            yi = y_shuffled[i:i + batch_size]\n",
    "\n",
    "            gradients = 2 / batch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "            theta = theta - learning_rate * gradients\n",
    "\n",
    "    return theta\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Apply function to some data\n",
    "X = np.random.rand(100, 3)\n",
    "y = 5 * X[:, 0] - 3 * X[:, 1] + 2 * X[:, 2] + np.random.randn(100, 1)  # sample linear regression problem\n",
    "# true_theta = np.array([5, -3, 2]).reshape(-1, 1)\n",
    "# y = X.dot(true_theta) + np.random.randn(100, 1) * 0.5  # Added some noise\n",
    "\n",
    "print((5 * X[:, 0]).shape)\n",
    "\n",
    "# # METHOD 1: Using reshape (fixes your original approach)\n",
    "linear_combo = 5 * X[:, 0] - 3 * X[:, 1] + 2 * X[:, 2]\n",
    "y = linear_combo.reshape(-1, 1) + np.random.randn(100, 1)\n",
    "\n",
    "\n",
    "theta = gradient_descent(X, y)\n",
    "\n",
    "print(\"X: \", X.shape, X, \"y: \", y.shape, y)\n",
    "print(\"theta: \", theta.shape, theta)\n",
    "\n",
    "# Predict and calculate MAE\n",
    "predictions = X.dot(theta)\n",
    "mae = mean_absolute_error(y, predictions)\n",
    "print(f\"MAE: {mae}\")  # MAE: 1.0887166179544072\n",
    "\n",
    "\n",
    "# GET THE FINAL COEFFICIENTS HERE:\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL COEFFICIENTS FROM GRADIENT DESCENT:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Method 1: Direct print\n",
    "print(f\"\\nTheta (coefficients) vector:\\n{theta}\")\n",
    "\n",
    "# Method 2: Formatted print (more readable)\n",
    "print(f\"\\nFormatted coefficients:\")\n",
    "for i in range(len(theta)):\n",
    "    print(f\"θ{i} (coefficient for X[:, {i}]): {theta[i, 0]:.6f}\")\n",
    "\n",
    "# Method 3: Compare with true values (you know they should be ~5, -3, 2)\n",
    "print(f\"\\nComparison with true values used to generate y:\")\n",
    "true_coefficients = [5, -3, 2]\n",
    "for i in range(len(theta)):\n",
    "    print(f\"θ{i}: True={true_coefficients[i]}, Estimated={theta[i, 0]:.6f}, \"\n",
    "          f\"Error={abs(true_coefficients[i] - theta[i, 0]):.6f}\")\n",
    "\n",
    "# Method 4: Create a prediction equation\n",
    "print(f\"\\nFinal prediction equation:\")\n",
    "terms = []\n",
    "for i in range(len(theta)):\n",
    "    terms.append(f\"{theta[i, 0]:.4f}*X{i+1}\")\n",
    "print(f\"y_pred = {' + '.join(terms)}\")\n",
    "\n",
    "# Calculate error\n",
    "predictions = X.dot(theta)\n",
    "mae = mean_absolute_error(y, predictions)\n",
    "print(f\"\\nMean Absolute Error: {mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45ceae-fd42-49af-aec0-556a06a04e5c",
   "metadata": {},
   "source": [
    "#### Add Momentum to Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730a3a7-3f9b-4e65-ad89-78955ec43e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def func(x):   \n",
    "    return x**2\n",
    "\n",
    "def grad_func(x): \n",
    "    return 2*x\n",
    "\n",
    "gamma = 0.9\n",
    "learning_rate = 0.01\n",
    "v = 0\n",
    "epochs = 50\n",
    "\n",
    "theta_plain = 4.0  \n",
    "theta_momentum = 4.0\n",
    "\n",
    "history_plain = []    \n",
    "history_momentum = []    \n",
    "\n",
    "for _ in range(epochs):\n",
    "    history_plain.append(theta_plain)\n",
    "    gradient = grad_func(theta_plain)\n",
    "    theta_plain = theta_plain - learning_rate * gradient\n",
    "\n",
    "    history_momentum.append(theta_momentum)\n",
    "    gradient = grad_func(theta_momentum)\n",
    "    v = gamma * v + learning_rate * gradient\n",
    "    theta_momentum = theta_momentum - v\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot([func(theta) for theta in history_plain], label='Gradient Descent')\n",
    "plt.plot([func(theta) for theta in history_momentum], label='Momentum-based Gradient Descent')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ed1de0-da51-405a-b7a8-d8a194ada867",
   "metadata": {},
   "source": [
    "#### RMSProp in Python Code (Root Mean Square Propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4c52c-0dfe-4ca7-84ef-67700571e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSProp(learning_rate, rho, epsilon, grad, s_prev):\n",
    "    # Update squared gradient\n",
    "    s = rho * s_prev + (1 - rho) * np.power(grad, 2)\n",
    "\n",
    "    # Calculate updates\n",
    "    updates = learning_rate * grad / (np.sqrt(s) + epsilon)\n",
    "    return updates, s\n",
    "\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def df(x, y):\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "coordinates = np.array([5.0, 4.0])\n",
    "learning_rate = 0.1\n",
    "rho = 0.9\n",
    "epsilon = 1e-6\n",
    "max_epochs = 100\n",
    "\n",
    "s_prev = np.array([0, 0])\n",
    "\n",
    "for epoch in range(max_epochs + 1):\n",
    "    grad = df(coordinates[0], coordinates[1])\n",
    "    updates, s_prev = RMSProp(learning_rate, rho, epsilon, grad, s_prev)\n",
    "    coordinates -= updates\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, current state: {coordinates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0ffde-bf71-4d24-a3c0-ec79e6279866",
   "metadata": {},
   "source": [
    "#### ADAM (Adaptive Moment Estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db9418-5e8a-4b53-82d9-39f802509603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAM(beta1, beta2, epsilon, grad, m_prev, v_prev, learning_rate):\n",
    "    # Update biased first-moment estimate\n",
    "    m = beta1 * m_prev + (1 - beta1) * grad\n",
    "\n",
    "    # Update biased second raw moment estimate\n",
    "    v = beta2 * v_prev + (1 - beta2) * np.power(grad, 2)\n",
    "\n",
    "    # Calculate updates\n",
    "    updates = learning_rate * m / (np.sqrt(v) + epsilon)\n",
    "    return updates, m, v\n",
    "\n",
    "    # m_hat = m / (1 - np.power(beta1, epoch+1))  # Correcting the bias for the first moment\n",
    "    # v_hat = v / (1 - np.power(beta2, epoch+1))  # Correcting the bias for the second moment\n",
    "    \n",
    "    # updates = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    # return updates, m, v\n",
    "\n",
    "def f(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "def df(x, y):\n",
    "    return np.array([2 * x, 2 * y])\n",
    "\n",
    "coordinates = np.array([3.0, 4.0])\n",
    "learning_rate = 0.02\n",
    "beta1 = 0.9\n",
    "beta2 = 0.9999\n",
    "epsilon = 1e-8\n",
    "max_epochs = 150\n",
    "\n",
    "m_prev = np.array([0, 0])\n",
    "v_prev = np.array([0, 0])\n",
    "\n",
    "for epoch in range(max_epochs + 1):\n",
    "    grad = df(coordinates[0], coordinates[1])\n",
    "    updates, m_prev, v_prev = ADAM(beta1, beta2, epsilon, grad, m_prev, v_prev, learning_rate)\n",
    "    coordinates -= updates\n",
    "    if epoch % 30 == 0:\n",
    "        print(f\"Epoch {epoch}, current state: {coordinates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd680fd7-5e48-4ffa-9aef-7198fe5570e9",
   "metadata": {},
   "source": [
    "### Ensemble Methods from Scratch\n",
    "\n",
    "Bagging, or bootstrap aggregating, is a technique in ensemble learning that aims to reduce the variance of the machine learning model. The essence of bagging involves generating multiple subsets from the original dataset and then using these subsets to train separate models. Note that the subsets are chosen with replacement, so it is possible to have duplicate data points in a single subset. The final prediction is then made by aggregating the predictions from these individual models. Essentially, it is a voting for the best answer: the final class prediction is the class that was predicted by the majority of votes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aab217-b029-4dfb-80d0-7f2fe78d2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Parameters\n",
    "n_models = 100\n",
    "random_states = [i for i in range(n_models)]\n",
    "\n",
    "# Helper function for bootstrapping\n",
    "def bootstrapping(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    return X[idxs], y[idxs]\n",
    "\n",
    "# Helper function for bagging prediction\n",
    "def predict(X, models):\n",
    "    predictions = np.array([model.predict(X) for model in models])\n",
    "    predictions = stats.mode(predictions)[0]\n",
    "    return predictions\n",
    "\n",
    "# Create a list to store all the tree models\n",
    "tree_models = []\n",
    "\n",
    "# Iteratively train decision trees on bootstrapped samples\n",
    "for i in range(n_models):\n",
    "    X_, y_ = bootstrapping(X_train, y_train)\n",
    "    tree = DecisionTreeClassifier(max_depth=2, random_state=random_states[i])\n",
    "    tree.fit(X_, y_)\n",
    "    tree_models.append(tree)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = predict(X_test, tree_models)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64f1e1-6043-40fe-bcda-3300a2dbe8c6",
   "metadata": {},
   "source": [
    "#### Implementing the Random Forest in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d56c22-8d43-4d50-a6d3-e55135b14e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=100, max_depth=None, random_state=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.random_states = np.random.RandomState(random_state).randint(0,10000,size=n_trees)\n",
    "        self.trees = []\n",
    "\n",
    "    def bootstrapping(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[idxs], y[idxs]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_trees):\n",
    "            X_, y_ = self.bootstrapping(X, y)\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, random_state=self.random_states[i])\n",
    "            tree.fit(X_, y_)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return stats.mode(tree_preds)[0]\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "rf = RandomForest(n_trees=100, max_depth=2, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de23af-0855-4ddd-8eee-20b98e18de7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c50e0775-f415-43c7-b35b-57d3e8eea7b9",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790cf84-1986-4dfe-b800-d38eee9e3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \"\"\"随机森林主类\"\"\"\n",
    "    def __init__(self, n_estimators=100, max_depth=10, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # 1. Bootstrap采样（有放回随机抽样）\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_boot = X[indices]\n",
    "            y_boot = y[indices]\n",
    "            \n",
    "            # 2. 训练决策树\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split\n",
    "            )\n",
    "            tree.fit(X_boot, y_boot)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # 可选：打印进度\n",
    "            if (i+1) % 20 == 0:\n",
    "                print(f\"Trained tree {i+1}/{self.n_estimators}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # 收集所有树的预测\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "        # 多数投票（沿树的方向取众数）\n",
    "        final_predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            votes = tree_preds[:, i]\n",
    "            # 使用Counter找众数\n",
    "            most_common = Counter(votes).most_common(1)[0][0]\n",
    "            final_predictions.append(most_common)\n",
    "        \n",
    "        return np.array(final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcecdaa1-1d8d-4793-aea4-46f4abe3206a",
   "metadata": {},
   "source": [
    "First, let's define our terms. Boosting is a technique in which several weak learners are combined to create a strong learner, thereby improving our predictive model. AdaBoost largely follows the same principle. However, it introduces an important twist: it adapts by focusing more on instances that were incorrectly predicted in previous iterations by assigning them higher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f4096-5c61-49df-a82c-875b20f6b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, num_learners=10, learning_rate=1):\n",
    "        self.num_learners = num_learners\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.model_weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        M, N = X.shape\n",
    "        W = np.ones(M) / M  # Initialize weights\n",
    "        y = y * 2 - 1  # Convert y to {-1, 1}\n",
    "\n",
    "        for _ in range(self.num_learners):\n",
    "            tree = DecisionTreeClassifier(max_depth=1)\n",
    "            tree.fit(X, y, sample_weight=W)\n",
    "            \n",
    "            pred = tree.predict(X)\n",
    "            error = W.dot(pred != y)\n",
    "            if error > 0.5:\n",
    "                break\n",
    "\n",
    "            beta = self.learning_rate * np.log((1 - error) / error)  # Compute beta\n",
    "            W = W * np.exp(beta * (pred != y))  # Update weights\n",
    "    \n",
    "            W = W / W.sum()  # Normalize weights\n",
    "            \n",
    "            self.models.append(tree)\n",
    "            self.model_weights.append(beta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        Hx = sum(beta * h.predict(X) for h, beta in zip(self.models, self.model_weights))  # Weighted aggregate of predictions\n",
    "        return Hx > 0  # Calculate majority vote\n",
    "\n",
    "data = make_classification(n_samples=1000)  # Creates a synthetic dataset\n",
    "X = data[0]\n",
    "y = data[1]\n",
    "\n",
    "# Split data into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ada = AdaBoost(num_learners=10, learning_rate=0.5)  # Initialize AdaBoost model\n",
    "ada.fit(X_train, y_train)  # Train the model\n",
    "pred = ada.predict(X_test)\n",
    "print('AdaBoost accuracy:', accuracy_score(y_test, pred))  # Accuracy as correct predictions over total predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bfceef-9877-4912-9648-b1e21c4567e8",
   "metadata": {},
   "source": [
    "### Unsupervised Learning and Clustering\n",
    "#### k-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bb074-d6d7-4d0c-85f3-f8a718ffa3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Toy dataset with 2D points\n",
    "data = [(2,3), (5,3.4), (1.3,1), (3,4), (2,3.5), (7,5)]\n",
    "\n",
    "# k-Means settings\n",
    "k = 2  \n",
    "centers = random.sample(data, k)  \n",
    "\n",
    "print(centers)\n",
    "\n",
    "# Definition of Euclidean distance\n",
    "def distance(point1, point2):\n",
    "    return ((point1[0]-point2[0])**2 + (point1[1]-point2[1])**2)**0.5\n",
    "\n",
    "# k-Means algorithm\n",
    "def k_means(data, centers, k):\n",
    "    while True:\n",
    "        clusters = [[] for _ in range(k)] \n",
    "\n",
    "        # Assign data points to the closest center\n",
    "        for point in data:\n",
    "            distances = [distance(point, center) for center in centers]\n",
    "            index = distances.index(min(distances)) \n",
    "            clusters[index].append(point)\n",
    "\n",
    "        # Update centers to be the mean of points in a cluster\n",
    "        new_centers = []\n",
    "        for cluster in clusters:\n",
    "            center = (sum([point[0] for point in cluster])/len(cluster), \n",
    "                      sum([point[1] for point in cluster])/len(cluster)) \n",
    "            new_centers.append(center)\n",
    "\n",
    "        # Break loop if centers don't change significantly\n",
    "        if max([distance(new, old) for new, old in zip(new_centers, centers)]) < 0.0001:\n",
    "            break\n",
    "        else:\n",
    "            centers = new_centers\n",
    "    return clusters, centers\n",
    "\n",
    "clusters, centers = k_means(data, centers, k)\n",
    "\n",
    "# Let's print the cluster centers\n",
    "for i, center in enumerate(centers):\n",
    "    print(f\"Cluster{i+1} center is : {center}\")\n",
    "# Cluster1 center is : (2.66, 2.98)\n",
    "# Cluster2 center is : (7.0, 5.0)\n",
    "\n",
    "# Let's print the clusters\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"Cluster{i+1} points are : {cluster}\")\n",
    "# Cluster1 points are : [(2, 3), (5, 3.4), (1.3, 1), (3, 4), (2, 3.5)]\n",
    "# Cluster2 points are : [(7, 5)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['r', 'g', 'b', 'y', 'c', 'm']\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot points\n",
    "for i, cluster in enumerate(clusters):\n",
    "    for point in cluster:\n",
    "        ax.scatter(*point, color=colors[i])\n",
    "\n",
    "# Plot centers\n",
    "for i, center in enumerate(centers):\n",
    "    ax.scatter(*center, color='black', marker='x', s=300)\n",
    "\n",
    "ax.set_title('Clusters and their centers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e746de-8b55-45fc-9c7a-d9d51946b7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faf8b8a6-63d1-48e0-bb49-b0d5647ba561",
   "metadata": {},
   "source": [
    "K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330718d7-0199-486a-8354-0f36b71b84b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initialize_centroids(X, k):\n",
    "    \"\"\"Randomly initialize centroids from the dataset.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    random_indices = np.random.permutation(X.shape[0])\n",
    "    centroids = X[random_indices[:k]]\n",
    "    return centroids\n",
    "\n",
    "def compute_distances(X, centroids):\n",
    "    \"\"\"Compute the distance between each data point and the centroids.\"\"\"\n",
    "    distances = np.zeros((X.shape[0], centroids.shape[0]))\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        distances[:, i] = np.linalg.norm(X - centroid, axis=1)\n",
    "    return distances\n",
    "\n",
    "def assign_clusters(distances):\n",
    "    \"\"\"Assign each data point to the closest centroid.\"\"\"\n",
    "    return np.argmin(distances, axis=1)\n",
    "\n",
    "def compute_centroids(X, labels, k):\n",
    "    \"\"\"Compute the new centroids as the mean of all data points assigned to each cluster.\"\"\"\n",
    "    centroids = np.zeros((k, X.shape[1]))\n",
    "    for i in range(k):\n",
    "        centroids[i, :] = X[labels == i].mean(axis=0)\n",
    "    return centroids\n",
    "\n",
    "def kmeans(X, k, max_iters=100):\n",
    "    \"\"\"K-means clustering algorithm.\"\"\"\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        old_centroids = centroids\n",
    "        distances = compute_distances(X, centroids)\n",
    "        labels = assign_clusters(distances)\n",
    "        centroids = compute_centroids(X, labels, k)\n",
    "        \n",
    "        # If centroids do not change, we have converged\n",
    "        if np.all(centroids == old_centroids):\n",
    "            break\n",
    "    \n",
    "    return centroids, labels\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some synthetic data\n",
    "    from sklearn.datasets import make_blobs\n",
    "    X, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)\n",
    "    print(X.shape, X[:3])\n",
    "\n",
    "    # Run the K-means algorithm\n",
    "    k = 3\n",
    "    centroids, labels = kmeans(X, k)\n",
    "\n",
    "    # Visualize the results\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red')  # Centroids\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('K-means Clustering')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612e3ce-cda3-4d00-a3bb-d1d172fbd3ad",
   "metadata": {},
   "source": [
    "#### Mini-Batch K-Means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06500492-d655-454c-8298-95543801bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "data = np.vstack([np.random.normal(loc=3, scale=1, size=(100,2)), np.random.normal(loc=-3, scale=1, size=(100,2))])\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a - b, axis=-1)\n",
    "\n",
    "def initialize_centers(data, k):\n",
    "    idx = np.random.choice(len(data), size=k)\n",
    "    return data[idx, :]\n",
    "\n",
    "# Implement mini-batch K-Means\n",
    "def mini_batch_kMeans(data, k, iterations=10, batch_size=20):\n",
    "    centers = initialize_centers(data, k)\n",
    "    for _ in range(iterations):\n",
    "        idx = np.random.choice(len(data), size=batch_size)\n",
    "        batch = data[idx, :]\n",
    "        dists = euclidean_distance(batch[:, None, :], centers[None, :, :])\n",
    "        labels = np.argmin(dists, axis=1)\n",
    "        for i in range(k):\n",
    "            if np.sum(labels == i) > 0:\n",
    "                centers[i] = np.mean(batch[labels == i], axis=0)\n",
    "    return centers\n",
    "\n",
    "centers = mini_batch_kMeans(data, k=2)\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], s=50)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87f88c-8c74-4d3e-a41e-60abdd9aee05",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933faca8-d878-47cf-b79f-838f44e2591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "np.random.seed(0)\n",
    "# Creating 200-point 3D dataset\n",
    "X = np.dot(np.random.random(size=(3, 3)), np.random.normal(size=(3, 200))).T\n",
    "# Plotting the dataset\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2])\n",
    "plt.title(\"Scatter Plot of Original Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the mean and the standard deviation\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "# Make the dataset standard \n",
    "X = (X - X_mean) / X_std\n",
    "\n",
    "# Calculate Covariance Matrix \n",
    "cov_matrix = np.cov(X.T)\n",
    "\n",
    "# Break into eigenvectors and eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort out eigenvalues and corresponding eigenvectors\n",
    "eigen_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:,i]) for i in range(len(eigenvalues))]\n",
    "eigen_pairs.sort(reverse=True)\n",
    "\n",
    "# Make the projection matrix\n",
    "W = np.hstack((eigen_pairs[0][1].reshape(3,1), eigen_pairs[1][1].reshape(3,1)))\n",
    "# Change the original dataset\n",
    "X_pca = X.dot(W)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_pca[:, 0],X_pca[:, 1])\n",
    "plt.title(\"Scatter Plot of Transformed Dataset Using PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7707575-dba0-4e08-8162-b9826a95df1d",
   "metadata": {},
   "source": [
    "#### DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a1fc9-9cfe-4333-a2ff-6db081523e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = np.array([\n",
    "    [1.2, 1.9], [2.1, 2], [2, 3.5], [3.3, 3.9], [3.2, 5.1],\n",
    "    [8.5, 7.9], [8.1, 7.8], [9.5, 6.5], [9.5, 7.2], [7.7, 8.6],\n",
    "    [6.0, 6.0]\n",
    "])\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a - b, axis=-1)\n",
    "\n",
    "def dbscan(data, Eps, MinPt):\n",
    "    point_label = [0] * len(data)\n",
    "    # Initialize list to maintain count of surrounding points within radius Eps for each point. \n",
    "    point_count = []\n",
    "    core = []\n",
    "    noncore = []\n",
    "\n",
    "    # Check for each point if it falls within the Eps radius of point at index i\n",
    "    for i in range(len(data)):\n",
    "        point_count.append([])\n",
    "        for j in range(len(data)):\n",
    "            if euclidean_distance(data[i], data[j]) <= Eps and i != j:\n",
    "                point_count[i].append(j)\n",
    "        \n",
    "        # If a point has atleast MinPt points within its Eps radius (excluding itself), classify it as a core point, and vice versa\n",
    "        if len(point_count[i]) >= MinPt:\n",
    "            core.append(i)\n",
    "        else:\n",
    "            noncore.append(i)\n",
    "    ...\n",
    "    ...\n",
    "    \"\"\"\n",
    "    This code iterates over core points and assesses its neighbors for each. If a neighbor has not been assigned to a cluster, \n",
    "    it's assigned to the current core point's cluster. Core points among these neighbors are put in a queue to repeat the \n",
    "    same process. Once all points in a cluster are labeled, they go to the next cluster. The final output lists all points \n",
    "    with their respective cluster IDs.\n",
    "    \"\"\"    \n",
    "    ID = 1\n",
    "    for point in core:\n",
    "        # If the point has not been assigned to a cluster yet\n",
    "        if point_label[point] == 0:\n",
    "            point_label[point] = ID\n",
    "            # Create an empty list to hold 'neighbour points'  \n",
    "            queue = []\n",
    "            for x in point_count[point]:\n",
    "                if point_label[x] == 0:\n",
    "                    point_label[x] = ID\n",
    "                    # If neighbor point is also a core point, add it to the queue \n",
    "                    if x in core:\n",
    "                        queue.append(x)\n",
    "            \n",
    "            # Check points from the queue\n",
    "            while queue:\n",
    "                neighbours = point_count[queue.pop(0)]\n",
    "                for y in neighbours:\n",
    "                    if point_label[y] == 0:\n",
    "                        point_label[y] = ID\n",
    "                        if y in core:\n",
    "                            queue.append(y)\n",
    "            ID += 1  \n",
    "\n",
    "    return point_label\n",
    "\n",
    "labels = dbscan(data_points, 2, 2)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 1:\n",
    "        plt.scatter(data_points[i][0], data_points[i][1], s=100, c='r')\n",
    "    elif labels[i] == 2:\n",
    "        plt.scatter(data_points[i][0], data_points[i][1], s=100, c='g')\n",
    "    else:\n",
    "        plt.scatter(data_points[i][0], data_points[i][1], s=100, c='b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f4043-7748-4187-ace8-ad1f623f5ae8",
   "metadata": {},
   "source": [
    "This code iterates over core points and assesses its neighbors for each. If a neighbor has not been assigned to a cluster, it's assigned to the current core point's cluster. Core points among these neighbors are put in a queue to repeat the same process. Once all points in a cluster are labeled, they go to the next cluster. The final output lists all points with their respective cluster IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1742b92-4422-4008-8d0d-f47c18f4dfe5",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8451926d-0fc4-4c4b-b158-643490ffde97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "Input: [0 0 1] ---> Prediction: [[0.01466264]], Expected: [0]\n",
      "Input: [0 1 1] ---> Prediction: [[0.9647285]], Expected: [1]\n",
      "Input: [1 0 1] ---> Prediction: [[0.9648335]], Expected: [1]\n",
      "Input: [1 1 1] ---> Prediction: [[0.04059226]], Expected: [0]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y, learning_rate=0.1):\n",
    "        self.input = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4)\n",
    "        self.weights2   = np.random.rand(4,1)\n",
    "        self.y = y\n",
    "        self.output = np.zeros(self.y.shape)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def feedforward(self):\n",
    "        # Implements feedforward method using dot product and sigmoid function\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "\n",
    "    def backprop(self):\n",
    "        # Performs backpropagation and updates weights\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T, (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "    \n",
    "        self.weights1 += self.learning_rate * d_weights1\n",
    "        self.weights2 += self.learning_rate * d_weights2\n",
    "\n",
    "    def train(self, epochs):\n",
    "        # Repeatedly performs feedforward and backpropagation for several epochs\n",
    "        for epoch in range(epochs):\n",
    "            self.feedforward()\n",
    "            self.backprop()\n",
    "\n",
    "    def predict(self, new_input):\n",
    "        layer1 = sigmoid(np.dot(new_input, self.weights1))\n",
    "        output = sigmoid(np.dot(layer1, self.weights2))\n",
    "        return output\n",
    "\n",
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "nn = NeuralNetwork(X, Y)\n",
    "\n",
    "nn.train(10000)\n",
    "print(\"\\nPredictions:\")\n",
    "for i, x in enumerate(X):\n",
    "    print(f\"Input: {x} ---> Prediction: {nn.predict(np.array([x]))}, Expected: {Y[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdbb9e-f9af-4cb0-9d68-59f24ea49512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de17c9bf-3b2d-40ec-a40f-49111f4ab8bc",
   "metadata": {},
   "source": [
    "https://www.deep-ml.com/problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86b89b-d719-43bc-80f6-0acda4c73d2c",
   "metadata": {},
   "source": [
    "#### Implement Decision Tree for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a29efd2-498e-438c-943d-ae4665a5693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def decision_tree_regressor(X_train, y_train, X_test, max_depth=2, min_samples_split=2):\n",
    "    \"\"\"\n",
    "    Build a decision tree for regression and predict on test data.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features, shape (n_samples, n_features)\n",
    "        y_train: Training targets, shape (n_samples,)\n",
    "        X_test: Test features, shape (m_samples, n_features)\n",
    "        max_depth: Maximum depth of the tree\n",
    "        min_samples_split: Minimum samples required to split a node\n",
    "    \n",
    "    Returns:\n",
    "        List of predictions for X_test, rounded to 4 decimal places\n",
    "    \"\"\"\n",
    "    X_train = np.array(X_train, dtype=float)\n",
    "    y_train = np.array(y_train, dtype=float)\n",
    "    X_test = np.array(X_test, dtype=float)\n",
    "    \n",
    "    def calculate_mse(y):\n",
    "        if len(y) == 0:\n",
    "            return 0.0\n",
    "        return np.mean((y - np.mean(y))**2)\n",
    "    \n",
    "    def find_best_split(X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples < min_samples_split:\n",
    "            return None\n",
    "        \n",
    "        current_mse = calculate_mse(y)\n",
    "        best_gain = 0\n",
    "        best_split = None\n",
    "        \n",
    "        for feature_idx in range(n_features):\n",
    "            values = np.unique(X[:, feature_idx])\n",
    "            for i in range(len(values) - 1):\n",
    "                threshold = (values[i] + values[i + 1]) / 2\n",
    "                \n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                left_mse = calculate_mse(y[left_mask])\n",
    "                right_mse = calculate_mse(y[right_mask])\n",
    "                \n",
    "                n_left = np.sum(left_mask)\n",
    "                n_right = np.sum(right_mask)\n",
    "                weighted_mse = (n_left * left_mse + n_right * right_mse) / n_samples\n",
    "                \n",
    "                gain = current_mse - weighted_mse\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_split = (feature_idx, threshold)\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    def build_tree(X, y, depth):\n",
    "        if depth >= max_depth or len(y) < min_samples_split or len(np.unique(y)) == 1:\n",
    "            return {'leaf': True, 'value': float(np.mean(y))}\n",
    "        \n",
    "        split = find_best_split(X, y)\n",
    "        \n",
    "        if split is None:\n",
    "            return {'leaf': True, 'value': float(np.mean(y))}\n",
    "        \n",
    "        feature_idx, threshold = split\n",
    "        left_mask = X[:, feature_idx] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        return {\n",
    "            'leaf': False,\n",
    "            'feature': feature_idx,\n",
    "            'threshold': threshold,\n",
    "            'left': build_tree(X[left_mask], y[left_mask], depth + 1),\n",
    "            'right': build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        }\n",
    "    \n",
    "    def predict_single(node, x):\n",
    "        if node['leaf']:\n",
    "            return node['value']\n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return predict_single(node['left'], x)\n",
    "        else:\n",
    "            return predict_single(node['right'], x)\n",
    "    \n",
    "    tree = build_tree(X_train, y_train, 0)\n",
    "    predictions = [predict_single(tree, x) for x in X_test]\n",
    "    \n",
    "    return [round(p, 4) for p in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8d92c-3464-4e10-a503-5b558f500c42",
   "metadata": {},
   "source": [
    "#### Train Logistic Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a3ed0-e342-4f58-8ca4-043ae7277773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n",
    "    \"\"\"\n",
    "    Train logistic regression using gradient descent with sum-based BCE loss.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Binary labels of shape (n_samples,)\n",
    "        learning_rate: Step size for gradient descent\n",
    "        iterations: Number of training iterations\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (coefficients, losses) where:\n",
    "        - coefficients: List of learned weights (bias first, then feature weights)\n",
    "        - losses: List of sum-based BCE loss values at each iteration\n",
    "    \"\"\"\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Reshape y to column vector\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    # Add bias column (ones) as FIRST column\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    # Initialize coefficients to ZEROS\n",
    "    B = np.zeros((X.shape[1], 1))\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Forward pass: compute predictions\n",
    "        y_pred = sigmoid(X @ B)\n",
    "        \n",
    "        # Gradient descent update\n",
    "        gradient = X.T @ (y_pred - y)\n",
    "        B -= learning_rate * gradient\n",
    "        \n",
    "        # Compute SUM-based BCE loss (not mean)\n",
    "        loss = -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        losses.append(float(loss))\n",
    "    \n",
    "    coefficients = [float(b) for b in B.flatten()]\n",
    "    return coefficients, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06221a5a-3f90-4f68-b7e1-a5252bea44e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042f0e7-38e1-49b0-b447-2f5d475f37c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fb0ae-ef06-461c-8160-cf19154e91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "我现在有interview的一些资料，根据下面资料生成interview时的面试题和对应答案，\n",
    "\n",
    "第二个OA: 70分钟，6道选择题+1道填空题＋3道code。\n",
    "选择题基本上都是ML相关题，比如要你算recall，LDA和PCA区别，overfitting处理啥的。填空题是给你一个NN结构和input，你算最后output。最搞心态的是，前面都是linear function，最后输出层它搞一个sigmoid function，然后算出来x还是个小数。你告诉我这没计算器怎么算？最后结果要求小数点后三位。\n",
    "\n",
    "\n",
    "第一道code：给一个数组和一个区间n，算有哪些local max value。比如[1,3,10,4,2,19,5,5]和区间n=2，10是一个local max value，因为[1,3,10,4,2]里10最大，然后10前面的数字严格单调递增，后面的数字严格单调递减（大于等于，小于等于都不行）。所以19不是。\n",
    "第二道code：手写bootstrap算法，补全code块（不准对已有code任何改动）。给你多个sklearn的classifiers，x和y。输出经过majority voting后的predicted y。答案大概长这样：\n",
    "from random import randint, seed\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "def bootstrap(n: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Step 1: Bootstrap the train samples for each base classifier.\n",
    "    \"\"\"\n",
    "    indices = [randint(0, n-1) for _ in range(n)]\n",
    "    return indices\n",
    "\n",
    "def fit(classifiers: list[DecisionTreeClassifier], x: list[list[float]], y: list[int]):\n",
    "    \"\"\"\n",
    "    Step 2: Train each classifier based on its own bootstrapped samples.\n",
    "    \"\"\"\n",
    "    n_samples = len(x)\n",
    "    for clf in classifiers:\n",
    "        indices = bootstrap(n_samples)\n",
    "        x_bootstrapped = [x[i] for i in indices]\n",
    "        y_bootstrapped = [y[i] for i in indices]\n",
    "        clf.fit(x_bootstrapped, y_bootstrapped)\n",
    "\n",
    "def predict(classifiers: list[DecisionTreeClassifier], x: list[list[float]]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Step 3: Assign class labels by a majority vote of the base classifiers.\n",
    "    \"\"\"\n",
    "    predictions = np.array([clf.predict(x) for clf in classifiers])\n",
    "    # Majority vote\n",
    "    final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
    "    return final_predictions.tolist()\n",
    "\n",
    "def solution(x_train: list[list[float]], y_train: list[int], x_test: list[list[float]], n_estimators: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Step 4: Pull everything together\n",
    "    \"\"\"\n",
    "    seed(42)\n",
    "    classifiers = [DecisionTreeClassifier(random_state=0) for _ in range(n_estimators)]\n",
    "    fit(classifiers, x_train, y_train)\n",
    "    return predict(classifiers, x_test)\n",
    "复制代码\n",
    "第三道code：手动实现Naive Bayes算法。到这我已经没时间了。\n",
    "\n",
    "\n",
    "OA2\n",
    "\n",
    "\n",
    "5 道选择\n",
    "问Random Forest和XGBoost的区别\n",
    "怎么increase bias and reduce variance\n",
    "LDA和PCA分别的适用场景\n",
    "validation loss is significantly higher than training loss，问有可能是什么原因\n",
    "给了4个confusion matrices, 选出所有recall >= 90%同时FPR < 10%的\n",
    "\n",
    "\n",
    "\n",
    "1道填空\n",
    "一个有1个hidden layer的MLP，hidden activation是linear，output activation是sigmoid，给了input和所有weights，算output\n",
    "\n",
    "\n",
    "\n",
    "3道coding\n",
    "find the longest contiguous substring consisting of the same character：找到连续出现最多次的character，返回这个character和连续出现的次数\n",
    "Bootstrap\n",
    "Decision Tree\n",
    "\n",
    "70分钟10题， CodeSignal Online Assessment。时间很紧，楼主也不记得原题了，请大家谅解。几个tips\n",
    "1- coding基本写出来就不错了。时间太紧了，不要worry about optimality，除非test 没过再回来改。ML coding 的description非常的长，可能看起来会很花时间，所以要复习一下tree algorithm自己写一写，再去。可以先做coding，再回来做选择题。\n",
    "2 - multiple choices 考ML fundamental 考的非常非常细。如果不是很confident就不要花太多时间纠结了。\n",
    "\n",
    "\n",
    "7个Multiple Choices，抱歉不是全记得了，就写几个记得 ，TLDR 硬币基地很喜欢考各种tree algorithm，复习好了tree再去做这个tech screen\n",
    "\n",
    "1 - naive bayes 和 knn的优劣（比如说curse of dimensionality, multi-colinearity 方面）\n",
    "2- implement forward propagation for 3 layers of simple feedforward with linear/no activation function in first 2 layers and sigmoid in last layers. 这题有点变态，因为没说能让用calculator，楼主很诚实， 所以楼主纠结了一下sigmoid 估算。\n",
    "3- emsemble algorithm的优劣（比如说是不是training extensive，是不是容易overfit）\n",
    "4- random forest和GBT 的优劣 (比如说inference time，overfitting， difficult to train\n",
    "5 - LDA 和PCA的优劣和区别\n",
    "\n",
    "\n",
    "3个coding。\n",
    "第一题coding非常简单，求string中longest consequtive sequence with identical characters 长度. 比如说aaabbc -> 3 。\n",
    "第二题要求implement random forest里面的bootstrapping，bagging 的training和prediction。\n",
    "第三道题楼主，没时间做了，也是implement random forest里面的其他的一些compoment\n",
    "楼主的Multiple choices是随便答的因为没有复习，coding做到后面也没太多时间做了，总之没啥准备。希望想去的同学还是准备好再做这个。\n",
    "楼主确实做的很烂，主要是真的很久没有用到各种tree algorithm了，但是recruiter说senior 这个算过，staff不算过。\n",
    "\n",
    "70 分钟10道题\n",
    "前几个都是MLE八股文， 还有算MLP output, 最后是两道coding, 一道 logistic regression, test case 全过， 第二道naive bayes, 没时间写了直接提交了。\n",
    "\n",
    "70 分钟 10 道题目\n",
    "\n",
    "1-7 是一些简单的ml basics, 问一些model的不同，怎么处理各种情况，比如overfitting\n",
    "8 一个非常简单的leetcode，大概是找字符串里面最后一个最大的连续重复的substring\n",
    "9 不让用numpy然后手写gradient descent，但是给的蛮简单的，是一个linear function然后迭代的formula也都给了，稍微注意一下matrix dimension就好了\n",
    "10 最后一题也是手写，写kmeans，我只写到了更新cluster center，最后的main function没时间了， 但是最后也过了\n",
    "\n",
    "OA2是mle coding+basics；有选择+填空+coding；影响最深的是有个填空题算loss，精确到小数点后三位… 还没给计算器，实在不知道这题的意义在哪里\n",
    "coding是 decision tree，logistic regression和knn；里面给了些implementation然后让你填上剩下的、我觉得这种对我非常不友好，不如让我写一个…\n",
    "还有就是之前看instruction说不用run，就没看test case是不是pass；太蠢了，太久没做这种OA了；不知道自己咋想的\n",
    "反正fail了，move forward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
