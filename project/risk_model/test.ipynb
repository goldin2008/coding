{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f44dce-e54f-4eea-9d19-dc9fa9d9a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a2e3ca-5757-4fc0-a2fa-4e1723785424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_library.csv saved.\n",
      "feature_score.csv saved.\n",
      "risk_score.csv saved.\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Feature Library\n",
    "# ===============================\n",
    "\n",
    "feature_library_data = {\n",
    "    \"feature_name\": [\n",
    "        \"wirein_ct\",\n",
    "        \"perc_hrg_wire_amt\",\n",
    "        \"degree_centrality\"\n",
    "    ],\n",
    "    \"feature_meaning\": [\n",
    "        \"Number of wire inbound transactions\",\n",
    "        \"Percentage of wire amount associated with high-risk geographic country\",\n",
    "        \"Number of connections an entity has in a network (degree centrality)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "feature_library_df = pd.DataFrame(feature_library_data)\n",
    "feature_library_df.to_csv(\"feature_library.csv\", index=False)\n",
    "print(\"feature_library.csv saved.\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Feature Score\n",
    "# ===============================\n",
    "\n",
    "feature_score_data = {\n",
    "    \"feature_name\": [\n",
    "        \"wirein_ct\",\n",
    "        \"perc_hrg_wire_amt\",\n",
    "        \"degree_centrality\"\n",
    "    ],\n",
    "    \"score\": [\n",
    "        0.52,\n",
    "        0.36,\n",
    "        0.12\n",
    "    ]\n",
    "}\n",
    "\n",
    "feature_score_df = pd.DataFrame(feature_score_data)\n",
    "feature_score_df.to_csv(\"feature_score.csv\", index=False)\n",
    "print(\"feature_score.csv saved.\")\n",
    "\n",
    "# ===============================\n",
    "# 3. Risk Score\n",
    "# ===============================\n",
    "\n",
    "risk_score_data = {\n",
    "    \"risk_score\": [\n",
    "        0.8\n",
    "    ]\n",
    "}\n",
    "\n",
    "risk_score_df = pd.DataFrame(risk_score_data)\n",
    "risk_score_df.to_csv(\"risk_score.csv\", index=False)\n",
    "print(\"risk_score.csv saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d4f0a1-6f09-4876-8596-2d977bf4f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Load the data\n",
    "# ===============================\n",
    "\n",
    "feature_library = pd.read_csv(\"feature_library.csv\")\n",
    "feature_score = pd.read_csv(\"feature_score.csv\")\n",
    "risk_score_df = pd.read_csv(\"risk_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb544aa-7414-4d37-9d60-e3f654f0e0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        feature_name                                    feature_meaning\n",
       " 0          wirein_ct                Number of wire inbound transactions\n",
       " 1  perc_hrg_wire_amt  Percentage of wire amount associated with high...\n",
       " 2  degree_centrality  Number of connections an entity has in a netwo...,\n",
       "         feature_name  score\n",
       " 0          wirein_ct   0.52\n",
       " 1  perc_hrg_wire_amt   0.36\n",
       " 2  degree_centrality   0.12,\n",
       "    risk_score\n",
       " 0         0.8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_library, feature_score, risk_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d5caa-22ee-41a1-aeab-b0277e8f01d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ab6f7-7a49-46a1-aed6-c542cdc72fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e7cb056-c730-425c-b0ab-199d0b72d23b",
   "metadata": {},
   "source": [
    "### Generate Narrative based on Model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df7024e-15a0-4608-9d44-726b074b2b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt Text ===\n",
      "You are a risk model explanation assistant. Given a risk score and a list of features with their descriptions and contributions, generate a clear, concise narrative explaining the risk score.\n",
      "\n",
      "Risk Score: 80%\n",
      "Top Features and Contributions:\n",
      "- wirein_ct (Number of wire inbound transactions): 52% contribution\n",
      "- perc_hrg_wire_amt (Percentage of wire amount associated with high-risk geographic country): 36% contribution\n",
      "- degree_centrality (Number of connections an entity has in a network (degree centrality)): 12% contribution\n",
      "\n",
      "Please produce a narrative that:\n",
      "- Starts with the risk score\n",
      "- Explains each feature’s contribution in plain language\n",
      "- Highlights why each feature might indicate a higher risk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from narrative.prompt_generator import build_feature_contribution_prompt\n",
    "\n",
    "# Assume you already have:\n",
    "# - feature_score_df\n",
    "# - feature_library_df\n",
    "# - risk_score_df\n",
    "\n",
    "prompt_text = build_feature_contribution_prompt(\n",
    "    feature_score_df,\n",
    "    feature_library_df,\n",
    "    risk_score_df\n",
    ")\n",
    "\n",
    "print(\"=== Prompt Text ===\")\n",
    "print(prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b234cc-5f58-4ef6-922e-d8f90be2f689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9b47d02-2d3b-4ae5-bb51-6fc9ee1a7dc9",
   "metadata": {},
   "source": [
    "### Judge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de675f-9125-4de2-92eb-5eb57aa19033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3. Call Azure OpenAI API ==========\n",
    "\n",
    "# Fill in your Azure details here:\n",
    "api_key = \"YOUR_AZURE_OPENAI_API_KEY\"\n",
    "api_base = \"YOUR_AZURE_OPENAI_ENDPOINT\"       # e.g. \"https://your-resource-name.openai.azure.com/\"\n",
    "api_version = \"2024-02-15-preview\"            # Adjust if needed\n",
    "deployment_name = \"YOUR_DEPLOYMENT_NAME\"      # e.g. \"gpt-4\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=api_base\n",
    ")\n",
    "\n",
    "\n",
    "# Measure generation time\n",
    "start_time = time.time()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that explains model risk scores.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "generation_time = end_time - start_time\n",
    "\n",
    "generated_text = response.choices[0].message.content.strip()\n",
    "\n",
    "# ========== 4. Print the Response ==========\n",
    "generated_text = response.choices[0].message.content.strip()\n",
    "print(\"\\n=== Generated Narrative ===\")\n",
    "print(generated_text)\n",
    "print(f\"\\nGeneration Time: {generation_time:.2f} seconds\")\n",
    "\n",
    "# ========== 4. Evaluate the Explanation ==========\n",
    "evaluation_prompt = f\"\"\"\n",
    "Please evaluate the following risk explanation on the following criteria (scale 1-5):\n",
    "1. Clarity\n",
    "2. Conciseness\n",
    "3. Completeness\n",
    "\n",
    "Provide a short justification for each score.\n",
    "\n",
    "Generated Explanation:\n",
    "\\\"\\\"\\\"\n",
    "{generated_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "evaluation_response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an evaluation assistant that rates the quality of risk explanations.\"},\n",
    "        {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "evaluation_text = evaluation_response.choices[0].message.content.strip()\n",
    "\n",
    "print(\"\\n=== Evaluation ===\")\n",
    "print(evaluation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59222d50-ba0c-43fd-b92f-8b710300d749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34df853-f7ec-4e9b-9824-efd99c63c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your deployed Azure OpenAI judge models\n",
    "judge_models = {\n",
    "    \"GPT-4\": \"gpt-4-deployment\",          # Replace with your Azure deployment name\n",
    "    \"GPT-4o\": \"gpt-4o-deployment\",        # Replace with your Azure deployment name\n",
    "    \"GPT-35-Turbo\": \"gpt-35-turbo-deployment\"  # Replace with your Azure deployment name\n",
    "}\n",
    "\n",
    "# Function to evaluate with each judge model\n",
    "def evaluate_with_judge(deployment_name, prompt, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an evaluation assistant. Given a prompt and a model-generated answer, please assess the quality of the answer based on:\n",
    "- Clarity (1-5)\n",
    "- Conciseness (1-5)\n",
    "- Completeness (1-5)\n",
    "Provide a score for each, then write a short summary comment.\n",
    "\n",
    "Prompt:\n",
    "{prompt}\n",
    "\n",
    "Model-Generated Answer:\n",
    "{output}\n",
    "\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that evaluates AI-generated text.\"},\n",
    "            {\"role\": \"user\", \"content\": eval_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    eval_text = response['choices'][0]['message']['content']\n",
    "    return eval_text\n",
    "\n",
    "def extract_scores(eval_text):\n",
    "    \"\"\"\n",
    "    Extracts clarity, conciseness, and completeness scores from the evaluation text.\n",
    "    \"\"\"\n",
    "    clarity = conciseness = completeness = None\n",
    "\n",
    "    clarity_match = re.search(r'Clarity[:\\s]+(\\d)', eval_text, re.IGNORECASE)\n",
    "    conciseness_match = re.search(r'Conciseness[:\\s]+(\\d)', eval_text, re.IGNORECASE)\n",
    "    completeness_match = re.search(r'Completeness[:\\s]+(\\d)', eval_text, re.IGNORECASE)\n",
    "\n",
    "    if clarity_match:\n",
    "        clarity = int(clarity_match.group(1))\n",
    "    if conciseness_match:\n",
    "        conciseness = int(conciseness_match.group(1))\n",
    "    if completeness_match:\n",
    "        completeness = int(completeness_match.group(1))\n",
    "\n",
    "    return clarity, conciseness, completeness\n",
    "\n",
    "# Evaluate using each judge model\n",
    "results = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "for judge_name, deployment_name in judge_models.items():\n",
    "    eval_start = datetime.now()\n",
    "    eval_text = evaluate_with_judge(deployment_name, prompt_text, generated_narrative)\n",
    "    eval_end = datetime.now()\n",
    "    time_taken = (eval_end - eval_start).total_seconds()\n",
    "    # results.append({\n",
    "    #     \"Judge Model\": judge_name,\n",
    "    #     \"Evaluation Text\": eval_text.strip(),\n",
    "    #     \"Time Taken (s)\": time_taken\n",
    "    # })\n",
    "    clarity, conciseness, completeness = extract_scores(eval_text)\n",
    "    results.append({\n",
    "        \"Judge Model\": judge_name,\n",
    "        \"Clarity\": clarity,\n",
    "        \"Conciseness\": conciseness,\n",
    "        \"Completeness\": completeness,\n",
    "        \"Evaluation Summary\": eval_text.strip(),\n",
    "        \"Time Taken (s)\": f\"{time_taken:.2f}\"\n",
    "    })\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(df_results.to_markdown(index=False))\n",
    "\n",
    "df_results.to_csv(\"evaluation_results_azure_openai.csv\", index=False)\n",
    "print(f\"\\nTotal evaluation time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fbb40-082a-453d-a251-e67678d8e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results as a table using tabulate\n",
    "print(\"\\n=== Evaluation Results ===\\n\")\n",
    "print(tabulate(df_results, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d138135-db95-42d9-9893-e6670ff839dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d728c40f-96e6-41b8-8f1c-8380ffa53a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explain the theory of relativity.</td>\n",
       "      <td>The theory of relativity was developed by Albe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>The capital of France is Paris.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              prompt  \\\n",
       "0  Explain the theory of relativity.   \n",
       "1     What is the capital of France?   \n",
       "\n",
       "                                    generated_answer  \n",
       "0  The theory of relativity was developed by Albe...  \n",
       "1                    The capital of France is Paris.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example DataFrame with prompt and output columns\n",
    "df = pd.DataFrame({\n",
    "    \"prompt\": [\n",
    "        \"Explain the theory of relativity.\",\n",
    "        \"What is the capital of France?\"\n",
    "    ],\n",
    "    \"generated_answer\": [\n",
    "        \"The theory of relativity was developed by Albert Einstein...\",\n",
    "        \"The capital of France is Paris.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "df.to_csv(\"test.csv\", index=False)\n",
    "df\n",
    "\n",
    "# Run evaluation\n",
    "# df_with_scores = evaluate_dataframe(df, prompt_col=\"prompt\", output_col=\"generated_answer\")\n",
    "# df_with_scores.to_csv(save_to_csv, index=False)\n",
    "# print(f\"Saved evaluation results to '{save_to_csv}'\")\n",
    "\n",
    "# print(df_with_scores.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e30e7-7804-4a2d-90df-5931bcda1059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f6099-865b-4d6d-8c74-4f0ecdac6bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71707b-fba8-40c1-848b-66b81d9f4ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5402a3-27cf-4354-bf27-901382a1c618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b0ae5-cb10-494c-b623-c30e0719e814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1be5ecbc-e37f-4848-8d9d-507e0b976421",
   "metadata": {},
   "source": [
    "Can I use LLMs to figure out the relationship between features?\n",
    "\n",
    "Not directly from the raw data — LLMs like GPT-4 are language models, and they do not learn relationships from raw data the way statistical or ML models (like XGBoost) do.\n",
    "\n",
    "You can prompt an LLM to summarize relationships if you:\n",
    "\n",
    "- Provide a summary of your feature importances or interactions (e.g. from SHAP values or partial dependence plots).\n",
    "- Ask the LLM to generate human-readable explanations about possible relationships (e.g. \"If feature A and feature B both have high values, how might that influence risk?\").\n",
    "\n",
    "For example:\n",
    "If you compute SHAP interaction values (which show how pairs of features interact in the XGBoost model), you can prompt the LLM:\n",
    "\n",
    "“Given that Feature A and Feature B have a strong interaction (SHAP interaction value = 0.25), what might that indicate about risk behavior?”\n",
    "\n",
    "The LLM can then verbalize or hypothesize the interaction in plain language — but it’s not discovering new interactions itself. That part is still done by your ML tools.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "They cannot themselves build a predictive model from raw data.\n",
    "They can summarize existing knowledge you give them (like feature interactions, partial dependence plots, or SHAP values).\n",
    "\n",
    "\n",
    "\n",
    "They cannot uncover feature interactions from raw tabular data on their own — that’s what tree-based models or techniques like SHAP do.\n",
    "\n",
    "Given that Feature X has a contribution of 0.5% to the risk score, what might explain its low contribution relative to other features?\n",
    "\n",
    "The LLM can generate reasons like:\n",
    "\n",
    "“This feature might have a narrow value range, so it has less discriminative power.”\n",
    "\n",
    "“This feature might overlap with other highly predictive features, so its unique contribution is reduced.”\n",
    "\n",
    "Again, the LLM is explaining, not discovering the underlying math.\n",
    "\n",
    "\n",
    "| **Question**                               | **LLM’s Role**                                         | **Best Tool**                                             |\n",
    "| ------------------------------------------ | ------------------------------------------------------ | --------------------------------------------------------- |\n",
    "| Discovering relationships between features | Explain them verbally, given data from XGBoost or SHAP | XGBoost (tree-based splits) + SHAP                        |\n",
    "| Explaining low contribution features       | Very good at explaining why, given contribution data   | XGBoost + SHAP for quantifying                            |\n",
    "| Generating human-readable insights         | Excellent, given data                                  | LLM (with model explanations)                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0ddba-884d-4e73-837a-9141d3860358",
   "metadata": {},
   "source": [
    "✅ Style guidance — Helps the LLM learn to produce text that matches your team's preferred tone and structure\n",
    "\n",
    "✅ Evaluation — Provides a \"gold standard\" baseline to compare LLM outputs against"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
