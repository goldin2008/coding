{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94f44dce-e54f-4eea-9d19-dc9fa9d9a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a2e3ca-5757-4fc0-a2fa-4e1723785424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_library.csv saved.\n",
      "feature_score.csv saved.\n",
      "risk_score.csv saved.\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Feature Library\n",
    "# ===============================\n",
    "\n",
    "feature_library_data = {\n",
    "    \"feature_name\": [\n",
    "        \"wirein_ct\",\n",
    "        \"perc_hrg_wire_amt\",\n",
    "        \"degree_centrality\"\n",
    "    ],\n",
    "    \"feature_meaning\": [\n",
    "        \"Number of wire inbound transactions\",\n",
    "        \"Percentage of wire amount associated with high-risk geographic country\",\n",
    "        \"Number of connections an entity has in a network (degree centrality)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "feature_library_df = pd.DataFrame(feature_library_data)\n",
    "feature_library_df.to_csv(\"feature_library.csv\", index=False)\n",
    "print(\"feature_library.csv saved.\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Feature Score\n",
    "# ===============================\n",
    "\n",
    "feature_score_data = {\n",
    "    \"feature_name\": [\n",
    "        \"wirein_ct\",\n",
    "        \"perc_hrg_wire_amt\",\n",
    "        \"degree_centrality\"\n",
    "    ],\n",
    "    \"score\": [\n",
    "        0.52,\n",
    "        0.36,\n",
    "        0.12\n",
    "    ]\n",
    "}\n",
    "\n",
    "feature_score_df = pd.DataFrame(feature_score_data)\n",
    "feature_score_df.to_csv(\"feature_score.csv\", index=False)\n",
    "print(\"feature_score.csv saved.\")\n",
    "\n",
    "# ===============================\n",
    "# 3. Risk Score\n",
    "# ===============================\n",
    "\n",
    "risk_score_data = {\n",
    "    \"risk_score\": [\n",
    "        0.8\n",
    "    ]\n",
    "}\n",
    "\n",
    "risk_score_df = pd.DataFrame(risk_score_data)\n",
    "risk_score_df.to_csv(\"risk_score.csv\", index=False)\n",
    "print(\"risk_score.csv saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d4f0a1-6f09-4876-8596-2d977bf4f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Load the data\n",
    "# ===============================\n",
    "\n",
    "feature_library = pd.read_csv(\"feature_library.csv\")\n",
    "feature_score = pd.read_csv(\"feature_score.csv\")\n",
    "risk_score_df = pd.read_csv(\"risk_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb544aa-7414-4d37-9d60-e3f654f0e0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        feature_name                                    feature_meaning\n",
       " 0          wirein_ct                Number of wire inbound transactions\n",
       " 1  perc_hrg_wire_amt  Percentage of wire amount associated with high...\n",
       " 2  degree_centrality  Number of connections an entity has in a netwo...,\n",
       "         feature_name  score\n",
       " 0          wirein_ct   0.52\n",
       " 1  perc_hrg_wire_amt   0.36\n",
       " 2  degree_centrality   0.12,\n",
       "    risk_score\n",
       " 0         0.8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_library, feature_score, risk_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d1406d7-e358-47c7-baf6-32a2f78b7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt Text ===\n",
      "\n",
      "You are a risk model explanation assistant. Given a risk score and a list of features with their descriptions and contributions, generate a clear, concise narrative explaining the risk score.\n",
      "\n",
      "Risk Score: 80%\n",
      "Top Features and Contributions:\n",
      "- wirein_ct (Number of wire inbound transactions): 52% contribution\n",
      "- perc_hrg_wire_amt (Percentage of wire amount associated with high-risk geographic country): 36% contribution\n",
      "- degree_centrality (Number of connections an entity has in a network (degree centrality)): 12% contribution\n",
      "\n",
      "\n",
      "Please produce a narrative that:\n",
      "- Starts with the risk score\n",
      "- Explains each feature’s contribution in plain language\n",
      "- Highlights why each feature might indicate a higher risk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== 2. Prepare the Prompt ==========\n",
    "# Sort feature scores by descending contribution\n",
    "feature_score_df = feature_score_df.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# Merge the scores with the feature meanings\n",
    "merged_df = pd.merge(\n",
    "    feature_score_df,\n",
    "    feature_library_df,\n",
    "    on=\"feature_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Extract the risk score\n",
    "risk_score = risk_score_df.iloc[0][\"risk_score\"]\n",
    "\n",
    "# Format features for the prompt\n",
    "features_text = \"\"\n",
    "for _, row in merged_df.iterrows():\n",
    "    feature_name = row[\"feature_name\"]\n",
    "    feature_meaning = row[\"feature_meaning\"]\n",
    "    score = row[\"score\"]\n",
    "    features_text += (\n",
    "        f\"- {feature_name} ({feature_meaning}): {score:.0%} contribution\\n\"\n",
    "    )\n",
    "\n",
    "# Build the prompt\n",
    "prompt_text = f\"\"\"\n",
    "You are a risk model explanation assistant. Given a risk score and a list of features with their descriptions and contributions, generate a clear, concise narrative explaining the risk score.\n",
    "\n",
    "Risk Score: {risk_score:.0%}\n",
    "Top Features and Contributions:\n",
    "{features_text}\n",
    "\n",
    "Please produce a narrative that:\n",
    "- Starts with the risk score\n",
    "- Explains each feature’s contribution in plain language\n",
    "- Highlights why each feature might indicate a higher risk\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Prompt Text ===\")\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de675f-9125-4de2-92eb-5eb57aa19033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3. Call Azure OpenAI API ==========\n",
    "\n",
    "# Fill in your Azure details here:\n",
    "api_key = \"YOUR_AZURE_OPENAI_API_KEY\"\n",
    "api_base = \"YOUR_AZURE_OPENAI_ENDPOINT\"       # e.g. \"https://your-resource-name.openai.azure.com/\"\n",
    "api_version = \"2024-02-15-preview\"            # Adjust if needed\n",
    "deployment_name = \"YOUR_DEPLOYMENT_NAME\"      # e.g. \"gpt-4\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=api_base\n",
    ")\n",
    "\n",
    "\n",
    "# Measure generation time\n",
    "start_time = time.time()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that explains model risk scores.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "generation_time = end_time - start_time\n",
    "\n",
    "generated_text = response.choices[0].message.content.strip()\n",
    "\n",
    "# ========== 4. Print the Response ==========\n",
    "generated_text = response.choices[0].message.content.strip()\n",
    "print(\"\\n=== Generated Narrative ===\")\n",
    "print(generated_text)\n",
    "print(f\"\\nGeneration Time: {generation_time:.2f} seconds\")\n",
    "\n",
    "# ========== 4. Evaluate the Explanation ==========\n",
    "evaluation_prompt = f\"\"\"\n",
    "Please evaluate the following risk explanation on the following criteria (scale 1-5):\n",
    "1. Clarity\n",
    "2. Conciseness\n",
    "3. Completeness\n",
    "\n",
    "Provide a short justification for each score.\n",
    "\n",
    "Generated Explanation:\n",
    "\\\"\\\"\\\"\n",
    "{generated_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "evaluation_response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an evaluation assistant that rates the quality of risk explanations.\"},\n",
    "        {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "evaluation_text = evaluation_response.choices[0].message.content.strip()\n",
    "\n",
    "print(\"\\n=== Evaluation ===\")\n",
    "print(evaluation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34df853-f7ec-4e9b-9824-efd99c63c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your deployed Azure OpenAI judge models\n",
    "judge_models = {\n",
    "    \"GPT-4\": \"gpt-4-deployment\",          # Replace with your Azure deployment name\n",
    "    \"GPT-4o\": \"gpt-4o-deployment\",        # Replace with your Azure deployment name\n",
    "    \"GPT-35-Turbo\": \"gpt-35-turbo-deployment\"  # Replace with your Azure deployment name\n",
    "}\n",
    "\n",
    "# Function to evaluate with each judge model\n",
    "def evaluate_with_judge(deployment_name, prompt, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an evaluation assistant. Given a prompt and a model-generated answer, please assess the quality of the answer based on:\n",
    "- Clarity (1-5)\n",
    "- Conciseness (1-5)\n",
    "- Completeness (1-5)\n",
    "Provide a score for each, then write a short summary comment.\n",
    "\n",
    "Prompt:\n",
    "{prompt}\n",
    "\n",
    "Model-Generated Answer:\n",
    "{output}\n",
    "\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that evaluates AI-generated text.\"},\n",
    "            {\"role\": \"user\", \"content\": eval_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    eval_text = response['choices'][0]['message']['content']\n",
    "    return eval_text\n",
    "\n",
    "# Evaluate using each judge model\n",
    "results = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "for judge_name, deployment_name in judge_models.items():\n",
    "    eval_start = datetime.now()\n",
    "    eval_text = evaluate_with_judge(deployment_name, prompt_text, generated_narrative)\n",
    "    eval_end = datetime.now()\n",
    "    time_taken = (eval_end - eval_start).total_seconds()\n",
    "    results.append({\n",
    "        \"Judge Model\": judge_name,\n",
    "        \"Evaluation Text\": eval_text.strip(),\n",
    "        \"Time Taken (s)\": time_taken\n",
    "    })\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(df_results.to_markdown(index=False))\n",
    "\n",
    "df_results.to_csv(\"evaluation_results_azure_openai.csv\", index=False)\n",
    "print(f\"\\nTotal evaluation time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fbb40-082a-453d-a251-e67678d8e529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
