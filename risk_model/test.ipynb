{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f44dce-e54f-4eea-9d19-dc9fa9d9a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a2e3ca-5757-4fc0-a2fa-4e1723785424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_library.csv saved.\n",
      "feature_score.csv saved.\n",
      "risk_score.csv saved.\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Feature Library\n",
    "# ===============================\n",
    "\n",
    "feature_library_data = {\n",
    "    \"feature_name\": [\n",
    "        \"wirein_ct\",\n",
    "        \"perc_hrg_wire_amt\",\n",
    "        \"degree_centrality\"\n",
    "    ],\n",
    "    \"feature_meaning\": [\n",
    "        \"Number of wire inbound transactions\",\n",
    "        \"Percentage of wire amount associated with high-risk geographic country\",\n",
    "        \"Number of connections an entity has in a network (degree centrality)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "feature_library_df = pd.DataFrame(feature_library_data)\n",
    "feature_library_df.to_csv(\"feature_library.csv\", index=False)\n",
    "print(\"feature_library.csv saved.\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Feature Score\n",
    "# ===============================\n",
    "\n",
    "feature_score_data = {\n",
    "    \"feature_name\": [\n",
    "        \"wirein_ct\",\n",
    "        \"perc_hrg_wire_amt\",\n",
    "        \"degree_centrality\"\n",
    "    ],\n",
    "    \"score\": [\n",
    "        0.52,\n",
    "        0.36,\n",
    "        0.12\n",
    "    ]\n",
    "}\n",
    "\n",
    "feature_score_df = pd.DataFrame(feature_score_data)\n",
    "feature_score_df.to_csv(\"feature_score.csv\", index=False)\n",
    "print(\"feature_score.csv saved.\")\n",
    "\n",
    "# ===============================\n",
    "# 3. Risk Score\n",
    "# ===============================\n",
    "\n",
    "risk_score_data = {\n",
    "    \"risk_score\": [\n",
    "        0.8\n",
    "    ]\n",
    "}\n",
    "\n",
    "risk_score_df = pd.DataFrame(risk_score_data)\n",
    "risk_score_df.to_csv(\"risk_score.csv\", index=False)\n",
    "print(\"risk_score.csv saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d4f0a1-6f09-4876-8596-2d977bf4f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Load the data\n",
    "# ===============================\n",
    "\n",
    "feature_library = pd.read_csv(\"feature_library.csv\")\n",
    "feature_score = pd.read_csv(\"feature_score.csv\")\n",
    "risk_score_df = pd.read_csv(\"risk_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb544aa-7414-4d37-9d60-e3f654f0e0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        feature_name                                    feature_meaning\n",
       " 0          wirein_ct                Number of wire inbound transactions\n",
       " 1  perc_hrg_wire_amt  Percentage of wire amount associated with high...\n",
       " 2  degree_centrality  Number of connections an entity has in a netwo...,\n",
       "         feature_name  score\n",
       " 0          wirein_ct   0.52\n",
       " 1  perc_hrg_wire_amt   0.36\n",
       " 2  degree_centrality   0.12,\n",
       "    risk_score\n",
       " 0         0.8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_library, feature_score, risk_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d1406d7-e358-47c7-baf6-32a2f78b7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt Text ===\n",
      "\n",
      "You are a risk model explanation assistant. Given a risk score and a list of features with their descriptions and contributions, generate a clear, concise narrative explaining the risk score.\n",
      "\n",
      "Risk Score: 80%\n",
      "Top Features and Contributions:\n",
      "- wirein_ct (Number of wire inbound transactions): 52% contribution\n",
      "- perc_hrg_wire_amt (Percentage of wire amount associated with high-risk geographic country): 36% contribution\n",
      "- degree_centrality (Number of connections an entity has in a network (degree centrality)): 12% contribution\n",
      "\n",
      "\n",
      "Please produce a narrative that:\n",
      "- Starts with the risk score\n",
      "- Explains each featureâ€™s contribution in plain language\n",
      "- Highlights why each feature might indicate a higher risk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== 2. Prepare the Prompt ==========\n",
    "# Sort feature scores by descending contribution\n",
    "feature_score_df = feature_score_df.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# Merge the scores with the feature meanings\n",
    "merged_df = pd.merge(\n",
    "    feature_score_df,\n",
    "    feature_library_df,\n",
    "    on=\"feature_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Extract the risk score\n",
    "risk_score = risk_score_df.iloc[0][\"risk_score\"]\n",
    "\n",
    "# Format features for the prompt\n",
    "features_text = \"\"\n",
    "for _, row in merged_df.iterrows():\n",
    "    feature_name = row[\"feature_name\"]\n",
    "    feature_meaning = row[\"feature_meaning\"]\n",
    "    score = row[\"score\"]\n",
    "    features_text += (\n",
    "        f\"- {feature_name} ({feature_meaning}): {score:.0%} contribution\\n\"\n",
    "    )\n",
    "\n",
    "# Build the prompt\n",
    "prompt_text = f\"\"\"\n",
    "You are a risk model explanation assistant. Given a risk score and a list of features with their descriptions and contributions, generate a clear, concise narrative explaining the risk score.\n",
    "\n",
    "Risk Score: {risk_score:.0%}\n",
    "Top Features and Contributions:\n",
    "{features_text}\n",
    "\n",
    "Please produce a narrative that:\n",
    "- Starts with the risk score\n",
    "- Explains each featureâ€™s contribution in plain language\n",
    "- Highlights why each feature might indicate a higher risk\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Prompt Text ===\")\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de675f-9125-4de2-92eb-5eb57aa19033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3. Call Azure OpenAI API ==========\n",
    "\n",
    "# Fill in your Azure details here:\n",
    "api_key = \"YOUR_AZURE_OPENAI_API_KEY\"\n",
    "api_base = \"YOUR_AZURE_OPENAI_ENDPOINT\"       # e.g. \"https://your-resource-name.openai.azure.com/\"\n",
    "api_version = \"2024-02-15-preview\"            # Adjust if needed\n",
    "deployment_name = \"YOUR_DEPLOYMENT_NAME\"      # e.g. \"gpt-4\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=api_base\n",
    ")\n",
    "\n",
    "\n",
    "# Measure generation time\n",
    "start_time = time.time()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that explains model risk scores.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "generation_time = end_time - start_time\n",
    "\n",
    "generated_text = response.choices[0].message.content.strip()\n",
    "\n",
    "# ========== 4. Print the Response ==========\n",
    "generated_text = response.choices[0].message.content.strip()\n",
    "print(\"\\n=== Generated Narrative ===\")\n",
    "print(generated_text)\n",
    "print(f\"\\nGeneration Time: {generation_time:.2f} seconds\")\n",
    "\n",
    "# ========== 4. Evaluate the Explanation ==========\n",
    "evaluation_prompt = f\"\"\"\n",
    "Please evaluate the following risk explanation on the following criteria (scale 1-5):\n",
    "1. Clarity\n",
    "2. Conciseness\n",
    "3. Completeness\n",
    "\n",
    "Provide a short justification for each score.\n",
    "\n",
    "Generated Explanation:\n",
    "\\\"\\\"\\\"\n",
    "{generated_text}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "evaluation_response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an evaluation assistant that rates the quality of risk explanations.\"},\n",
    "        {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "evaluation_text = evaluation_response.choices[0].message.content.strip()\n",
    "\n",
    "print(\"\\n=== Evaluation ===\")\n",
    "print(evaluation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34df853-f7ec-4e9b-9824-efd99c63c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your deployed Azure OpenAI judge models\n",
    "judge_models = {\n",
    "    \"GPT-4\": \"gpt-4-deployment\",          # Replace with your Azure deployment name\n",
    "    \"GPT-4o\": \"gpt-4o-deployment\",        # Replace with your Azure deployment name\n",
    "    \"GPT-35-Turbo\": \"gpt-35-turbo-deployment\"  # Replace with your Azure deployment name\n",
    "}\n",
    "\n",
    "# Function to evaluate with each judge model\n",
    "def evaluate_with_judge(deployment_name, prompt, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an evaluation assistant. Given a prompt and a model-generated answer, please assess the quality of the answer based on:\n",
    "- Clarity (1-5)\n",
    "- Conciseness (1-5)\n",
    "- Completeness (1-5)\n",
    "Provide a score for each, then write a short summary comment.\n",
    "\n",
    "Prompt:\n",
    "{prompt}\n",
    "\n",
    "Model-Generated Answer:\n",
    "{output}\n",
    "\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that evaluates AI-generated text.\"},\n",
    "            {\"role\": \"user\", \"content\": eval_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    eval_text = response['choices'][0]['message']['content']\n",
    "    return eval_text\n",
    "\n",
    "def extract_scores(eval_text):\n",
    "    \"\"\"\n",
    "    Extracts clarity, conciseness, and completeness scores from the evaluation text.\n",
    "    \"\"\"\n",
    "    clarity = conciseness = completeness = None\n",
    "\n",
    "    clarity_match = re.search(r'Clarity[:\\s]+(\\d)', eval_text, re.IGNORECASE)\n",
    "    conciseness_match = re.search(r'Conciseness[:\\s]+(\\d)', eval_text, re.IGNORECASE)\n",
    "    completeness_match = re.search(r'Completeness[:\\s]+(\\d)', eval_text, re.IGNORECASE)\n",
    "\n",
    "    if clarity_match:\n",
    "        clarity = int(clarity_match.group(1))\n",
    "    if conciseness_match:\n",
    "        conciseness = int(conciseness_match.group(1))\n",
    "    if completeness_match:\n",
    "        completeness = int(completeness_match.group(1))\n",
    "\n",
    "    return clarity, conciseness, completeness\n",
    "\n",
    "# Evaluate using each judge model\n",
    "results = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "for judge_name, deployment_name in judge_models.items():\n",
    "    eval_start = datetime.now()\n",
    "    eval_text = evaluate_with_judge(deployment_name, prompt_text, generated_narrative)\n",
    "    eval_end = datetime.now()\n",
    "    time_taken = (eval_end - eval_start).total_seconds()\n",
    "    # results.append({\n",
    "    #     \"Judge Model\": judge_name,\n",
    "    #     \"Evaluation Text\": eval_text.strip(),\n",
    "    #     \"Time Taken (s)\": time_taken\n",
    "    # })\n",
    "    clarity, conciseness, completeness = extract_scores(eval_text)\n",
    "    results.append({\n",
    "        \"Judge Model\": judge_name,\n",
    "        \"Clarity\": clarity,\n",
    "        \"Conciseness\": conciseness,\n",
    "        \"Completeness\": completeness,\n",
    "        \"Evaluation Summary\": eval_text.strip(),\n",
    "        \"Time Taken (s)\": f\"{time_taken:.2f}\"\n",
    "    })\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(df_results.to_markdown(index=False))\n",
    "\n",
    "df_results.to_csv(\"evaluation_results_azure_openai.csv\", index=False)\n",
    "print(f\"\\nTotal evaluation time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fbb40-082a-453d-a251-e67678d8e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results as a table using tabulate\n",
    "print(\"\\n=== Evaluation Results ===\\n\")\n",
    "print(tabulate(df_results, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d138135-db95-42d9-9893-e6670ff839dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1be5ecbc-e37f-4848-8d9d-507e0b976421",
   "metadata": {},
   "source": [
    "Can I use LLMs to figure out the relationship between features?\n",
    "\n",
    "Not directly from the raw data â€” LLMs like GPT-4 are language models, and they do not learn relationships from raw data the way statistical or ML models (like XGBoost) do.\n",
    "\n",
    "You can prompt an LLM to summarize relationships if you:\n",
    "\n",
    "- Provide a summary of your feature importances or interactions (e.g. from SHAP values or partial dependence plots).\n",
    "- Ask the LLM to generate human-readable explanations about possible relationships (e.g. \"If feature A and feature B both have high values, how might that influence risk?\").\n",
    "\n",
    "For example:\n",
    "If you compute SHAP interaction values (which show how pairs of features interact in the XGBoost model), you can prompt the LLM:\n",
    "\n",
    "â€œGiven that Feature A and Feature B have a strong interaction (SHAP interaction value = 0.25), what might that indicate about risk behavior?â€\n",
    "\n",
    "The LLM can then verbalize or hypothesize the interaction in plain language â€” but itâ€™s not discovering new interactions itself. That part is still done by your ML tools.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "They cannot themselves build a predictive model from raw data.\n",
    "They can summarize existing knowledge you give them (like feature interactions, partial dependence plots, or SHAP values).\n",
    "\n",
    "\n",
    "\n",
    "They cannot uncover feature interactions from raw tabular data on their own â€” thatâ€™s what tree-based models or techniques like SHAP do.\n",
    "\n",
    "Given that Feature X has a contribution of 0.5% to the risk score, what might explain its low contribution relative to other features?\n",
    "\n",
    "The LLM can generate reasons like:\n",
    "\n",
    "â€œThis feature might have a narrow value range, so it has less discriminative power.â€\n",
    "\n",
    "â€œThis feature might overlap with other highly predictive features, so its unique contribution is reduced.â€\n",
    "\n",
    "Again, the LLM is explaining, not discovering the underlying math.\n",
    "\n",
    "\n",
    "| **Question**                               | **LLMâ€™s Role**                                         | **Best Tool**                                             |\n",
    "| ------------------------------------------ | ------------------------------------------------------ | --------------------------------------------------------- |\n",
    "| Discovering relationships between features | Explain them verbally, given data from XGBoost or SHAP | XGBoost (tree-based splits) + SHAP                        |\n",
    "| Explaining low contribution features       | Very good at explaining why, given contribution data   | XGBoost + SHAP for quantifying                            |\n",
    "| Generating human-readable insights         | Excellent, given data                                  | LLM (with model explanations)                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0ddba-884d-4e73-837a-9141d3860358",
   "metadata": {},
   "source": [
    "âœ… Style guidance â€” Helps the LLM learn to produce text that matches your team's preferred tone and structure\n",
    "\n",
    "âœ… Evaluation â€” Provides a \"gold standard\" baseline to compare LLM outputs against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96d7612a-5245-4686-951a-ee01bcc24a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human-written narrative CSV file created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data\n",
    "\n",
    "data = {\n",
    "    \"entity_id\": [123],\n",
    "    \"risk_score\": [0.80],\n",
    "    \"input\": [\n",
    "        \"\"\"Risk Score: 80%\n",
    "Top Features and Contributions:\n",
    "- Number of wire inbound transactions (Number of inbound wire transactions initiated by the entity): 52% contribution\n",
    "- Percentage of wire amount associated with HRG country (Proportion of transactions linked to high-risk geographies): 36% contribution\n",
    "- Number of connections an entity has in a network (Degree centrality in a network analysis): 12% contribution\n",
    "\"\"\"\n",
    "    ],\n",
    "    \"narrative\": [\n",
    "        \"\"\"123: 80%\n",
    "Number of wire inbound transactions is the most significant contributor to the risk score, accounting for 52% of the total risk. A high number of inbound wire transactions can be indicative of unusual or suspicious activity, especially if the volume is significantly higher than typical patterns for similar entities.\n",
    "Percentage of wire amount associated with HRG country: Contributing 36% to the risk score, this feature highlights the proportion of wire transactions linked to a high-risk geographic (HRG) country.\n",
    "Number of connections an entity has in a network: This feature contributes 12% to the risk score. Degree centrality measures the number of connections an entity has within a network, which can be indicative of its influence or involvement in complex networks. A high degree centrality may suggest that the entity is well-connected, potentially facilitating or participating in coordinated activities that could pose a risk.\n",
    "\"\"\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df_human_narratives = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df_human_narratives.to_csv(\"human_narratives.csv\", index=False)\n",
    "\n",
    "print(\"Human-written narrative CSV file created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72c129e7-5a05-4dca-8179-3316a68c8b96",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'contribution'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'contribution'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df_feature_score\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     20\u001b[0m     feature_name \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m     contribution \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontribution\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Assuming contribution is between 0 and 1\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     description \u001b[38;5;241m=\u001b[39m df_feature_library[df_feature_library[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m feature_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_meaning\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m     feature_contributions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontribution\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% contribution\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'contribution'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load human narrative examples\n",
    "df_human_narratives = pd.read_csv(\"human_narratives.csv\")\n",
    "\n",
    "# Select the first example\n",
    "example_input = df_human_narratives.iloc[0]['input']\n",
    "example_narrative = df_human_narratives.iloc[0]['narrative']\n",
    "\n",
    "# Load model output files\n",
    "df_feature_library = pd.read_csv(\"feature_library.csv\")\n",
    "df_feature_score = pd.read_csv(\"feature_score.csv\")\n",
    "df_risk_score = pd.read_csv(\"risk_score.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Construct the prompt with the human example included\n",
    "prompt_text = f\"\"\"\n",
    "You are a risk model explanation assistant. Given a risk score and a list of features with their descriptions and contributions, generate a clear, concise narrative explaining the risk score.\n",
    "\n",
    "Here is an example of a high-quality explanation from a human:\n",
    "\n",
    "Input:\n",
    "{example_input}\n",
    "\n",
    "Output:\n",
    "{example_narrative}\n",
    "\n",
    "Now generate a narrative for the following data:\n",
    "\n",
    "Input:\n",
    "Risk Score: {risk_score:.0f}%\n",
    "Top Features and Contributions:\n",
    "{top_features_text}\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "# # Send to Azure OpenAI\n",
    "# client = AzureOpenAI(\n",
    "#     api_key=\"YOUR_AZURE_OPENAI_API_KEY\",\n",
    "#     api_version=\"2023-05-15\",\n",
    "#     azure_endpoint=\"YOUR_AZURE_OPENAI_ENDPOINT\"\n",
    "# )\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-35-turbo\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": prompt_text}\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# generated_narrative = response.choices[0].message.content.strip()\n",
    "\n",
    "# # Print the generated narrative\n",
    "# print(\"=== Generated Narrative ===\")\n",
    "# print(generated_narrative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4dc060-71ec-4892-8062-925b440391ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt = f\"\"\"\n",
    "You are an evaluation assistant. You will compare two narratives explaining a risk score for a financial model. The first narrative is from a human expert, and the second narrative is generated by a language model. Please rate the clarity, completeness, and overall quality of each narrative on a scale from 1 to 5, and then provide an overall judgment of which narrative is better.\n",
    "\n",
    "Human Narrative:\n",
    "{human_narrative}\n",
    "\n",
    "LLM-Generated Narrative:\n",
    "{llm_narrative}\n",
    "\n",
    "Please provide:\n",
    "1. A table comparing clarity, completeness, and quality for each narrative.\n",
    "2. An overall rating indicating which narrative is better and why.\n",
    "\"\"\"\n",
    "\n",
    "judge_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # or gpt-35-turbo-instruct\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a fair and thorough evaluator.\"},\n",
    "        {\"role\": \"user\", \"content\": judge_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "judge_evaluation = judge_response.choices[0].message.content.strip()\n",
    "print(\"=== Judge Evaluation ===\")\n",
    "print(judge_evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1931b49-685c-4027-b0e7-6bfafc9f6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = f\"\"\"\n",
    "You are a risk model explanation assistant. Given a risk score and a list of features with their descriptions and contributions, generate a clear, concise narrative explaining the risk score.\n",
    "\n",
    "Here is an example of a high-quality narrative:\n",
    "\n",
    "[Example Narrative]\n",
    "\n",
    "Risk Score: 80%\n",
    "Top Features and Contributions:\n",
    "- wirein_ct (Number of wire inbound transactions): 52% contribution\n",
    "- perc_hrg_wire_amt (Percentage of wire amount associated with HRG country): 36% contribution\n",
    "- degree_centrality (Number of connections): 12% contribution\n",
    "\n",
    "Now generate a narrative for the following data:\n",
    "Risk Score: {risk_score * 100:.2f}%\n",
    "Top Features and Contributions:\n",
    "\"\"\"\n",
    "\n",
    "# Append the top features list (same as before)\n",
    "\n",
    "\n",
    "# Step 1. Load human-written narrative samples\n",
    "df_human_samples = pd.read_csv(\"human_narratives.csv\")\n",
    "\n",
    "# Example: select a random sample or a relevant one\n",
    "sample_narrative = df_human_samples.iloc[0][\"narrative\"]\n",
    "\n",
    "# Step 2. Include it in the prompt\n",
    "prompt_text = f\"\"\"\n",
    "You are a risk model explanation assistant. Given a risk score and a list of features with their descriptions and contributions, generate a clear, concise narrative explaining the risk score.\n",
    "\n",
    "Here is an example of a high-quality narrative:\n",
    "{sample_narrative}\n",
    "\n",
    "Now generate a narrative for the following data:\n",
    "Risk Score: {round(risk_score * 100, 2)}%\n",
    "Top Features and Contributions:\n",
    "\"\"\"\n",
    "\n",
    "# Continue as before...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e169a-f744-4f47-84df-84d5f329d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ 1. Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from azure.ai.openai import OpenAIClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from tabulate import tabulate\n",
    "\n",
    "# ðŸ“¦ 2. Load your data\n",
    "# Replace with your actual file paths\n",
    "df_features = pd.read_csv(\"your_features.csv\")  # includes all 700 features\n",
    "df_labels = pd.read_csv(\"your_labels.csv\")      # risk labels\n",
    "\n",
    "X = df_features\n",
    "y = df_labels['risk_score']\n",
    "\n",
    "# ðŸ“¦ 3. Train XGBoost\n",
    "model = xgb.XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# ðŸ“¦ 4. Predict risk score for an example entity\n",
    "entity_index = 0  # Choose the row you want to explain\n",
    "entity_features = X.iloc[[entity_index]]\n",
    "risk_score = model.predict(entity_features)[0]\n",
    "\n",
    "# ðŸ“¦ 5. Compute SHAP values\n",
    "explainer = shap.Explainer(model, X)\n",
    "shap_values = explainer(entity_features)\n",
    "\n",
    "# Extract feature importances\n",
    "contributions = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"SHAP Value\": shap_values.values[0]\n",
    "})\n",
    "contributions[\"Abs SHAP Value\"] = np.abs(contributions[\"SHAP Value\"])\n",
    "contributions = contributions.sort_values(\"Abs SHAP Value\", ascending=False)\n",
    "\n",
    "# Select top N features for explanation\n",
    "top_n = 5\n",
    "top_features = contributions.head(top_n)\n",
    "\n",
    "# ðŸ“¦ 6. Generate prompt for Azure OpenAI\n",
    "prompt_text = f\"\"\"\n",
    "You are a risk model explanation assistant. Given a risk score and a list of features with their descriptions and contributions, generate a clear, concise narrative explaining the risk score.\n",
    "\n",
    "Risk Score: {round(risk_score * 100, 2)}%\n",
    "\n",
    "Top Features and Contributions:\n",
    "\"\"\"\n",
    "\n",
    "# Load feature library (if you have one) to get descriptions\n",
    "try:\n",
    "    feature_library = pd.read_csv(\"feature_library.csv\")\n",
    "    feature_descriptions = feature_library.set_index(\"Feature Name\")[\"Feature Meaning\"].to_dict()\n",
    "except FileNotFoundError:\n",
    "    feature_descriptions = {f: f for f in X.columns}  # fallback\n",
    "\n",
    "for _, row in top_features.iterrows():\n",
    "    feature = row[\"Feature\"]\n",
    "    contribution = round(row[\"Abs SHAP Value\"] / top_features[\"Abs SHAP Value\"].sum() * 100, 1)\n",
    "    description = feature_descriptions.get(feature, \"No description available\")\n",
    "    prompt_text += f\"- {feature} ({description}): {contribution}% contribution\\n\"\n",
    "\n",
    "prompt_text += \"\\nPlease produce a narrative that:\\n- Starts with the risk score\\n- Explains each featureâ€™s contribution in plain language\\n- Highlights why each feature might indicate a higher risk.\\n\"\n",
    "\n",
    "print(\"\\n=== PROMPT TO LLM ===\\n\")\n",
    "print(prompt_text)\n",
    "\n",
    "# ðŸ“¦ 7. Connect to Azure OpenAI and call the model\n",
    "AZURE_OPENAI_ENDPOINT = \"https://your-endpoint.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY = \"your-azure-openai-api-key\"\n",
    "DEPLOYMENT_NAME = \"your-gpt-deployment\"\n",
    "\n",
    "client = OpenAIClient(\n",
    "    endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=DEPLOYMENT_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant for risk model explanations.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ],\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "generated_narrative = response.choices[0].message.content.strip()\n",
    "\n",
    "# ðŸ“¦ 8. Print results\n",
    "print(\"\\n=== LLM RESPONSE ===\\n\")\n",
    "print(generated_narrative)\n",
    "\n",
    "# ðŸ“¦ 9. Store evaluation score placeholder (since Azure OpenAI API doesn't have native evaluation)\n",
    "evaluation_score = \"N/A (external manual evaluation recommended)\"\n",
    "\n",
    "# ðŸ“¦ 10. Display summary table\n",
    "summary_table = pd.DataFrame({\n",
    "    \"Risk Score\": [round(risk_score * 100, 2)],\n",
    "    \"Top Features\": [\", \".join(top_features['Feature'])],\n",
    "    \"Narrative\": [generated_narrative],\n",
    "    \"Evaluation Score\": [evaluation_score]\n",
    "})\n",
    "\n",
    "print(\"\\n=== SUMMARY TABLE ===\\n\")\n",
    "print(tabulate(summary_table, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "612da115-fb52-4180-8e94-313bfb67b7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prompt Text ===\n",
      "\n",
      "You are a risk model explanation assistant. Given a risk score and a list of features with their descriptions and contributions, generate a clear, concise narrative explaining the risk score.\n",
      "\n",
      "Here is an example of a high-quality narrative from a human for reference:\n",
      "\n",
      "Risk Score: 80%\n",
      "Narrative:\n",
      "123: 80%\n",
      "Number of wire inbound transactions is the most significant contributor to the risk score, accounting for 52% of the total risk. A high number of inbound wire transactions can be indicative of unusual or suspicious activity, especially if the volume is significantly higher than typical patterns for similar entities.\n",
      "Percentage of wire amount associated with HRG country: Contributing 36% to the risk score, this feature highlights the proportion of wire transactions linked to a high-risk geographic (HRG) country.\n",
      "Number of connections an entity has in a network: This feature contributes 12% to the risk score. Degree centrality measures the number of connections an entity has within a network, which can be indicative of its influence or involvement in complex networks. A high degree centrality may suggest that the entity is well-connected, potentially facilitating or participating in coordinated activities that could pose a risk.\n",
      "\n",
      "Now generate a narrative for the following data:\n",
      "\n",
      "Risk Score: 80%\n",
      "Top Features and Contributions:\n",
      "- wirein_ct (Number of wire inbound transactions): 52% contribution\n",
      "- perc_hrg_wire_amt (Percentage of wire amount associated with high-risk geographic country): 36% contribution\n",
      "- degree_centrality (Number of connections an entity has in a network (degree centrality)): 12% contribution\n",
      "\n",
      "\n",
      "Please produce a narrative that:\n",
      "- Starts with the risk score\n",
      "- Explains each featureâ€™s contribution in plain language\n",
      "- Highlights why each feature might indicate a higher risk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import AzureOpenAI  # or your Azure OpenAI SDK import\n",
    "\n",
    "# --- Load your dataframes ---\n",
    "feature_score_df = pd.read_csv(\"feature_score.csv\")\n",
    "feature_library_df = pd.read_csv(\"feature_library.csv\")\n",
    "risk_score_df = pd.read_csv(\"risk_score.csv\")\n",
    "\n",
    "# Load human narrative CSV that contains at least columns 'entity_id', 'risk_score', 'narrative'\n",
    "df_human_narratives = pd.read_csv(\"human_narratives.csv\")\n",
    "\n",
    "# Sort feature scores descending by 'score' (assumed between 0 and 1)\n",
    "feature_score_df = feature_score_df.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# Merge feature scores with meanings\n",
    "merged_df = pd.merge(\n",
    "    feature_score_df,\n",
    "    feature_library_df,\n",
    "    on=\"feature_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Extract risk score\n",
    "risk_score = risk_score_df.iloc[0][\"risk_score\"]\n",
    "\n",
    "# Format features for prompt\n",
    "features_text = \"\"\n",
    "for _, row in merged_df.iterrows():\n",
    "    feature_name = row[\"feature_name\"]\n",
    "    feature_meaning = row[\"feature_meaning\"]\n",
    "    score = row[\"score\"]\n",
    "    features_text += f\"- {feature_name} ({feature_meaning}): {score:.0%} contribution\\n\"\n",
    "\n",
    "# Select human narrative example â€” here assuming you use the first row\n",
    "human_example_risk_score = df_human_narratives.iloc[0][\"risk_score\"]\n",
    "human_example_narrative = df_human_narratives.iloc[0][\"narrative\"]\n",
    "\n",
    "# Construct the prompt including human example for LLM to learn style\n",
    "prompt_text = f\"\"\"\n",
    "You are a risk model explanation assistant. Given a risk score and a list of features with their descriptions and contributions, generate a clear, concise narrative explaining the risk score.\n",
    "\n",
    "Here is an example of a high-quality narrative from a human for reference:\n",
    "\n",
    "Risk Score: {human_example_risk_score:.0%}\n",
    "Narrative:\n",
    "{human_example_narrative.strip()}\n",
    "\n",
    "Now generate a narrative for the following data:\n",
    "\n",
    "Risk Score: {risk_score:.0%}\n",
    "Top Features and Contributions:\n",
    "{features_text}\n",
    "\n",
    "Please produce a narrative that:\n",
    "- Starts with the risk score\n",
    "- Explains each featureâ€™s contribution in plain language\n",
    "- Highlights why each feature might indicate a higher risk\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Prompt Text ===\")\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bf348-07ae-4c0f-aac3-e1eab5000c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Azure OpenAI API Setup ---\n",
    "client = AzureOpenAI(\n",
    "    api_key=\"YOUR_AZURE_OPENAI_API_KEY\",\n",
    "    azure_endpoint=\"YOUR_AZURE_OPENAI_ENDPOINT\",\n",
    "    api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "# Generate narrative from LLM\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in risk model explanation.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "llm_narrative = response.choices[0].message.content.strip()\n",
    "\n",
    "print(\"\\n=== Generated Narrative ===\")\n",
    "print(llm_narrative)\n",
    "\n",
    "\n",
    "# --- Evaluate the narratives with LLM as judge ---\n",
    "\n",
    "judge_prompt = f\"\"\"\n",
    "You are a judge AI that compares two narratives that explain a risk score based on given feature contributions. Evaluate their clarity, completeness, and informativeness.\n",
    "\n",
    "Narrative 1 (Human):\n",
    "{human_example_narrative.strip()}\n",
    "\n",
    "Narrative 2 (LLM):\n",
    "{llm_narrative}\n",
    "\n",
    "Please provide a short evaluation score for each (1-10), and a brief explanation of which one you prefer and why.\n",
    "\"\"\"\n",
    "\n",
    "judge_response = client.chat.completions.create(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that judges narrative explanations.\"},\n",
    "        {\"role\": \"user\", \"content\": judge_prompt}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "evaluation = judge_response.choices[0].message.content.strip()\n",
    "\n",
    "print(\"\\n=== Evaluation by LLM Judge ===\")\n",
    "print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
